{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uyJtGyxagvl",
        "outputId": "2508798e-cce1-42c3-e953-4222930dd70d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1P3tuCADA0G"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import validation_curve\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgDJF3MxtdWm"
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import  ViTForImageClassification\n",
        "\n",
        "# !pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQYezTFDpV2-",
        "outputId": "2608d2a1-55fc-4e98-dbc4-0f148aadfe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzexOSr5pf_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdornCzOW7zo",
        "outputId": "fef8cca1-d343-4bd3-88f5-4b0c6760632e"
      },
      "source": [
        "!pip install timm\n",
        "import timm\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H5TbJ_AXCZC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF-IZNL6bUbo"
      },
      "source": [
        "# !unzip './gdrive/My Drive/Pest Detection/competition/train.zip' -d './gdrive/My Drive/Pest Detection/wheat rust/variant'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Pj4U6C-WHN",
        "outputId": "ee38ab18-d4c1-4448-bd46-8fe0dea2f2af"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGNIAMg9xYv0"
      },
      "source": [
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dir ./gdrive/MyDrive/Pest\\ Detection/wheat\\ rust/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU8TcjXVCLoe",
        "outputId": "c97a4e77-83e6-4540-a0aa-592c88f5f97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dir: cannot access './gdrive/MyDrive/Pest Detection/wheat rust/dataset': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moX7m0LUxZdF"
      },
      "source": [
        "DATADIR = './gdrive/MyDrive/Thesis/sMURA-v1.1/smallDataset'\n",
        "# DATADIR = './gdrive/MyDrive/DeepLearning/pests/overfit'\n",
        "SYSPATH = './gdrive/MyDrive/2022-Thesis-II'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(SYSPATH)"
      ],
      "metadata": {
        "id": "dTppLTzUj-yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from MyClassifier import Classifier, EarlyStopper"
      ],
      "metadata": {
        "id": "8_NKoVpkkOj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBmyxoNDxgqS",
        "outputId": "eff6386a-3fa0-4b13-e098-86dfe1a052c6"
      },
      "source": [
        "# Load training data\n",
        "# define batch and image size\n",
        "# dataset transformations and augmentation\n",
        "\n",
        "batch_size = 4\n",
        "image_size = 224\n",
        "Datatransforms = transforms.Compose([transforms.Resize(image_size),\n",
        "                                       transforms.CenterCrop(image_size),\n",
        "                                       transforms.ToTensor()\n",
        "                                       ,transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])\n",
        "                                     ])                                                  \n",
        "\n",
        "train_data = datasets.ImageFolder(DATADIR + '/train', transform=Datatransforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Classes: \")\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: \n",
            "['XR_ELBOW_negative', 'XR_ELBOW_positive', 'XR_FINGER_negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqPSydfW_I-t",
        "outputId": "17ab7338-dbb8-47e9-f556-b10acbeec347"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 23\n",
              "    Root location: ./gdrive/MyDrive/Thesis/sMURA-v1.1/smallDataset/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C80BSisZ8SCC"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq_qo47Y8dA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d6b824-cd25-4eae-c14a-87ba5eb8b454"
      },
      "source": [
        "inputs, classes = next(iter(trainloader))\n",
        "inputs.shape, classes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 3, 224, 224]), torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBXM2oOW8fCz",
        "outputId": "d946924d-50c1-4faa-9975-23e05f1e9216"
      },
      "source": [
        "classes.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "QgdpfoNR8qQM",
        "outputId": "ae5f5b44-f08d-4470-8929-6fcbd57f819a"
      },
      "source": [
        "\n",
        "show_databatch(inputs, classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDXbvvlMgN68",
        "outputId": "dab1f520-331a-493e-a5cb-75f181727e55"
      },
      "source": [
        "model_names = timm.list_models(pretrained=True)\n",
        "pprint(model_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adv_inception_v3',\n",
            " 'bat_resnext26ts',\n",
            " 'beit_base_patch16_224',\n",
            " 'beit_base_patch16_224_in22k',\n",
            " 'beit_base_patch16_384',\n",
            " 'beit_large_patch16_224',\n",
            " 'beit_large_patch16_224_in22k',\n",
            " 'beit_large_patch16_384',\n",
            " 'beit_large_patch16_512',\n",
            " 'botnet26t_256',\n",
            " 'cait_m36_384',\n",
            " 'cait_m48_448',\n",
            " 'cait_s24_224',\n",
            " 'cait_s24_384',\n",
            " 'cait_s36_384',\n",
            " 'cait_xs24_384',\n",
            " 'cait_xxs24_224',\n",
            " 'cait_xxs24_384',\n",
            " 'cait_xxs36_224',\n",
            " 'cait_xxs36_384',\n",
            " 'coat_lite_mini',\n",
            " 'coat_lite_small',\n",
            " 'coat_lite_tiny',\n",
            " 'coat_mini',\n",
            " 'coat_tiny',\n",
            " 'convit_base',\n",
            " 'convit_small',\n",
            " 'convit_tiny',\n",
            " 'convmixer_768_32',\n",
            " 'convmixer_1024_20_ks9_p14',\n",
            " 'convmixer_1536_20',\n",
            " 'convnext_base',\n",
            " 'convnext_base_384_in22ft1k',\n",
            " 'convnext_base_in22ft1k',\n",
            " 'convnext_base_in22k',\n",
            " 'convnext_large',\n",
            " 'convnext_large_384_in22ft1k',\n",
            " 'convnext_large_in22ft1k',\n",
            " 'convnext_large_in22k',\n",
            " 'convnext_nano',\n",
            " 'convnext_small',\n",
            " 'convnext_small_384_in22ft1k',\n",
            " 'convnext_small_in22ft1k',\n",
            " 'convnext_small_in22k',\n",
            " 'convnext_tiny',\n",
            " 'convnext_tiny_384_in22ft1k',\n",
            " 'convnext_tiny_hnf',\n",
            " 'convnext_tiny_in22ft1k',\n",
            " 'convnext_tiny_in22k',\n",
            " 'convnext_xlarge_384_in22ft1k',\n",
            " 'convnext_xlarge_in22ft1k',\n",
            " 'convnext_xlarge_in22k',\n",
            " 'crossvit_9_240',\n",
            " 'crossvit_9_dagger_240',\n",
            " 'crossvit_15_240',\n",
            " 'crossvit_15_dagger_240',\n",
            " 'crossvit_15_dagger_408',\n",
            " 'crossvit_18_240',\n",
            " 'crossvit_18_dagger_240',\n",
            " 'crossvit_18_dagger_408',\n",
            " 'crossvit_base_240',\n",
            " 'crossvit_small_240',\n",
            " 'crossvit_tiny_240',\n",
            " 'cs3darknet_focus_l',\n",
            " 'cs3darknet_focus_m',\n",
            " 'cs3darknet_l',\n",
            " 'cs3darknet_m',\n",
            " 'cs3darknet_x',\n",
            " 'cs3edgenet_x',\n",
            " 'cs3se_edgenet_x',\n",
            " 'cs3sedarknet_l',\n",
            " 'cs3sedarknet_x',\n",
            " 'cspdarknet53',\n",
            " 'cspresnet50',\n",
            " 'cspresnext50',\n",
            " 'darknet53',\n",
            " 'darknetaa53',\n",
            " 'deit3_base_patch16_224',\n",
            " 'deit3_base_patch16_224_in21ft1k',\n",
            " 'deit3_base_patch16_384',\n",
            " 'deit3_base_patch16_384_in21ft1k',\n",
            " 'deit3_huge_patch14_224',\n",
            " 'deit3_huge_patch14_224_in21ft1k',\n",
            " 'deit3_large_patch16_224',\n",
            " 'deit3_large_patch16_224_in21ft1k',\n",
            " 'deit3_large_patch16_384',\n",
            " 'deit3_large_patch16_384_in21ft1k',\n",
            " 'deit3_small_patch16_224',\n",
            " 'deit3_small_patch16_224_in21ft1k',\n",
            " 'deit3_small_patch16_384',\n",
            " 'deit3_small_patch16_384_in21ft1k',\n",
            " 'deit_base_distilled_patch16_224',\n",
            " 'deit_base_distilled_patch16_384',\n",
            " 'deit_base_patch16_224',\n",
            " 'deit_base_patch16_384',\n",
            " 'deit_small_distilled_patch16_224',\n",
            " 'deit_small_patch16_224',\n",
            " 'deit_tiny_distilled_patch16_224',\n",
            " 'deit_tiny_patch16_224',\n",
            " 'densenet121',\n",
            " 'densenet161',\n",
            " 'densenet169',\n",
            " 'densenet201',\n",
            " 'densenetblur121d',\n",
            " 'dla34',\n",
            " 'dla46_c',\n",
            " 'dla46x_c',\n",
            " 'dla60',\n",
            " 'dla60_res2net',\n",
            " 'dla60_res2next',\n",
            " 'dla60x',\n",
            " 'dla60x_c',\n",
            " 'dla102',\n",
            " 'dla102x',\n",
            " 'dla102x2',\n",
            " 'dla169',\n",
            " 'dm_nfnet_f0',\n",
            " 'dm_nfnet_f1',\n",
            " 'dm_nfnet_f2',\n",
            " 'dm_nfnet_f3',\n",
            " 'dm_nfnet_f4',\n",
            " 'dm_nfnet_f5',\n",
            " 'dm_nfnet_f6',\n",
            " 'dpn68',\n",
            " 'dpn68b',\n",
            " 'dpn92',\n",
            " 'dpn98',\n",
            " 'dpn107',\n",
            " 'dpn131',\n",
            " 'eca_botnext26ts_256',\n",
            " 'eca_halonext26ts',\n",
            " 'eca_nfnet_l0',\n",
            " 'eca_nfnet_l1',\n",
            " 'eca_nfnet_l2',\n",
            " 'eca_resnet33ts',\n",
            " 'eca_resnext26ts',\n",
            " 'ecaresnet26t',\n",
            " 'ecaresnet50d',\n",
            " 'ecaresnet50d_pruned',\n",
            " 'ecaresnet50t',\n",
            " 'ecaresnet101d',\n",
            " 'ecaresnet101d_pruned',\n",
            " 'ecaresnet269d',\n",
            " 'ecaresnetlight',\n",
            " 'edgenext_small',\n",
            " 'edgenext_small_rw',\n",
            " 'edgenext_x_small',\n",
            " 'edgenext_xx_small',\n",
            " 'efficientnet_b0',\n",
            " 'efficientnet_b1',\n",
            " 'efficientnet_b1_pruned',\n",
            " 'efficientnet_b2',\n",
            " 'efficientnet_b2_pruned',\n",
            " 'efficientnet_b3',\n",
            " 'efficientnet_b3_pruned',\n",
            " 'efficientnet_b4',\n",
            " 'efficientnet_el',\n",
            " 'efficientnet_el_pruned',\n",
            " 'efficientnet_em',\n",
            " 'efficientnet_es',\n",
            " 'efficientnet_es_pruned',\n",
            " 'efficientnet_lite0',\n",
            " 'efficientnetv2_rw_m',\n",
            " 'efficientnetv2_rw_s',\n",
            " 'efficientnetv2_rw_t',\n",
            " 'ens_adv_inception_resnet_v2',\n",
            " 'ese_vovnet19b_dw',\n",
            " 'ese_vovnet39b',\n",
            " 'fbnetc_100',\n",
            " 'fbnetv3_b',\n",
            " 'fbnetv3_d',\n",
            " 'fbnetv3_g',\n",
            " 'gc_efficientnetv2_rw_t',\n",
            " 'gcresnet33ts',\n",
            " 'gcresnet50t',\n",
            " 'gcresnext26ts',\n",
            " 'gcresnext50ts',\n",
            " 'gernet_l',\n",
            " 'gernet_m',\n",
            " 'gernet_s',\n",
            " 'ghostnet_100',\n",
            " 'gluon_inception_v3',\n",
            " 'gluon_resnet18_v1b',\n",
            " 'gluon_resnet34_v1b',\n",
            " 'gluon_resnet50_v1b',\n",
            " 'gluon_resnet50_v1c',\n",
            " 'gluon_resnet50_v1d',\n",
            " 'gluon_resnet50_v1s',\n",
            " 'gluon_resnet101_v1b',\n",
            " 'gluon_resnet101_v1c',\n",
            " 'gluon_resnet101_v1d',\n",
            " 'gluon_resnet101_v1s',\n",
            " 'gluon_resnet152_v1b',\n",
            " 'gluon_resnet152_v1c',\n",
            " 'gluon_resnet152_v1d',\n",
            " 'gluon_resnet152_v1s',\n",
            " 'gluon_resnext50_32x4d',\n",
            " 'gluon_resnext101_32x4d',\n",
            " 'gluon_resnext101_64x4d',\n",
            " 'gluon_senet154',\n",
            " 'gluon_seresnext50_32x4d',\n",
            " 'gluon_seresnext101_32x4d',\n",
            " 'gluon_seresnext101_64x4d',\n",
            " 'gluon_xception65',\n",
            " 'gmixer_24_224',\n",
            " 'gmlp_s16_224',\n",
            " 'halo2botnet50ts_256',\n",
            " 'halonet26t',\n",
            " 'halonet50ts',\n",
            " 'haloregnetz_b',\n",
            " 'hardcorenas_a',\n",
            " 'hardcorenas_b',\n",
            " 'hardcorenas_c',\n",
            " 'hardcorenas_d',\n",
            " 'hardcorenas_e',\n",
            " 'hardcorenas_f',\n",
            " 'hrnet_w18',\n",
            " 'hrnet_w18_small',\n",
            " 'hrnet_w18_small_v2',\n",
            " 'hrnet_w30',\n",
            " 'hrnet_w32',\n",
            " 'hrnet_w40',\n",
            " 'hrnet_w44',\n",
            " 'hrnet_w48',\n",
            " 'hrnet_w64',\n",
            " 'ig_resnext101_32x8d',\n",
            " 'ig_resnext101_32x16d',\n",
            " 'ig_resnext101_32x32d',\n",
            " 'ig_resnext101_32x48d',\n",
            " 'inception_resnet_v2',\n",
            " 'inception_v3',\n",
            " 'inception_v4',\n",
            " 'jx_nest_base',\n",
            " 'jx_nest_small',\n",
            " 'jx_nest_tiny',\n",
            " 'lambda_resnet26rpt_256',\n",
            " 'lambda_resnet26t',\n",
            " 'lambda_resnet50ts',\n",
            " 'lamhalobotnet50ts_256',\n",
            " 'lcnet_050',\n",
            " 'lcnet_075',\n",
            " 'lcnet_100',\n",
            " 'legacy_senet154',\n",
            " 'legacy_seresnet18',\n",
            " 'legacy_seresnet34',\n",
            " 'legacy_seresnet50',\n",
            " 'legacy_seresnet101',\n",
            " 'legacy_seresnet152',\n",
            " 'legacy_seresnext26_32x4d',\n",
            " 'legacy_seresnext50_32x4d',\n",
            " 'legacy_seresnext101_32x4d',\n",
            " 'levit_128',\n",
            " 'levit_128s',\n",
            " 'levit_192',\n",
            " 'levit_256',\n",
            " 'levit_384',\n",
            " 'mixer_b16_224',\n",
            " 'mixer_b16_224_in21k',\n",
            " 'mixer_b16_224_miil',\n",
            " 'mixer_b16_224_miil_in21k',\n",
            " 'mixer_l16_224',\n",
            " 'mixer_l16_224_in21k',\n",
            " 'mixnet_l',\n",
            " 'mixnet_m',\n",
            " 'mixnet_s',\n",
            " 'mixnet_xl',\n",
            " 'mnasnet_100',\n",
            " 'mnasnet_small',\n",
            " 'mobilenetv2_050',\n",
            " 'mobilenetv2_100',\n",
            " 'mobilenetv2_110d',\n",
            " 'mobilenetv2_120d',\n",
            " 'mobilenetv2_140',\n",
            " 'mobilenetv3_large_100',\n",
            " 'mobilenetv3_large_100_miil',\n",
            " 'mobilenetv3_large_100_miil_in21k',\n",
            " 'mobilenetv3_rw',\n",
            " 'mobilenetv3_small_050',\n",
            " 'mobilenetv3_small_075',\n",
            " 'mobilenetv3_small_100',\n",
            " 'mobilevit_s',\n",
            " 'mobilevit_xs',\n",
            " 'mobilevit_xxs',\n",
            " 'mobilevitv2_050',\n",
            " 'mobilevitv2_075',\n",
            " 'mobilevitv2_100',\n",
            " 'mobilevitv2_125',\n",
            " 'mobilevitv2_150',\n",
            " 'mobilevitv2_150_384_in22ft1k',\n",
            " 'mobilevitv2_150_in22ft1k',\n",
            " 'mobilevitv2_175',\n",
            " 'mobilevitv2_175_384_in22ft1k',\n",
            " 'mobilevitv2_175_in22ft1k',\n",
            " 'mobilevitv2_200',\n",
            " 'mobilevitv2_200_384_in22ft1k',\n",
            " 'mobilevitv2_200_in22ft1k',\n",
            " 'nasnetalarge',\n",
            " 'nf_regnet_b1',\n",
            " 'nf_resnet50',\n",
            " 'nfnet_l0',\n",
            " 'pit_b_224',\n",
            " 'pit_b_distilled_224',\n",
            " 'pit_s_224',\n",
            " 'pit_s_distilled_224',\n",
            " 'pit_ti_224',\n",
            " 'pit_ti_distilled_224',\n",
            " 'pit_xs_224',\n",
            " 'pit_xs_distilled_224',\n",
            " 'pnasnet5large',\n",
            " 'poolformer_m36',\n",
            " 'poolformer_m48',\n",
            " 'poolformer_s12',\n",
            " 'poolformer_s24',\n",
            " 'poolformer_s36',\n",
            " 'regnetv_040',\n",
            " 'regnetv_064',\n",
            " 'regnetx_002',\n",
            " 'regnetx_004',\n",
            " 'regnetx_006',\n",
            " 'regnetx_008',\n",
            " 'regnetx_016',\n",
            " 'regnetx_032',\n",
            " 'regnetx_040',\n",
            " 'regnetx_064',\n",
            " 'regnetx_080',\n",
            " 'regnetx_120',\n",
            " 'regnetx_160',\n",
            " 'regnetx_320',\n",
            " 'regnety_002',\n",
            " 'regnety_004',\n",
            " 'regnety_006',\n",
            " 'regnety_008',\n",
            " 'regnety_016',\n",
            " 'regnety_032',\n",
            " 'regnety_040',\n",
            " 'regnety_064',\n",
            " 'regnety_080',\n",
            " 'regnety_120',\n",
            " 'regnety_160',\n",
            " 'regnety_320',\n",
            " 'regnetz_040',\n",
            " 'regnetz_040h',\n",
            " 'regnetz_b16',\n",
            " 'regnetz_c16',\n",
            " 'regnetz_c16_evos',\n",
            " 'regnetz_d8',\n",
            " 'regnetz_d8_evos',\n",
            " 'regnetz_d32',\n",
            " 'regnetz_e8',\n",
            " 'repvgg_a2',\n",
            " 'repvgg_b0',\n",
            " 'repvgg_b1',\n",
            " 'repvgg_b1g4',\n",
            " 'repvgg_b2',\n",
            " 'repvgg_b2g4',\n",
            " 'repvgg_b3',\n",
            " 'repvgg_b3g4',\n",
            " 'res2net50_14w_8s',\n",
            " 'res2net50_26w_4s',\n",
            " 'res2net50_26w_6s',\n",
            " 'res2net50_26w_8s',\n",
            " 'res2net50_48w_2s',\n",
            " 'res2net101_26w_4s',\n",
            " 'res2next50',\n",
            " 'resmlp_12_224',\n",
            " 'resmlp_12_224_dino',\n",
            " 'resmlp_12_distilled_224',\n",
            " 'resmlp_24_224',\n",
            " 'resmlp_24_224_dino',\n",
            " 'resmlp_24_distilled_224',\n",
            " 'resmlp_36_224',\n",
            " 'resmlp_36_distilled_224',\n",
            " 'resmlp_big_24_224',\n",
            " 'resmlp_big_24_224_in22ft1k',\n",
            " 'resmlp_big_24_distilled_224',\n",
            " 'resnest14d',\n",
            " 'resnest26d',\n",
            " 'resnest50d',\n",
            " 'resnest50d_1s4x24d',\n",
            " 'resnest50d_4s2x40d',\n",
            " 'resnest101e',\n",
            " 'resnest200e',\n",
            " 'resnest269e',\n",
            " 'resnet10t',\n",
            " 'resnet14t',\n",
            " 'resnet18',\n",
            " 'resnet18d',\n",
            " 'resnet26',\n",
            " 'resnet26d',\n",
            " 'resnet26t',\n",
            " 'resnet32ts',\n",
            " 'resnet33ts',\n",
            " 'resnet34',\n",
            " 'resnet34d',\n",
            " 'resnet50',\n",
            " 'resnet50_gn',\n",
            " 'resnet50d',\n",
            " 'resnet51q',\n",
            " 'resnet61q',\n",
            " 'resnet101',\n",
            " 'resnet101d',\n",
            " 'resnet152',\n",
            " 'resnet152d',\n",
            " 'resnet200d',\n",
            " 'resnetaa50',\n",
            " 'resnetblur50',\n",
            " 'resnetrs50',\n",
            " 'resnetrs101',\n",
            " 'resnetrs152',\n",
            " 'resnetrs200',\n",
            " 'resnetrs270',\n",
            " 'resnetrs350',\n",
            " 'resnetrs420',\n",
            " 'resnetv2_50',\n",
            " 'resnetv2_50d_evos',\n",
            " 'resnetv2_50d_gn',\n",
            " 'resnetv2_50x1_bit_distilled',\n",
            " 'resnetv2_50x1_bitm',\n",
            " 'resnetv2_50x1_bitm_in21k',\n",
            " 'resnetv2_50x3_bitm',\n",
            " 'resnetv2_50x3_bitm_in21k',\n",
            " 'resnetv2_101',\n",
            " 'resnetv2_101x1_bitm',\n",
            " 'resnetv2_101x1_bitm_in21k',\n",
            " 'resnetv2_101x3_bitm',\n",
            " 'resnetv2_101x3_bitm_in21k',\n",
            " 'resnetv2_152x2_bit_teacher',\n",
            " 'resnetv2_152x2_bit_teacher_384',\n",
            " 'resnetv2_152x2_bitm',\n",
            " 'resnetv2_152x2_bitm_in21k',\n",
            " 'resnetv2_152x4_bitm',\n",
            " 'resnetv2_152x4_bitm_in21k',\n",
            " 'resnext26ts',\n",
            " 'resnext50_32x4d',\n",
            " 'resnext50d_32x4d',\n",
            " 'resnext101_32x8d',\n",
            " 'resnext101_64x4d',\n",
            " 'rexnet_100',\n",
            " 'rexnet_130',\n",
            " 'rexnet_150',\n",
            " 'rexnet_200',\n",
            " 'sebotnet33ts_256',\n",
            " 'sehalonet33ts',\n",
            " 'selecsls42b',\n",
            " 'selecsls60',\n",
            " 'selecsls60b',\n",
            " 'semnasnet_075',\n",
            " 'semnasnet_100',\n",
            " 'sequencer2d_l',\n",
            " 'sequencer2d_m',\n",
            " 'sequencer2d_s',\n",
            " 'seresnet33ts',\n",
            " 'seresnet50',\n",
            " 'seresnet152d',\n",
            " 'seresnext26d_32x4d',\n",
            " 'seresnext26t_32x4d',\n",
            " 'seresnext26ts',\n",
            " 'seresnext50_32x4d',\n",
            " 'seresnext101_32x8d',\n",
            " 'seresnext101d_32x8d',\n",
            " 'seresnextaa101d_32x8d',\n",
            " 'skresnet18',\n",
            " 'skresnet34',\n",
            " 'skresnext50_32x4d',\n",
            " 'spnasnet_100',\n",
            " 'ssl_resnet18',\n",
            " 'ssl_resnet50',\n",
            " 'ssl_resnext50_32x4d',\n",
            " 'ssl_resnext101_32x4d',\n",
            " 'ssl_resnext101_32x8d',\n",
            " 'ssl_resnext101_32x16d',\n",
            " 'swin_base_patch4_window7_224',\n",
            " 'swin_base_patch4_window7_224_in22k',\n",
            " 'swin_base_patch4_window12_384',\n",
            " 'swin_base_patch4_window12_384_in22k',\n",
            " 'swin_large_patch4_window7_224',\n",
            " 'swin_large_patch4_window7_224_in22k',\n",
            " 'swin_large_patch4_window12_384',\n",
            " 'swin_large_patch4_window12_384_in22k',\n",
            " 'swin_s3_base_224',\n",
            " 'swin_s3_small_224',\n",
            " 'swin_s3_tiny_224',\n",
            " 'swin_small_patch4_window7_224',\n",
            " 'swin_tiny_patch4_window7_224',\n",
            " 'swinv2_base_window8_256',\n",
            " 'swinv2_base_window12_192_22k',\n",
            " 'swinv2_base_window12to16_192to256_22kft1k',\n",
            " 'swinv2_base_window12to24_192to384_22kft1k',\n",
            " 'swinv2_base_window16_256',\n",
            " 'swinv2_cr_small_224',\n",
            " 'swinv2_cr_small_ns_224',\n",
            " 'swinv2_cr_tiny_ns_224',\n",
            " 'swinv2_large_window12_192_22k',\n",
            " 'swinv2_large_window12to16_192to256_22kft1k',\n",
            " 'swinv2_large_window12to24_192to384_22kft1k',\n",
            " 'swinv2_small_window8_256',\n",
            " 'swinv2_small_window16_256',\n",
            " 'swinv2_tiny_window8_256',\n",
            " 'swinv2_tiny_window16_256',\n",
            " 'swsl_resnet18',\n",
            " 'swsl_resnet50',\n",
            " 'swsl_resnext50_32x4d',\n",
            " 'swsl_resnext101_32x4d',\n",
            " 'swsl_resnext101_32x8d',\n",
            " 'swsl_resnext101_32x16d',\n",
            " 'tf_efficientnet_b0',\n",
            " 'tf_efficientnet_b0_ap',\n",
            " 'tf_efficientnet_b0_ns',\n",
            " 'tf_efficientnet_b1',\n",
            " 'tf_efficientnet_b1_ap',\n",
            " 'tf_efficientnet_b1_ns',\n",
            " 'tf_efficientnet_b2',\n",
            " 'tf_efficientnet_b2_ap',\n",
            " 'tf_efficientnet_b2_ns',\n",
            " 'tf_efficientnet_b3',\n",
            " 'tf_efficientnet_b3_ap',\n",
            " 'tf_efficientnet_b3_ns',\n",
            " 'tf_efficientnet_b4',\n",
            " 'tf_efficientnet_b4_ap',\n",
            " 'tf_efficientnet_b4_ns',\n",
            " 'tf_efficientnet_b5',\n",
            " 'tf_efficientnet_b5_ap',\n",
            " 'tf_efficientnet_b5_ns',\n",
            " 'tf_efficientnet_b6',\n",
            " 'tf_efficientnet_b6_ap',\n",
            " 'tf_efficientnet_b6_ns',\n",
            " 'tf_efficientnet_b7',\n",
            " 'tf_efficientnet_b7_ap',\n",
            " 'tf_efficientnet_b7_ns',\n",
            " 'tf_efficientnet_b8',\n",
            " 'tf_efficientnet_b8_ap',\n",
            " 'tf_efficientnet_cc_b0_4e',\n",
            " 'tf_efficientnet_cc_b0_8e',\n",
            " 'tf_efficientnet_cc_b1_8e',\n",
            " 'tf_efficientnet_el',\n",
            " 'tf_efficientnet_em',\n",
            " 'tf_efficientnet_es',\n",
            " 'tf_efficientnet_l2_ns',\n",
            " 'tf_efficientnet_l2_ns_475',\n",
            " 'tf_efficientnet_lite0',\n",
            " 'tf_efficientnet_lite1',\n",
            " 'tf_efficientnet_lite2',\n",
            " 'tf_efficientnet_lite3',\n",
            " 'tf_efficientnet_lite4',\n",
            " 'tf_efficientnetv2_b0',\n",
            " 'tf_efficientnetv2_b1',\n",
            " 'tf_efficientnetv2_b2',\n",
            " 'tf_efficientnetv2_b3',\n",
            " 'tf_efficientnetv2_l',\n",
            " 'tf_efficientnetv2_l_in21ft1k',\n",
            " 'tf_efficientnetv2_l_in21k',\n",
            " 'tf_efficientnetv2_m',\n",
            " 'tf_efficientnetv2_m_in21ft1k',\n",
            " 'tf_efficientnetv2_m_in21k',\n",
            " 'tf_efficientnetv2_s',\n",
            " 'tf_efficientnetv2_s_in21ft1k',\n",
            " 'tf_efficientnetv2_s_in21k',\n",
            " 'tf_efficientnetv2_xl_in21ft1k',\n",
            " 'tf_efficientnetv2_xl_in21k',\n",
            " 'tf_inception_v3',\n",
            " 'tf_mixnet_l',\n",
            " 'tf_mixnet_m',\n",
            " 'tf_mixnet_s',\n",
            " 'tf_mobilenetv3_large_075',\n",
            " 'tf_mobilenetv3_large_100',\n",
            " 'tf_mobilenetv3_large_minimal_100',\n",
            " 'tf_mobilenetv3_small_075',\n",
            " 'tf_mobilenetv3_small_100',\n",
            " 'tf_mobilenetv3_small_minimal_100',\n",
            " 'tinynet_a',\n",
            " 'tinynet_b',\n",
            " 'tinynet_c',\n",
            " 'tinynet_d',\n",
            " 'tinynet_e',\n",
            " 'tnt_s_patch16_224',\n",
            " 'tresnet_l',\n",
            " 'tresnet_l_448',\n",
            " 'tresnet_m',\n",
            " 'tresnet_m_448',\n",
            " 'tresnet_m_miil_in21k',\n",
            " 'tresnet_xl',\n",
            " 'tresnet_xl_448',\n",
            " 'tv_densenet121',\n",
            " 'tv_resnet34',\n",
            " 'tv_resnet50',\n",
            " 'tv_resnet101',\n",
            " 'tv_resnet152',\n",
            " 'tv_resnext50_32x4d',\n",
            " 'twins_pcpvt_base',\n",
            " 'twins_pcpvt_large',\n",
            " 'twins_pcpvt_small',\n",
            " 'twins_svt_base',\n",
            " 'twins_svt_large',\n",
            " 'twins_svt_small',\n",
            " 'vgg11',\n",
            " 'vgg11_bn',\n",
            " 'vgg13',\n",
            " 'vgg13_bn',\n",
            " 'vgg16',\n",
            " 'vgg16_bn',\n",
            " 'vgg19',\n",
            " 'vgg19_bn',\n",
            " 'visformer_small',\n",
            " 'vit_base_patch8_224',\n",
            " 'vit_base_patch8_224_dino',\n",
            " 'vit_base_patch8_224_in21k',\n",
            " 'vit_base_patch16_224',\n",
            " 'vit_base_patch16_224_dino',\n",
            " 'vit_base_patch16_224_in21k',\n",
            " 'vit_base_patch16_224_miil',\n",
            " 'vit_base_patch16_224_miil_in21k',\n",
            " 'vit_base_patch16_224_sam',\n",
            " 'vit_base_patch16_384',\n",
            " 'vit_base_patch16_rpn_224',\n",
            " 'vit_base_patch32_224',\n",
            " 'vit_base_patch32_224_in21k',\n",
            " 'vit_base_patch32_224_sam',\n",
            " 'vit_base_patch32_384',\n",
            " 'vit_base_r50_s16_224_in21k',\n",
            " 'vit_base_r50_s16_384',\n",
            " 'vit_huge_patch14_224_in21k',\n",
            " 'vit_large_patch16_224',\n",
            " 'vit_large_patch16_224_in21k',\n",
            " 'vit_large_patch16_384',\n",
            " 'vit_large_patch32_224_in21k',\n",
            " 'vit_large_patch32_384',\n",
            " 'vit_large_r50_s32_224',\n",
            " 'vit_large_r50_s32_224_in21k',\n",
            " 'vit_large_r50_s32_384',\n",
            " 'vit_relpos_base_patch16_224',\n",
            " 'vit_relpos_base_patch16_clsgap_224',\n",
            " 'vit_relpos_base_patch32_plus_rpn_256',\n",
            " 'vit_relpos_medium_patch16_224',\n",
            " 'vit_relpos_medium_patch16_cls_224',\n",
            " 'vit_relpos_medium_patch16_rpn_224',\n",
            " 'vit_relpos_small_patch16_224',\n",
            " 'vit_small_patch8_224_dino',\n",
            " 'vit_small_patch16_224',\n",
            " 'vit_small_patch16_224_dino',\n",
            " 'vit_small_patch16_224_in21k',\n",
            " 'vit_small_patch16_384',\n",
            " 'vit_small_patch32_224',\n",
            " 'vit_small_patch32_224_in21k',\n",
            " 'vit_small_patch32_384',\n",
            " 'vit_small_r26_s32_224',\n",
            " 'vit_small_r26_s32_224_in21k',\n",
            " 'vit_small_r26_s32_384',\n",
            " 'vit_srelpos_medium_patch16_224',\n",
            " 'vit_srelpos_small_patch16_224',\n",
            " 'vit_tiny_patch16_224',\n",
            " 'vit_tiny_patch16_224_in21k',\n",
            " 'vit_tiny_patch16_384',\n",
            " 'vit_tiny_r_s16_p8_224',\n",
            " 'vit_tiny_r_s16_p8_224_in21k',\n",
            " 'vit_tiny_r_s16_p8_384',\n",
            " 'volo_d1_224',\n",
            " 'volo_d1_384',\n",
            " 'volo_d2_224',\n",
            " 'volo_d2_384',\n",
            " 'volo_d3_224',\n",
            " 'volo_d3_448',\n",
            " 'volo_d4_224',\n",
            " 'volo_d4_448',\n",
            " 'volo_d5_224',\n",
            " 'volo_d5_448',\n",
            " 'volo_d5_512',\n",
            " 'wide_resnet50_2',\n",
            " 'wide_resnet101_2',\n",
            " 'xception',\n",
            " 'xception41',\n",
            " 'xception41p',\n",
            " 'xception65',\n",
            " 'xception65p',\n",
            " 'xception71',\n",
            " 'xcit_large_24_p8_224',\n",
            " 'xcit_large_24_p8_224_dist',\n",
            " 'xcit_large_24_p8_384_dist',\n",
            " 'xcit_large_24_p16_224',\n",
            " 'xcit_large_24_p16_224_dist',\n",
            " 'xcit_large_24_p16_384_dist',\n",
            " 'xcit_medium_24_p8_224',\n",
            " 'xcit_medium_24_p8_224_dist',\n",
            " 'xcit_medium_24_p8_384_dist',\n",
            " 'xcit_medium_24_p16_224',\n",
            " 'xcit_medium_24_p16_224_dist',\n",
            " 'xcit_medium_24_p16_384_dist',\n",
            " 'xcit_nano_12_p8_224',\n",
            " 'xcit_nano_12_p8_224_dist',\n",
            " 'xcit_nano_12_p8_384_dist',\n",
            " 'xcit_nano_12_p16_224',\n",
            " 'xcit_nano_12_p16_224_dist',\n",
            " 'xcit_nano_12_p16_384_dist',\n",
            " 'xcit_small_12_p8_224',\n",
            " 'xcit_small_12_p8_224_dist',\n",
            " 'xcit_small_12_p8_384_dist',\n",
            " 'xcit_small_12_p16_224',\n",
            " 'xcit_small_12_p16_224_dist',\n",
            " 'xcit_small_12_p16_384_dist',\n",
            " 'xcit_small_24_p8_224',\n",
            " 'xcit_small_24_p8_224_dist',\n",
            " 'xcit_small_24_p8_384_dist',\n",
            " 'xcit_small_24_p16_224',\n",
            " 'xcit_small_24_p16_224_dist',\n",
            " 'xcit_small_24_p16_384_dist',\n",
            " 'xcit_tiny_12_p8_224',\n",
            " 'xcit_tiny_12_p8_224_dist',\n",
            " 'xcit_tiny_12_p8_384_dist',\n",
            " 'xcit_tiny_12_p16_224',\n",
            " 'xcit_tiny_12_p16_224_dist',\n",
            " 'xcit_tiny_12_p16_384_dist',\n",
            " 'xcit_tiny_24_p8_224',\n",
            " 'xcit_tiny_24_p8_224_dist',\n",
            " 'xcit_tiny_24_p8_384_dist',\n",
            " 'xcit_tiny_24_p16_224',\n",
            " 'xcit_tiny_24_p16_224_dist',\n",
            " 'xcit_tiny_24_p16_384_dist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyModel(torch.nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(MyModel, self).__init__()\n",
        "\n",
        "#         google_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "#         for param in google_model.parameters():\n",
        "#           param.requires_grad = False\n",
        "\n",
        "#         num_features = google_model.classifier.in_features\n",
        "#         newClassifierLayers = [nn.Linear(num_features, 512) , nn.Tanh() ]\n",
        "#         newClassifierLayers.extend([nn.Linear(512, len(class_names))])\n",
        "#         newClassifierLayers\n",
        "#         google_model.classifier = nn.Sequential(*newClassifierLayers)\n",
        "#         self.google_model = google_model\n",
        "\n",
        "#         timm_model = timm.create_model('vit_large_r50_s32_224_in21k', pretrained=True)\n",
        "\n",
        "#         for param in timm_model.parameters():\n",
        "#           param.requires_grad = False\n",
        "\n",
        "#         num_features = timm_model.head.in_features\n",
        "#         newClassifierLayers = [nn.Linear(num_features, 512) , nn.Tanh() ]\n",
        "#         newClassifierLayers.extend([nn.Linear(512, len(class_names))])\n",
        "#         newClassifierLayers\n",
        "#         timm_model.head = nn.Sequential(*newClassifierLayers)\n",
        "#         self.timm_model = timm_model\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         p = self.google_model(x)\n",
        "#         p = p.logits\n",
        "#         q = self.timm_model(x)\n",
        "        \n",
        "#         return (p+q)/2"
      ],
      "metadata": {
        "id": "xwxWIiW6oxo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timm_model = timm.create_model('vit_large_r50_s32_224_in21k', pretrained=True)"
      ],
      "metadata": {
        "id": "Z4fuX96pc7Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timm_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d0ul3wcc8Xc",
        "outputId": "9507f50a-d4e5-4a90-d4e4-bf68469cd8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): HybridEmbed(\n",
              "    (backbone): ResNetV2(\n",
              "      (stem): Sequential(\n",
              "        (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
              "        (norm): GroupNormAct(\n",
              "          32, 64, eps=1e-05, affine=True\n",
              "          (drop): Identity()\n",
              "          (act): ReLU(inplace=True)\n",
              "        )\n",
              "        (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "      )\n",
              "      (stages): Sequential(\n",
              "        (0): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (3): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (3): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (4): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (5): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 2048, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm): Identity()\n",
              "      (head): ClassifierHead(\n",
              "        (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
              "        (fc): Identity()\n",
              "        (flatten): Identity()\n",
              "      )\n",
              "    )\n",
              "    (proj): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (12): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (13): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (14): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (15): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (16): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (17): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (18): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (19): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (20): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (21): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (22): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (23): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head): Linear(in_features=1024, out_features=21843, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        timm_model = timm.create_model('vit_large_r50_s32_224_in21k', pretrained=True)\n",
        "\n",
        "        for param in timm_model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        num_features = timm_model.head.in_features\n",
        "        newClassifierLayers = [nn.Linear(num_features, 512) , nn.Tanh() ]   #use either sigmoid or tanh\n",
        "        newClassifierLayers.extend([nn.Linear(512, len(class_names))])\n",
        "        newClassifierLayers\n",
        "        timm_model.head = nn.Sequential(*newClassifierLayers)\n",
        "        self.timm_model = timm_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.timm_model(x)\n",
        "        \n",
        "        return q"
      ],
      "metadata": {
        "id": "mUCIK9Y-cvNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()"
      ],
      "metadata": {
        "id": "aLzHt7XOs54Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdFP3gUktAH1",
        "outputId": "d09153b5-eb0f-4238-adb1-192618b6fdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (timm_model): VisionTransformer(\n",
              "    (patch_embed): HybridEmbed(\n",
              "      (backbone): ResNetV2(\n",
              "        (stem): Sequential(\n",
              "          (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
              "          (norm): GroupNormAct(\n",
              "            32, 64, eps=1e-05, affine=True\n",
              "            (drop): Identity()\n",
              "            (act): ReLU(inplace=True)\n",
              "          )\n",
              "          (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "        )\n",
              "        (stages): Sequential(\n",
              "          (0): ResNetStage(\n",
              "            (blocks): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (downsample): DownsampleConv(\n",
              "                  (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (norm): GroupNormAct(\n",
              "                    32, 256, eps=1e-05, affine=True\n",
              "                    (drop): Identity()\n",
              "                    (act): Identity()\n",
              "                  )\n",
              "                )\n",
              "                (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (2): Bottleneck(\n",
              "                (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 64, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): ResNetStage(\n",
              "            (blocks): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (downsample): DownsampleConv(\n",
              "                  (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                  (norm): GroupNormAct(\n",
              "                    32, 512, eps=1e-05, affine=True\n",
              "                    (drop): Identity()\n",
              "                    (act): Identity()\n",
              "                  )\n",
              "                )\n",
              "                (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (2): Bottleneck(\n",
              "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (3): Bottleneck(\n",
              "                (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 128, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (2): ResNetStage(\n",
              "            (blocks): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (downsample): DownsampleConv(\n",
              "                  (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                  (norm): GroupNormAct(\n",
              "                    32, 1024, eps=1e-05, affine=True\n",
              "                    (drop): Identity()\n",
              "                    (act): Identity()\n",
              "                  )\n",
              "                )\n",
              "                (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (2): Bottleneck(\n",
              "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (3): Bottleneck(\n",
              "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (4): Bottleneck(\n",
              "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (5): Bottleneck(\n",
              "                (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (3): ResNetStage(\n",
              "            (blocks): Sequential(\n",
              "              (0): Bottleneck(\n",
              "                (downsample): DownsampleConv(\n",
              "                  (conv): StdConv2dSame(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                  (norm): GroupNormAct(\n",
              "                    32, 2048, eps=1e-05, affine=True\n",
              "                    (drop): Identity()\n",
              "                    (act): Identity()\n",
              "                  )\n",
              "                )\n",
              "                (conv1): StdConv2dSame(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 2048, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (1): Bottleneck(\n",
              "                (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 2048, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "              (2): Bottleneck(\n",
              "                (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm1): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                (norm2): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): ReLU(inplace=True)\n",
              "                )\n",
              "                (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm3): GroupNormAct(\n",
              "                  32, 2048, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "                (drop_path): Identity()\n",
              "                (act3): ReLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (norm): Identity()\n",
              "        (head): ClassifierHead(\n",
              "          (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
              "          (fc): Identity()\n",
              "          (flatten): Identity()\n",
              "        )\n",
              "      )\n",
              "      (proj): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): Sequential(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (12): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (13): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (14): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (15): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (16): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (17): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (18): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (19): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (20): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (21): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (22): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "      (23): Block(\n",
              "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls1): Identity()\n",
              "        (drop_path1): Identity()\n",
              "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate=none)\n",
              "          (drop1): Dropout(p=0.0, inplace=False)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ls2): Identity()\n",
              "        (drop_path2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "    (fc_norm): Identity()\n",
              "    (head): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=512, out_features=3, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1lL3U7gYZ4B"
      },
      "source": [
        "# define model architecture topology ect\n",
        "# model's hyper params like learning rate and momentum\n",
        "# loss criterion and optimizer\n",
        "# model name to save model's state on disk\n",
        "\n",
        "model_name = 'google_and_timm'\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.7\n",
        "def create_model():\n",
        "  model = MyModel()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  return model, criterion, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, criterion, optimizer = create_model()"
      ],
      "metadata": {
        "id": "4HL3MzyATzLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w9qKfDnk6_L",
        "outputId": "cf795c17-5f36-4454-e0b1-6eb87738aae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3748,  0.0492],\n",
              "        [-0.3180,  0.4942],\n",
              "        [ 0.2067,  0.4085],\n",
              "        [-0.1936,  0.2281]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ORM8J002k_VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCDwMLlAoz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFBajEnLowNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8d25DKt_l5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from disk if it's already saved\n",
        "# model.load_state_dict(torch.load(DATADIR+'/'+model_name+'.pth'))"
      ],
      "metadata": {
        "id": "-5WSNZZPJ5lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP4NEXGXT5TH",
        "outputId": "32ba6437-1f94-45a9-ae00-fabf588cc6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): HybridEmbed(\n",
              "    (backbone): ResNetV2(\n",
              "      (stem): Sequential(\n",
              "        (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
              "        (norm): GroupNormAct(\n",
              "          32, 64, eps=1e-05, affine=True\n",
              "          (drop): Identity()\n",
              "          (act): ReLU(inplace=True)\n",
              "        )\n",
              "        (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "      )\n",
              "      (stages): Sequential(\n",
              "        (0): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 256, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 64, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 512, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (3): Bottleneck(\n",
              "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 128, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 1024, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (3): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (4): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (5): Bottleneck(\n",
              "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 256, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 1024, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ResNetStage(\n",
              "          (blocks): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (downsample): DownsampleConv(\n",
              "                (conv): StdConv2dSame(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (norm): GroupNormAct(\n",
              "                  32, 2048, eps=1e-05, affine=True\n",
              "                  (drop): Identity()\n",
              "                  (act): Identity()\n",
              "                )\n",
              "              )\n",
              "              (conv1): StdConv2dSame(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "              (norm2): GroupNormAct(\n",
              "                32, 512, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "              )\n",
              "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm3): GroupNormAct(\n",
              "                32, 2048, eps=1e-05, affine=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (drop_path): Identity()\n",
              "              (act3): ReLU(inplace=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm): Identity()\n",
              "      (head): ClassifierHead(\n",
              "        (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
              "        (fc): Identity()\n",
              "        (flatten): Identity()\n",
              "      )\n",
              "    )\n",
              "    (proj): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (12): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (13): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (14): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (15): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (16): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (17): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (18): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (19): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (20): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (21): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (22): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (23): Block(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate=none)\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Classifier(train_data, class_names, create_model, DATADIR+\"/\"+model_name+'.pth')"
      ],
      "metadata": {
        "id": "UK8mQwkmaoQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopper(2, 0.1)\n",
        "valid_losses = classifier.kfold(10, 5, batch_size, None)  #(epochs, ? , _ , early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2YpHWtf2nF",
        "outputId": "67251c46-b07a-4014-9a83-10e98f494d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold:0 ======================================\n",
            "Epoch: 0; Training Loss: 0.7243053592102868; Running Accuracy: 0.5787671232876712; Time Taken: 103.32270956039429\n",
            "validation loss: 0.5248399966142394, validation accuracy: 0.8409090909090909\n",
            "Epoch: 1; Training Loss: 0.42336960664817264; Running Accuracy: 0.7077625570776256; Time Taken: 105.59633922576904\n",
            "validation loss: 0.41164860569617967, validation accuracy: 0.8806818181818182\n",
            "Epoch: 2; Training Loss: 0.3101960534283093; Running Accuracy: 0.7374429223744292; Time Taken: 105.70041632652283\n",
            "validation loss: 0.3857660510323264, validation accuracy: 0.8693181818181818\n",
            "Epoch: 3; Training Loss: 0.24730721939887318; Running Accuracy: 0.7511415525114156; Time Taken: 106.00595235824585\n",
            "validation loss: 0.31876374362036586, validation accuracy: 0.9147727272727273\n",
            "Epoch: 4; Training Loss: 0.19949501653867108; Running Accuracy: 0.7625570776255708; Time Taken: 105.37163949012756\n",
            "validation loss: 0.2928704016587951, validation accuracy: 0.9090909090909091\n",
            "Epoch: 5; Training Loss: 0.1669299633481673; Running Accuracy: 0.7682648401826484; Time Taken: 104.51485204696655\n",
            "validation loss: 0.28222766340794886, validation accuracy: 0.8977272727272727\n",
            "Epoch: 6; Training Loss: 0.13993154520434992; Running Accuracy: 0.7785388127853882; Time Taken: 104.74837493896484\n",
            "validation loss: 0.265471558933231, validation accuracy: 0.9147727272727273\n",
            "Epoch: 7; Training Loss: 0.12116335947066545; Running Accuracy: 0.7785388127853882; Time Taken: 104.98604226112366\n",
            "validation loss: 0.26375206966291775, validation accuracy: 0.9204545454545454\n",
            "Epoch: 8; Training Loss: 0.10469210308577333; Running Accuracy: 0.7831050228310502; Time Taken: 104.55267119407654\n",
            "validation loss: 0.2543316566893323, validation accuracy: 0.9147727272727273\n",
            "Epoch: 9; Training Loss: 0.09267035680689982; Running Accuracy: 0.7865296803652968; Time Taken: 104.66395688056946\n",
            "validation loss: 0.25744392492131074, validation accuracy: 0.9204545454545454\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "healthy_wheat       0.91      0.91      0.91        33\n",
            "    leaf_rust       0.87      0.97      0.92        67\n",
            "    stem_rust       0.99      0.88      0.93        76\n",
            "\n",
            "     accuracy                           0.92       176\n",
            "    macro avg       0.92      0.92      0.92       176\n",
            " weighted avg       0.93      0.92      0.92       176\n",
            "\n",
            "fold:1 ======================================\n",
            "Epoch: 0; Training Loss: 0.7331518513912504; Running Accuracy: 0.5742009132420092; Time Taken: 107.68325543403625\n",
            "validation loss: 0.5077405714175918, validation accuracy: 0.8857142857142857\n",
            "Epoch: 1; Training Loss: 0.43484761472791433; Running Accuracy: 0.6974885844748858; Time Taken: 107.19889211654663\n",
            "validation loss: 0.36824879541315814, validation accuracy: 0.9314285714285714\n",
            "Epoch: 2; Training Loss: 0.3186792268375443; Running Accuracy: 0.7317351598173516; Time Taken: 105.73433685302734\n",
            "validation loss: 0.3080418377437375, validation accuracy: 0.9371428571428572\n",
            "Epoch: 3; Training Loss: 0.2487919147295708; Running Accuracy: 0.75; Time Taken: 105.84154891967773\n",
            "validation loss: 0.26854226958345284, validation accuracy: 0.9428571428571428\n",
            "Epoch: 4; Training Loss: 0.20080585663460873; Running Accuracy: 0.7648401826484018; Time Taken: 105.23846673965454\n",
            "validation loss: 0.24549651789394292, validation accuracy: 0.9485714285714286\n",
            "Epoch: 5; Training Loss: 0.16432994804133408; Running Accuracy: 0.7751141552511416; Time Taken: 106.16242051124573\n",
            "validation loss: 0.23403724570843307, validation accuracy: 0.9428571428571428\n",
            "Epoch: 6; Training Loss: 0.13694887383396484; Running Accuracy: 0.7773972602739726; Time Taken: 104.32595992088318\n",
            "validation loss: 0.22749778795564038, validation accuracy: 0.9428571428571428\n",
            "Epoch: 7; Training Loss: 0.11842017440887337; Running Accuracy: 0.7796803652968036; Time Taken: 104.32253646850586\n",
            "validation loss: 0.21869914244267752, validation accuracy: 0.9428571428571428\n",
            "Epoch: 8; Training Loss: 0.10023100818084045; Running Accuracy: 0.7899543378995434; Time Taken: 103.83321785926819\n",
            "validation loss: 0.20988936688412319, validation accuracy: 0.9428571428571428\n",
            "Epoch: 9; Training Loss: 0.0886234058826548; Running Accuracy: 0.7888127853881278; Time Taken: 104.46872138977051\n",
            "validation loss: 0.21002049763178962, validation accuracy: 0.9371428571428572\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "healthy_wheat       0.96      0.83      0.89        30\n",
            "    leaf_rust       0.94      0.93      0.93        69\n",
            "    stem_rust       0.93      0.99      0.96        76\n",
            "\n",
            "     accuracy                           0.94       175\n",
            "    macro avg       0.94      0.92      0.93       175\n",
            " weighted avg       0.94      0.94      0.94       175\n",
            "\n",
            "fold:2 ======================================\n",
            "Epoch: 0; Training Loss: 0.7093310351060196; Running Accuracy: 0.5764840182648402; Time Taken: 105.45944809913635\n",
            "validation loss: 0.517014969478954, validation accuracy: 0.8342857142857143\n",
            "Epoch: 1; Training Loss: 0.41485333247956907; Running Accuracy: 0.708904109589041; Time Taken: 104.5760326385498\n",
            "validation loss: 0.4293483783575622, validation accuracy: 0.8742857142857143\n",
            "Epoch: 2; Training Loss: 0.30326297430490906; Running Accuracy: 0.7408675799086758; Time Taken: 103.70322513580322\n",
            "validation loss: 0.377687182277441, validation accuracy: 0.8685714285714285\n",
            "Epoch: 3; Training Loss: 0.23430889286100864; Running Accuracy: 0.7636986301369864; Time Taken: 103.51082968711853\n",
            "validation loss: 0.34932449053634296, validation accuracy: 0.8685714285714285\n",
            "Epoch: 4; Training Loss: 0.18628605371552773; Running Accuracy: 0.771689497716895; Time Taken: 103.3132836818695\n",
            "validation loss: 0.3288786503571001, validation accuracy: 0.9028571428571428\n",
            "Epoch: 5; Training Loss: 0.15135677545119755; Running Accuracy: 0.776255707762557; Time Taken: 103.3578953742981\n",
            "validation loss: 0.32230630461973225, validation accuracy: 0.9028571428571428\n",
            "Epoch: 6; Training Loss: 0.12534159882290458; Running Accuracy: 0.7819634703196348; Time Taken: 103.17874002456665\n",
            "validation loss: 0.3116013703613796, validation accuracy: 0.9028571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopper(2, 0.15)\n",
        "losses = classifier.experiments(10, 0.2, batch_size, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp9pRzCba7mE",
        "outputId": "33a038c9-45b0-4671-fd0d-c5db5c6c1199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0; Training Loss: 1.1950837969779968; Running Accuracy: 0.3888888888888889; Time Taken: 0.5002095699310303\n",
            "validation loss: 1.2387637197971344, validation accuracy: 0.2\n",
            "Epoch: 1; Training Loss: 1.1336274027824402; Running Accuracy: 0.5555555555555556; Time Taken: 0.4785332679748535\n",
            "validation loss: 1.1610392928123474, validation accuracy: 0.2\n",
            "Epoch: 2; Training Loss: 1.146437692642212; Running Accuracy: 0.16666666666666666; Time Taken: 0.49182891845703125\n",
            "validation loss: 1.7830167412757874, validation accuracy: 0.2\n",
            "Epoch: 3; Training Loss: 1.0261420130729675; Running Accuracy: 0.5555555555555556; Time Taken: 0.5968911647796631\n",
            "validation loss: 1.6814042329788208, validation accuracy: 0.2\n",
            "Epoch: 4; Training Loss: 1.0586309790611268; Running Accuracy: 0.5555555555555556; Time Taken: 0.5024387836456299\n",
            "validation loss: 1.5204434394836426, validation accuracy: 0.2\n",
            "Epoch: 5; Training Loss: 1.0412484884262085; Running Accuracy: 0.5555555555555556; Time Taken: 0.4580502510070801\n",
            "validation loss: 1.5149065852165222, validation accuracy: 0.2\n",
            "Epoch: 6; Training Loss: 1.0364428877830505; Running Accuracy: 0.5555555555555556; Time Taken: 0.48389244079589844\n",
            "validation loss: 1.1866779923439026, validation accuracy: 0.2\n",
            "Epoch: 7; Training Loss: 1.0654088377952575; Running Accuracy: 0.5555555555555556; Time Taken: 0.5409059524536133\n",
            "validation loss: 1.5902122855186462, validation accuracy: 0.2\n",
            "Epoch: 8; Training Loss: 1.0389222145080566; Running Accuracy: 0.5555555555555556; Time Taken: 0.48030591011047363\n",
            "validation loss: 1.1594915986061096, validation accuracy: 0.2\n",
            "Epoch: 9; Training Loss: 1.0327409267425538; Running Accuracy: 0.4444444444444444; Time Taken: 0.48552751541137695\n",
            "validation loss: 1.7726433873176575, validation accuracy: 0.2\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            " XR_ELBOW_negative       0.20      1.00      0.33         1\n",
            " XR_ELBOW_positive       0.00      0.00      0.00         3\n",
            "XR_FINGER_negative       0.00      0.00      0.00         1\n",
            "\n",
            "          accuracy                           0.20         5\n",
            "         macro avg       0.07      0.33      0.11         5\n",
            "      weighted avg       0.04      0.20      0.07         5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "losses_sum = 0\n",
        "for fold, losses in validation_losses.items():\n",
        "  avg_loss = sum(losses)/len(losses)\n",
        "  losses_sum += avg_loss\n",
        "  count+=1\n",
        "\n",
        "losses_sum/count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x_ZIcsA1U2p",
        "outputId": "ef5d64f6-e0c0-4354-deac-642538bc505c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3072259949890643"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses['train'])\n",
        "plt.plot(losses['valid'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xWnTW8KQkNx2",
        "outputId": "3de564c6-2493-4d5e-b192-abb79ff1c5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f764007bc90>]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Meibz-ZkR0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model for final deployment or getting results on real test data\n",
        "model = train(4, model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA0lvSgIjWxC",
        "outputId": "6eb4495e-21e2-4be9-a94a-fdad5985f989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0; Training Loss: 0.7268747985363007; Running Accuracy: 0.375; Time Taken: 17.2438223361969\n",
            "Epoch: 1; Training Loss: 0.7018822729587555; Running Accuracy: 0.5; Time Taken: 19.104748487472534\n",
            "Epoch: 2; Training Loss: 0.621985673904419; Running Accuracy: 0.875; Time Taken: 17.665613174438477\n",
            "Epoch: 3; Training Loss: 0.5902166813611984; Running Accuracy: 0.8125; Time Taken: 16.846441984176636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBLcHVacjax0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load test data that contains only test images\n",
        "def load_test_data(data_dir):\n",
        "    path = data_dir\n",
        "    images = []\n",
        "    names = []\n",
        "    for img in os.listdir(path):\n",
        "        try:\n",
        "            img_arr = Image.open(os.path.join(path, img), )\n",
        "            resized_arr = img_arr.resize((image_size, image_size))\n",
        "            images.append(resized_arr)\n",
        "            names.append(img)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return np.array(images), names"
      ],
      "metadata": {
        "id": "RxAF7VNqZPmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load actual test data from directory that contains unlabled test images\n",
        "test_images, names = load_test_data(DATADIR+'/test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9J3b4WYZexU",
        "outputId": "4acbae4b-5a2d-4b57-c9b5-92a686e14c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot identify image file './gdrive/My Drive/DeepLearning/animal-classifier/test/.DS_Store'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFamGoL9B5CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b156ad-3530-4232-b16f-4ab773a05483"
      },
      "source": [
        "# Prepare transformations for test images\n",
        "transformations = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "# Initialize variables for accuracy calculations\n",
        "results = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for image, fname in zip(test_images, names):\n",
        "\n",
        "        # Transform image to computable tensor\n",
        "        image = transformations(image)\n",
        "        image.unsqueeze_(0)\n",
        "        image.to(device)\n",
        "\n",
        "        # Get the output of the model\n",
        "        outputs = model(image)\n",
        "        \n",
        "        # Calculate class probabilities and output class\n",
        "        probabilities = nn.Softmax(dim=-1)(outputs).detach().cpu().numpy()\n",
        "        output_probabilities = probabilities[0]\n",
        "        output_class = np.argmax(output_probabilities)\n",
        "\n",
        "        # Prepare array for results on test images\n",
        "        results.append( [fname, output_class])\n",
        "\n",
        "# Create and save csv for final results\n",
        "results = np.array(results)\n",
        "cols = ['ID', 'label']\n",
        "df = pd.DataFrame(results, columns=cols)\n",
        "\n",
        "df.to_csv(DATADIR+'/output.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[[9.997980e-01 2.020572e-04]]\n",
            "[[0.996924   0.00307596]]\n",
            "[[0.9874549  0.01254507]]\n",
            "[[0.9962843  0.00371571]]\n",
            "[[0.99650687 0.00349307]]\n",
            "[[0.93200535 0.06799471]]\n",
            "[[0.90869087 0.09130909]]\n",
            "[[0.961475   0.03852495]]\n",
            "[[0.8797196  0.12028034]]\n",
            "[[0.99575776 0.00424228]]\n",
            "[[0.7916683  0.20833176]]\n",
            "[[0.99127614 0.00872385]]\n",
            "[[0.9857654  0.01423462]]\n",
            "[[0.31444067 0.68555933]]\n",
            "[[0.99620664 0.00379339]]\n",
            "[[0.7059871  0.29401284]]\n",
            "[[0.9018245  0.09817552]]\n",
            "[[0.9405667  0.05943326]]\n",
            "[[0.9878605  0.01213949]]\n",
            "[[0.96992725 0.03007277]]\n",
            "[[0.99875295 0.00124705]]\n",
            "[[0.48600277 0.51399726]]\n",
            "[[0.98626757 0.01373246]]\n",
            "[[0.98922616 0.01077389]]\n",
            "[[0.93604076 0.06395927]]\n",
            "[[0.13440318 0.86559683]]\n",
            "[[0.6111288 0.3888712]]\n",
            "[[0.9318988  0.06810122]]\n",
            "[[0.966693   0.03330698]]\n",
            "[[0.99686605 0.0031339 ]]\n",
            "[[0.6571417  0.34285828]]\n",
            "[[9.992638e-01 7.362071e-04]]\n",
            "[[0.9929517  0.00704826]]\n",
            "[[0.94579965 0.05420034]]\n",
            "[[0.98646426 0.01353578]]\n",
            "[[0.9958003  0.00419972]]\n",
            "[[0.7672034  0.23279665]]\n",
            "[[0.91047037 0.08952965]]\n",
            "[[0.92645746 0.07354251]]\n",
            "[[0.9396753  0.06032478]]\n",
            "[[0.8812568  0.11874319]]\n",
            "[[0.5443478  0.45565218]]\n",
            "[[0.99461776 0.00538228]]\n",
            "[[0.96551204 0.03448792]]\n",
            "[[0.38564217 0.6143578 ]]\n",
            "[[9.9982184e-01 1.7824020e-04]]\n",
            "[[0.8875591  0.11244087]]\n",
            "[[0.469298   0.53070194]]\n",
            "[[0.99537045 0.00462957]]\n",
            "[[0.99763024 0.00236971]]\n",
            "[[0.7245255 0.2754745]]\n",
            "[[0.8336654  0.16633455]]\n",
            "[[0.8746643  0.12533563]]\n",
            "[[0.9912608  0.00873919]]\n",
            "[[0.9975115  0.00248847]]\n",
            "[[0.47339413 0.5266059 ]]\n",
            "[[0.0529913 0.9470087]]\n",
            "[[9.9954909e-01 4.5086624e-04]]\n",
            "[[0.9097887  0.09021126]]\n",
            "[[0.99870634 0.00129363]]\n",
            "[[0.9978974  0.00210264]]\n",
            "[[0.3778976 0.6221024]]\n",
            "[[0.65774    0.34226006]]\n",
            "[[0.9967168  0.00328313]]\n",
            "[[0.4770923  0.52290773]]\n",
            "[[0.12530074 0.87469923]]\n",
            "[[0.9958145  0.00418547]]\n",
            "[[0.52001405 0.4799859 ]]\n",
            "[[0.9718603  0.02813968]]\n",
            "[[0.9635717  0.03642827]]\n",
            "[[9.992963e-01 7.036471e-04]]\n",
            "[[0.259 0.741]]\n",
            "[[0.42711642 0.57288355]]\n",
            "[[0.88994807 0.11005186]]\n",
            "[[0.88469845 0.1153015 ]]\n",
            "[[0.9346849  0.06531508]]\n",
            "[[0.9174979  0.08250209]]\n",
            "[[9.992487e-01 7.512813e-04]]\n",
            "[[0.86153644 0.1384636 ]]\n",
            "[[0.993806   0.00619406]]\n",
            "[[9.9995077e-01 4.9262002e-05]]\n",
            "[[0.85477877 0.14522126]]\n",
            "[[0.9515577  0.04844233]]\n",
            "[[0.8761237 0.1238763]]\n",
            "[[0.95191777 0.0480822 ]]\n",
            "[[0.8721517  0.12784824]]\n",
            "[[0.21340114 0.78659886]]\n",
            "[[0.5877957  0.41220438]]\n",
            "[[0.29894748 0.70105255]]\n",
            "[[0.15732563 0.84267443]]\n",
            "[[0.50794864 0.49205142]]\n",
            "[[9.996377e-01 3.623226e-04]]\n",
            "[[0.9748511  0.02514893]]\n",
            "[[9.9996626e-01 3.3679877e-05]]\n",
            "[[0.9378254  0.06217462]]\n",
            "[[0.9214193  0.07858065]]\n",
            "[[0.70373815 0.2962619 ]]\n",
            "[[0.97709364 0.0229063 ]]\n",
            "[[9.9977690e-01 2.2309323e-04]]\n",
            "[[0.9495733  0.05042674]]\n",
            "[[9.994266e-01 5.734401e-04]]\n",
            "[[0.93820864 0.06179131]]\n",
            "[[0.71732527 0.28267473]]\n",
            "[[0.9152819  0.08471806]]\n",
            "[[0.2219676  0.77803236]]\n",
            "[[9.9969733e-01 3.0267602e-04]]\n",
            "[[0.9858429  0.01415708]]\n",
            "[[0.39689 0.60311]]\n",
            "[[0.9830557  0.01694424]]\n",
            "[[9.9981111e-01 1.8894748e-04]]\n",
            "[[0.8417859  0.15821409]]\n",
            "[[9.9937159e-01 6.2848785e-04]]\n",
            "[[0.9328856  0.06711442]]\n",
            "[[0.9204359  0.07956408]]\n",
            "[[9.9954575e-01 4.5426658e-04]]\n",
            "[[0.90637034 0.09362967]]\n",
            "[[0.9194006 0.0805994]]\n",
            "[[0.48324332 0.5167567 ]]\n",
            "[[9.9983692e-01 1.6303228e-04]]\n",
            "[[0.94583654 0.05416346]]\n",
            "[[0.958432   0.04156805]]\n",
            "[[0.01081495 0.9891851 ]]\n",
            "[[9.9971765e-01 2.8236877e-04]]\n",
            "[[0.9796317  0.02036832]]\n",
            "[[0.8706898  0.12931015]]\n",
            "[[0.7403958  0.25960425]]\n",
            "[[0.9069547  0.09304534]]\n",
            "[[0.9929281  0.00707191]]\n",
            "[[0.9981963  0.00180376]]\n",
            "[[0.97659564 0.02340435]]\n",
            "[[0.9981875 0.0018125]]\n",
            "[[0.00628004 0.99371994]]\n",
            "[[0.99807125 0.00192879]]\n",
            "[[0.7662363  0.23376374]]\n",
            "[[0.7688357  0.23116435]]\n",
            "[[0.867149   0.13285102]]\n",
            "[[0.9055339  0.09446608]]\n",
            "[[0.4271483  0.57285166]]\n",
            "[[0.53120583 0.46879414]]\n",
            "[[0.97966796 0.020332  ]]\n",
            "[[0.8722006  0.12779942]]\n",
            "[[0.8929238  0.10707623]]\n",
            "[[0.33655715 0.6634428 ]]\n",
            "[[0.72960395 0.27039602]]\n",
            "[[0.9715963  0.02840368]]\n",
            "[[0.73793995 0.26206005]]\n",
            "[[0.9119292  0.08807073]]\n",
            "[[9.9998415e-01 1.5815860e-05]]\n",
            "[[9.9993134e-01 6.8705071e-05]]\n",
            "[[0.21307155 0.7869284 ]]\n",
            "[[0.990764  0.0092359]]\n",
            "[[0.990999  0.0090011]]\n",
            "[[0.99810106 0.00189898]]\n",
            "[[0.9546911  0.04530894]]\n",
            "[[0.88115853 0.11884153]]\n",
            "[[0.83611876 0.16388124]]\n",
            "[[0.08383834 0.91616166]]\n",
            "[[0.9889214  0.01107864]]\n",
            "[[0.21021423 0.7897858 ]]\n",
            "[[0.5200386 0.4799614]]\n",
            "[[0.90882564 0.09117436]]\n",
            "[[0.99839205 0.00160787]]\n",
            "[[0.9029021  0.09709786]]\n",
            "[[0.66825646 0.33174357]]\n",
            "[[0.84183586 0.15816413]]\n",
            "[[0.9966273  0.00337273]]\n",
            "[[0.36266604 0.6373339 ]]\n",
            "[[0.25573152 0.7442685 ]]\n",
            "[[9.9960667e-01 3.9341132e-04]]\n",
            "[[0.9079274  0.09207258]]\n",
            "[[0.9693797 0.0306203]]\n",
            "[[0.93359107 0.06640897]]\n",
            "[[0.97231996 0.02768002]]\n",
            "[[0.95422995 0.04577006]]\n",
            "[[0.5874316  0.41256842]]\n",
            "[[0.43221253 0.56778747]]\n",
            "[[0.22107428 0.7789257 ]]\n",
            "[[0.9979856  0.00201443]]\n",
            "[[0.9862899  0.01371006]]\n",
            "[[0.00345576 0.9965442 ]]\n",
            "[[0.71854025 0.28145972]]\n",
            "[[0.27417183 0.7258282 ]]\n",
            "[[0.95189995 0.04810004]]\n",
            "[[0.9925581  0.00744188]]\n",
            "[[9.9918205e-01 8.1795384e-04]]\n",
            "[[0.99787915 0.00212092]]\n",
            "[[0.9266964  0.07330359]]\n",
            "[[0.98909676 0.01090317]]\n",
            "[[9.990368e-01 9.632252e-04]]\n",
            "[[0.5886033  0.41139668]]\n",
            "[[0.98500514 0.01499483]]\n",
            "[[0.8666425 0.1333575]]\n",
            "[[0.74526995 0.25473002]]\n",
            "[[0.8005377  0.19946226]]\n",
            "[[9.9912566e-01 8.7434018e-04]]\n",
            "[[0.39088726 0.60911274]]\n",
            "[[0.87908787 0.12091209]]\n",
            "[[0.78309155 0.2169085 ]]\n",
            "[[0.8945715  0.10542852]]\n",
            "[[0.77002233 0.2299777 ]]\n",
            "[[0.9900045  0.00999556]]\n",
            "[[0.99584323 0.00415678]]\n",
            "[[0.95524925 0.04475075]]\n",
            "[[0.9472522  0.05274782]]\n",
            "[[0.99055487 0.00944515]]\n",
            "[[0.9983569  0.00164305]]\n",
            "[[0.77624357 0.22375639]]\n",
            "[[0.5626976  0.43730244]]\n",
            "[[0.9209755  0.07902449]]\n",
            "[[0.99891233 0.00108768]]\n",
            "[[0.99281234 0.0071877 ]]\n",
            "[[0.9988832  0.00111685]]\n",
            "[[0.9042919  0.09570807]]\n",
            "[[0.6385684 0.3614316]]\n",
            "[[0.99721634 0.00278364]]\n",
            "[[9.997260e-01 2.740214e-04]]\n",
            "[[0.9667496  0.03325047]]\n",
            "[[9.9998009e-01 1.9936604e-05]]\n",
            "[[0.7564175  0.24358249]]\n",
            "[[0.86875    0.13125004]]\n",
            "[[0.9239825  0.07601752]]\n",
            "[[0.99511623 0.00488378]]\n",
            "[[0.96310645 0.03689355]]\n",
            "[[0.8096689  0.19033104]]\n",
            "[[0.8668032 0.1331968]]\n",
            "[[0.79552996 0.20447002]]\n",
            "[[0.7743193  0.22568075]]\n",
            "[[0.60869503 0.39130488]]\n",
            "[[0.8952491 0.1047508]]\n",
            "[[0.9973802  0.00261976]]\n",
            "[[0.8816313  0.11836872]]\n",
            "[[0.77781457 0.22218545]]\n",
            "[[0.6300476 0.3699524]]\n",
            "[[0.9964018  0.00359823]]\n",
            "[[0.98563695 0.01436302]]\n",
            "[[0.8157952  0.18420483]]\n",
            "[[0.97360355 0.0263965 ]]\n",
            "[[0.96331114 0.0366888 ]]\n",
            "[[0.3927384  0.60726166]]\n",
            "[[0.8938224  0.10617757]]\n",
            "[[0.8644771  0.13552296]]\n",
            "[[0.9952206  0.00477936]]\n",
            "[[0.99570036 0.00429971]]\n",
            "[[0.4705781 0.5294219]]\n",
            "[[0.97283554 0.02716448]]\n",
            "[[0.68324614 0.31675386]]\n",
            "[[0.35195285 0.6480472 ]]\n",
            "[[0.9808606  0.01913947]]\n",
            "[[0.02616314 0.97383684]]\n",
            "[[0.9033447  0.09665528]]\n",
            "[[0.37557238 0.6244276 ]]\n",
            "[[0.95274544 0.0472546 ]]\n",
            "[[9.9935168e-01 6.4826413e-04]]\n",
            "[[0.09193093 0.90806913]]\n",
            "[[0.9548329  0.04516711]]\n",
            "[[0.8329608  0.16703919]]\n",
            "[[0.9944799 0.0055201]]\n",
            "[[0.9367379 0.0632621]]\n",
            "[[0.9879629  0.01203712]]\n",
            "[[0.9701554  0.02984455]]\n",
            "[[0.9409417  0.05905834]]\n",
            "[[0.40332818 0.5966718 ]]\n",
            "[[0.5245196  0.47548032]]\n",
            "[[0.1411712 0.8588287]]\n",
            "[[0.8687525  0.13124754]]\n",
            "[[0.7028515  0.29714853]]\n",
            "[[0.94260925 0.0573907 ]]\n",
            "[[0.99580705 0.00419288]]\n",
            "[[0.02540039 0.9745996 ]]\n",
            "[[0.99897027 0.00102977]]\n",
            "[[0.9951988  0.00480121]]\n",
            "[[0.9688079  0.03119214]]\n",
            "[[0.16304491 0.8369551 ]]\n",
            "[[0.9661222  0.03387784]]\n",
            "[[0.99083906 0.00916098]]\n",
            "[[0.997703   0.00229704]]\n",
            "[[0.3500542 0.6499458]]\n",
            "[[0.66722697 0.332773  ]]\n",
            "[[0.40359014 0.59640986]]\n",
            "[[0.9347703  0.06522976]]\n",
            "[[0.9856639  0.01433605]]\n",
            "[[0.94435817 0.0556418 ]]\n",
            "[[0.96600324 0.03399679]]\n",
            "[[0.9933854  0.00661458]]\n",
            "[[0.4628551  0.53714496]]\n",
            "[[0.9588173  0.04118267]]\n",
            "[[0.01053309 0.98946685]]\n",
            "[[0.94893605 0.05106393]]\n",
            "[[9.9997675e-01 2.3230283e-05]]\n",
            "[[0.340263 0.659737]]\n",
            "[[0.4448414 0.5551586]]\n",
            "[[0.99703276 0.00296721]]\n",
            "[[0.72233164 0.27766833]]\n",
            "[[9.9966252e-01 3.3750257e-04]]\n",
            "[[9.9943095e-01 5.6901964e-04]]\n",
            "[[0.94607687 0.05392314]]\n",
            "[[0.99571973 0.00428028]]\n",
            "[[0.8146851 0.1853149]]\n",
            "[[0.9366371  0.06336292]]\n",
            "[[9.99895930e-01 1.04043036e-04]]\n",
            "[[0.90280104 0.09719899]]\n",
            "[[0.99646986 0.00353016]]\n",
            "[[0.8228532  0.17714685]]\n",
            "[[0.8563769  0.14362316]]\n",
            "[[0.575998   0.42400202]]\n",
            "[[0.962118   0.03788198]]\n",
            "[[0.93185323 0.06814678]]\n",
            "[[0.9303981  0.06960184]]\n",
            "[[0.9762422  0.02375777]]\n",
            "[[0.9546188  0.04538114]]\n",
            "[[0.5364785  0.46352145]]\n",
            "[[0.99473226 0.00526779]]\n",
            "[[9.9997163e-01 2.8350687e-05]]\n",
            "[[0.9758281  0.02417183]]\n",
            "[[0.9980761  0.00192392]]\n",
            "[[0.3255914  0.67440856]]\n",
            "[[0.998694   0.00130599]]\n",
            "[[9.9977547e-01 2.2455100e-04]]\n",
            "[[0.9566684  0.04333166]]\n",
            "[[9.9992085e-01 7.9192017e-05]]\n",
            "[[0.57121474 0.42878526]]\n",
            "[[0.99056375 0.00943619]]\n",
            "[[0.99447894 0.00552104]]\n",
            "[[0.9911839  0.00881612]]\n",
            "[[0.87099135 0.12900865]]\n",
            "[[0.07266096 0.9273391 ]]\n",
            "[[0.9630356  0.03696441]]\n",
            "[[0.98091185 0.01908818]]\n",
            "[[0.822588   0.17741199]]\n",
            "[[0.99686676 0.00313318]]\n",
            "[[0.61224926 0.38775074]]\n",
            "[[0.66304 0.33696]]\n",
            "[[0.9951127  0.00488726]]\n",
            "[[9.9978215e-01 2.1782503e-04]]\n",
            "[[0.8249605  0.17503954]]\n",
            "[[0.9786876  0.02131249]]\n",
            "[[0.9872313 0.0127687]]\n",
            "[[0.8051363  0.19486372]]\n",
            "[[0.98239154 0.01760841]]\n",
            "[[0.97909355 0.02090652]]\n",
            "[[0.2301325 0.7698675]]\n",
            "[[0.9160766  0.08392341]]\n",
            "[[0.8838186  0.11618138]]\n",
            "[[9.9986959e-01 1.3036057e-04]]\n",
            "[[0.5222421 0.4777579]]\n",
            "[[0.9103315  0.08966855]]\n",
            "[[0.9754793  0.02452063]]\n",
            "[[0.9467461  0.05325386]]\n",
            "[[0.87190974 0.12809026]]\n",
            "[[0.54891586 0.45108414]]\n",
            "[[0.97816783 0.02183217]]\n",
            "[[0.8113565  0.18864357]]\n",
            "[[0.98460627 0.01539369]]\n",
            "[[0.30864334 0.6913566 ]]\n",
            "[[0.56334376 0.4366562 ]]\n",
            "[[0.7973556  0.20264441]]\n",
            "[[0.9860865  0.01391353]]\n",
            "[[0.86460847 0.1353915 ]]\n",
            "[[9.9961996e-01 3.7996800e-04]]\n",
            "[[0.9835122  0.01648781]]\n",
            "[[0.85747695 0.14252308]]\n",
            "[[0.91915315 0.08084689]]\n",
            "[[0.9979633  0.00203667]]\n",
            "[[0.92028576 0.07971425]]\n",
            "[[0.35789227 0.6421077 ]]\n",
            "[[0.9830892 0.0169108]]\n",
            "[[0.9947417  0.00525831]]\n",
            "[[9.9934214e-01 6.5784325e-04]]\n",
            "[[0.7527548 0.2472452]]\n",
            "[[0.20562714 0.7943728 ]]\n",
            "[[9.9979109e-01 2.0894186e-04]]\n",
            "[[0.9829677  0.01703231]]\n",
            "[[0.9988557  0.00114435]]\n",
            "[[0.8404855  0.15951441]]\n",
            "[[0.99763    0.00237003]]\n",
            "[[0.99766904 0.00233091]]\n",
            "[[0.9910124  0.00898758]]\n",
            "[[0.9828493  0.01715068]]\n",
            "[[0.4153883  0.58461165]]\n",
            "[[0.39755633 0.60244364]]\n",
            "[[9.9981827e-01 1.8177721e-04]]\n",
            "[[0.7950165  0.20498352]]\n",
            "[[0.9690168  0.03098313]]\n",
            "[[0.972996   0.02700406]]\n",
            "[[0.9181299 0.0818701]]\n",
            "[[0.6564909  0.34350914]]\n",
            "[[0.5647605  0.43523952]]\n",
            "[[0.9989004  0.00109954]]\n",
            "[[0.8949504  0.10504965]]\n",
            "[[0.7821842  0.21781577]]\n",
            "[[0.27364793 0.72635204]]\n",
            "[[0.53354275 0.46645722]]\n",
            "[[0.47874904 0.521251  ]]\n",
            "[[0.8341813  0.16581869]]\n",
            "[[0.9981902  0.00180975]]\n",
            "[[0.99870336 0.00129661]]\n",
            "[[0.19487722 0.8051228 ]]\n",
            "[[0.9564448  0.04355519]]\n",
            "[[0.88750917 0.11249079]]\n",
            "[[0.9978369  0.00216313]]\n",
            "[[0.9976355  0.00236446]]\n",
            "[[0.33272013 0.6672798 ]]\n",
            "[[0.7127516  0.28724828]]\n",
            "[[0.46723896 0.53276104]]\n",
            "[[0.7097834  0.29021662]]\n",
            "[[0.5698784  0.43012157]]\n",
            "[[9.9995184e-01 4.8187489e-05]]\n",
            "[[0.990256   0.00974396]]\n",
            "[[0.43779635 0.56220365]]\n",
            "[[0.9920731  0.00792691]]\n",
            "[[0.9387495 0.0612505]]\n",
            "[[0.9035251  0.09647493]]\n",
            "[[0.9834019  0.01659813]]\n",
            "[[0.9977755  0.00222453]]\n",
            "[[0.9928284  0.00717162]]\n",
            "[[0.9775647  0.02243533]]\n",
            "[[0.99055266 0.00944731]]\n",
            "[[0.08516779 0.91483223]]\n",
            "[[0.90051377 0.09948628]]\n",
            "[[0.6142318  0.38576818]]\n",
            "[[0.8809483  0.11905176]]\n",
            "[[0.5112813  0.48871863]]\n",
            "[[0.9968809 0.0031191]]\n",
            "[[0.94940096 0.05059906]]\n",
            "[[0.93174165 0.06825835]]\n",
            "[[9.9940073e-01 5.9923995e-04]]\n",
            "[[0.99552745 0.00447257]]\n",
            "[[0.03027899 0.96972096]]\n",
            "[[0.96000254 0.03999745]]\n",
            "[[0.970078   0.02992194]]\n",
            "[[0.9739992  0.02600083]]\n",
            "[[0.84153014 0.1584699 ]]\n",
            "[[0.51769644 0.48230353]]\n",
            "[[0.9507633  0.04923673]]\n",
            "[[9.9943703e-01 5.6291587e-04]]\n",
            "[[0.9876539  0.01234602]]\n",
            "[[9.9981719e-01 1.8283968e-04]]\n",
            "[[0.29528126 0.7047187 ]]\n",
            "[[0.710002   0.28999794]]\n",
            "[[9.9963474e-01 3.6526984e-04]]\n",
            "[[9.9999464e-01 5.3446961e-06]]\n",
            "[[0.95783544 0.04216452]]\n",
            "[[0.7652855  0.23471451]]\n",
            "[[9.9929261e-01 7.0730585e-04]]\n",
            "[[0.9000354  0.09996464]]\n",
            "[[0.91716677 0.0828333 ]]\n",
            "[[0.9580607  0.04193935]]\n",
            "[[0.9122426  0.08775737]]\n",
            "[[0.4226837 0.5773163]]\n",
            "[[0.9954692  0.00453082]]\n",
            "[[0.00994111 0.99005896]]\n",
            "[[0.8317729  0.16822702]]\n",
            "[[0.6916407 0.3083593]]\n",
            "[[0.82455534 0.17544466]]\n",
            "[[0.8447625  0.15523748]]\n",
            "[[0.98087627 0.01912371]]\n",
            "[[9.9995446e-01 4.5551562e-05]]\n",
            "[[0.47460148 0.5253985 ]]\n",
            "[[0.9986241  0.00137593]]\n",
            "[[0.89324474 0.10675521]]\n",
            "[[9.994343e-01 5.657301e-04]]\n",
            "[[0.96510756 0.03489245]]\n",
            "[[0.9765512  0.02344878]]\n",
            "[[0.9952142  0.00478578]]\n",
            "[[0.891764   0.10823599]]\n",
            "[[0.9107053 0.0892946]]\n",
            "[[0.9982663  0.00173375]]\n",
            "[[0.9763106  0.02368943]]\n",
            "[[0.9972146  0.00278546]]\n",
            "[[0.98000324 0.01999682]]\n",
            "[[9.9998999e-01 1.0068793e-05]]\n",
            "[[9.9930990e-01 6.9006777e-04]]\n",
            "[[0.7441753 0.2558247]]\n",
            "[[0.96184784 0.03815213]]\n",
            "[[0.99055743 0.00944258]]\n",
            "[[0.5486082  0.45139173]]\n",
            "[[0.8964455  0.10355449]]\n",
            "[[9.9944836e-01 5.5162486e-04]]\n",
            "[[0.7076364  0.29236358]]\n",
            "[[0.37968662 0.62031335]]\n",
            "[[0.82930505 0.170695  ]]\n",
            "[[0.9280593  0.07194071]]\n",
            "[[0.3128856 0.6871144]]\n",
            "[[0.91672367 0.08327638]]\n",
            "[[9.990771e-01 9.229397e-04]]\n",
            "[[0.7805226  0.21947739]]\n",
            "[[0.0559318  0.94406825]]\n",
            "[[0.23670882 0.7632912 ]]\n",
            "[[0.88692164 0.11307829]]\n",
            "[[0.82562757 0.17437246]]\n",
            "[[0.9978442  0.00215575]]\n",
            "[[0.5853478 0.4146523]]\n",
            "[[0.8063773  0.19362275]]\n",
            "[[0.5999325  0.40006754]]\n",
            "[[0.8072164  0.19278358]]\n",
            "[[0.92375726 0.0762428 ]]\n",
            "[[0.9845132  0.01548674]]\n",
            "[[0.84209883 0.15790114]]\n",
            "[[0.9857324  0.01426766]]\n",
            "[[0.99107575 0.00892424]]\n",
            "[[0.93874663 0.06125335]]\n",
            "[[0.51337844 0.48662153]]\n",
            "[[0.92951006 0.07048995]]\n",
            "[[0.8761005  0.12389955]]\n",
            "[[0.9895209  0.01047907]]\n",
            "[[0.06935254 0.9306475 ]]\n",
            "[[0.9764395  0.02356053]]\n",
            "[[0.8941088  0.10589121]]\n",
            "[[0.5097533 0.4902467]]\n",
            "[[9.9953139e-01 4.6862123e-04]]\n",
            "[[0.996861   0.00313904]]\n",
            "[[0.8997342  0.10026578]]\n",
            "[[0.79991347 0.20008652]]\n",
            "[[9.9979800e-01 2.0199784e-04]]\n",
            "[[9.999405e-01 5.953966e-05]]\n",
            "[[0.516442   0.48355794]]\n",
            "[[0.28453922 0.71546084]]\n",
            "[[0.96310633 0.03689364]]\n",
            "[[0.9888758  0.01112414]]\n",
            "[[0.6757059 0.3242941]]\n",
            "[[0.9752783  0.02472168]]\n",
            "[[0.9981261  0.00187396]]\n",
            "[[0.97088313 0.02911686]]\n",
            "[[0.18417129 0.81582874]]\n",
            "[[0.24965441 0.7503455 ]]\n",
            "[[0.99819154 0.00180848]]\n",
            "[[0.988971   0.01102899]]\n",
            "[[9.9959534e-01 4.0466391e-04]]\n",
            "[[0.66507846 0.33492157]]\n",
            "[[0.6841213  0.31587872]]\n",
            "[[0.9925476  0.00745242]]\n",
            "[[0.9862986  0.01370139]]\n",
            "[[0.99455863 0.00544138]]\n",
            "[[0.9366285  0.06337145]]\n",
            "[[0.2622991 0.7377009]]\n",
            "[[0.92761385 0.07238612]]\n",
            "[[0.36166018 0.6383399 ]]\n",
            "[[0.9233754  0.07662457]]\n",
            "[[9.9953461e-01 4.6544353e-04]]\n",
            "[[0.99108714 0.00891287]]\n",
            "[[0.27867112 0.7213289 ]]\n",
            "[[9.9902630e-01 9.7364903e-04]]\n",
            "[[9.9998057e-01 1.9379147e-05]]\n",
            "[[0.87356657 0.1264335 ]]\n",
            "[[0.8700776  0.12992238]]\n",
            "[[0.7045247 0.2954753]]\n",
            "[[0.05084948 0.94915044]]\n",
            "[[0.9635539  0.03644609]]\n",
            "[[0.8694343  0.13056569]]\n",
            "[[0.95277137 0.04722859]]\n",
            "[[0.97721726 0.02278278]]\n",
            "[[0.8886368  0.11136317]]\n",
            "[[0.95857775 0.04142225]]\n",
            "[[0.9798696  0.02013034]]\n",
            "[[0.59299505 0.40700498]]\n",
            "[[0.2930913  0.70690864]]\n",
            "[[0.98224336 0.01775663]]\n",
            "[[0.9922747  0.00772534]]\n",
            "[[0.9951255  0.00487456]]\n",
            "[[0.99116427 0.00883576]]\n",
            "[[0.9845814  0.01541855]]\n",
            "[[0.9445262 0.0554738]]\n",
            "[[0.93950456 0.06049547]]\n",
            "[[0.9985629  0.00143711]]\n",
            "[[9.995012e-01 4.987274e-04]]\n",
            "[[0.91162    0.08837996]]\n",
            "[[0.9728308  0.02716918]]\n",
            "[[0.99446857 0.00553147]]\n",
            "[[0.24799152 0.7520085 ]]\n",
            "[[9.9926621e-01 7.3378644e-04]]\n",
            "[[0.52623415 0.47376582]]\n",
            "[[9.9957126e-01 4.2878278e-04]]\n",
            "[[0.7567386  0.24326135]]\n",
            "[[0.328997   0.67100304]]\n",
            "[[0.9371293  0.06287066]]\n",
            "[[0.3850716  0.61492836]]\n",
            "[[0.9950826 0.0049174]]\n",
            "[[0.9968726  0.00312737]]\n",
            "[[0.7974645  0.20253548]]\n",
            "[[0.9988175  0.00118246]]\n",
            "[[9.9999869e-01 1.3134545e-06]]\n",
            "[[0.8539714 0.1460286]]\n",
            "[[0.9972397 0.0027603]]\n",
            "[[9.9966347e-01 3.3651653e-04]]\n",
            "[[0.8726482  0.12735179]]\n",
            "[[0.64460033 0.3553996 ]]\n",
            "[[0.33525407 0.6647459 ]]\n",
            "[[0.9137139  0.08628612]]\n",
            "[[0.9089182  0.09108182]]\n",
            "[[0.9966774  0.00332256]]\n",
            "[[9.9993360e-01 6.6378656e-05]]\n",
            "[[0.8780497  0.12195033]]\n",
            "[[0.9771821  0.02281794]]\n",
            "[[0.7912342  0.20876577]]\n",
            "[[9.9971515e-01 2.8483305e-04]]\n",
            "[[9.99893069e-01 1.06924155e-04]]\n",
            "[[0.85221815 0.14778185]]\n",
            "[[0.99655294 0.00344707]]\n",
            "[[0.9263057  0.07369432]]\n",
            "[[9.9994338e-01 5.6601308e-05]]\n",
            "[[0.22481506 0.7751849 ]]\n",
            "[[0.46386927 0.53613067]]\n",
            "[[0.1732501 0.8267499]]\n",
            "[[0.89346945 0.10653058]]\n",
            "[[9.9963403e-01 3.6595578e-04]]\n",
            "[[0.8528912  0.14710878]]\n",
            "[[0.04115238 0.95884764]]\n",
            "[[0.9834277  0.01657228]]\n",
            "[[0.9425176  0.05748245]]\n",
            "[[0.07895786 0.92104214]]\n",
            "[[0.18165076 0.8183493 ]]\n",
            "[[0.49385676 0.5061432 ]]\n",
            "[[0.57266 0.42734]]\n",
            "[[0.9298791  0.07012089]]\n",
            "[[0.89109415 0.10890587]]\n",
            "[[0.98167235 0.01832769]]\n",
            "[[0.99880767 0.00119228]]\n",
            "[[9.9967575e-01 3.2429610e-04]]\n",
            "[[0.98838776 0.01161226]]\n",
            "[[9.9976462e-01 2.3543982e-04]]\n",
            "[[0.87970966 0.12029033]]\n",
            "[[0.92960316 0.07039681]]\n",
            "[[0.9857744  0.01422562]]\n",
            "[[0.9908601  0.00913985]]\n",
            "[[0.92139405 0.07860601]]\n",
            "[[0.81976813 0.18023185]]\n",
            "[[0.80310917 0.19689079]]\n",
            "[[0.8820644  0.11793558]]\n",
            "[[0.9944078  0.00559231]]\n",
            "[[0.98194295 0.01805709]]\n",
            "[[0.82732785 0.17267214]]\n",
            "[[0.73880446 0.26119554]]\n",
            "[[9.991997e-01 8.002784e-04]]\n",
            "[[0.96821946 0.03178051]]\n",
            "[[0.9371087  0.06289134]]\n",
            "[[0.9759832  0.02401674]]\n",
            "[[0.57252556 0.42747447]]\n",
            "[[0.7046761  0.29532394]]\n",
            "[[0.97081053 0.0291894 ]]\n",
            "[[0.960878   0.03912197]]\n",
            "[[0.4316572 0.5683428]]\n",
            "[[0.45716837 0.5428316 ]]\n",
            "[[0.9936308  0.00636913]]\n",
            "[[0.28392124 0.71607876]]\n",
            "[[0.79788905 0.20211096]]\n",
            "[[0.71268857 0.28731146]]\n",
            "[[0.07583912 0.92416084]]\n",
            "[[0.9947896  0.00521042]]\n",
            "[[0.9973031  0.00269688]]\n",
            "[[0.18403912 0.8159609 ]]\n",
            "[[0.27760732 0.7223927 ]]\n",
            "[[0.9346683  0.06533173]]\n",
            "[[9.993011e-01 6.988994e-04]]\n",
            "[[0.9907182  0.00928177]]\n",
            "[[0.95941854 0.04058144]]\n",
            "[[0.99604887 0.00395108]]\n",
            "[[9.9976355e-01 2.3643476e-04]]\n",
            "[[0.26997998 0.73002   ]]\n",
            "[[0.22502224 0.77497774]]\n",
            "[[9.9923527e-01 7.6478353e-04]]\n",
            "[[0.9985802  0.00141977]]\n",
            "[[0.9736304  0.02636958]]\n",
            "[[0.69259363 0.3074064 ]]\n",
            "[[0.97294575 0.02705422]]\n",
            "[[0.9218984  0.07810166]]\n",
            "[[0.2165587  0.78344136]]\n",
            "[[9.9931633e-01 6.8370171e-04]]\n",
            "[[0.9031562  0.09684375]]\n",
            "[[0.8770733  0.12292664]]\n",
            "[[0.96994185 0.0300582 ]]\n",
            "[[9.9987733e-01 1.2268407e-04]]\n",
            "[[0.980172 0.019828]]\n",
            "[[0.99562776 0.00437224]]\n",
            "[[0.233761 0.766239]]\n",
            "[[0.9782     0.02179993]]\n",
            "[[0.9903258  0.00967416]]\n",
            "[[0.91098714 0.08901294]]\n",
            "[[0.998922   0.00107801]]\n",
            "[[0.91866815 0.08133183]]\n",
            "[[0.9239326  0.07606735]]\n",
            "[[0.88832515 0.11167483]]\n",
            "[[0.9301112  0.06988875]]\n",
            "[[0.76909596 0.2309041 ]]\n",
            "[[0.66600007 0.33399987]]\n",
            "[[0.90239173 0.09760829]]\n",
            "[[0.49442264 0.5055774 ]]\n",
            "[[0.0942871 0.9057129]]\n",
            "[[0.27041766 0.72958237]]\n",
            "[[0.7712557  0.22874433]]\n",
            "[[0.7925742  0.20742574]]\n",
            "[[0.7519304  0.24806957]]\n",
            "[[0.813051   0.18694906]]\n",
            "[[0.9143882  0.08561181]]\n",
            "[[9.9950182e-01 4.9815915e-04]]\n",
            "[[0.09062    0.90937996]]\n",
            "[[0.9483559  0.05164409]]\n",
            "[[9.996749e-01 3.250811e-04]]\n",
            "[[9.9985826e-01 1.4175440e-04]]\n",
            "[[0.3489959 0.6510041]]\n",
            "[[9.9949646e-01 5.0355372e-04]]\n",
            "[[0.98983604 0.01016399]]\n",
            "[[9.9980825e-01 1.9173729e-04]]\n",
            "[[9.9970824e-01 2.9169300e-04]]\n",
            "[[9.9950457e-01 4.9538643e-04]]\n",
            "[[0.9779874  0.02201258]]\n",
            "[[0.72873336 0.2712666 ]]\n",
            "[[0.6290947  0.37090534]]\n",
            "[[0.9704119  0.02958816]]\n",
            "[[0.97863    0.02137004]]\n",
            "[[0.61670345 0.38329658]]\n",
            "[[0.9303058  0.06969419]]\n",
            "[[0.9925891  0.00741084]]\n",
            "[[0.5390484  0.46095166]]\n",
            "[[0.99496067 0.00503929]]\n",
            "[[0.98400176 0.0159983 ]]\n",
            "[[0.96755666 0.03244327]]\n",
            "[[9.9971873e-01 2.8122475e-04]]\n",
            "[[9.9951661e-01 4.8338095e-04]]\n",
            "[[0.9890387  0.01096126]]\n",
            "[[0.9862645 0.0137354]]\n",
            "[[0.75839454 0.24160549]]\n",
            "[[0.97367203 0.02632797]]\n",
            "[[0.9895704  0.01042958]]\n",
            "[[0.8641677  0.13583231]]\n",
            "[[0.99868757 0.00131244]]\n",
            "[[0.591069   0.40893105]]\n",
            "[[9.9989855e-01 1.0138155e-04]]\n",
            "[[0.99706334 0.00293662]]\n",
            "[[0.99759704 0.00240294]]\n",
            "[[9.997588e-01 2.412703e-04]]\n",
            "[[0.9299971  0.07000294]]\n",
            "[[0.88555413 0.11444588]]\n",
            "[[0.326235 0.673765]]\n",
            "[[9.9954861e-01 4.5132005e-04]]\n",
            "[[0.99319154 0.00680847]]\n",
            "[[0.52342796 0.4765721 ]]\n",
            "[[0.14507249 0.85492754]]\n",
            "[[0.96381855 0.03618142]]\n",
            "[[0.73278546 0.2672145 ]]\n",
            "[[0.8107748  0.18922515]]\n",
            "[[0.8686271  0.13137296]]\n",
            "[[0.86939734 0.13060266]]\n",
            "[[0.9976508  0.00234915]]\n",
            "[[0.9959859  0.00401405]]\n",
            "[[0.9975569  0.00244303]]\n",
            "[[0.92173696 0.07826303]]\n",
            "[[0.9311718  0.06882826]]\n",
            "[[0.9785725  0.02142753]]\n",
            "[[9.9925023e-01 7.4973213e-04]]\n",
            "[[0.5677281 0.4322719]]\n",
            "[[0.9561675  0.04383241]]\n",
            "[[0.7822997  0.21770036]]\n",
            "[[0.87334985 0.12665018]]\n",
            "[[0.00534231 0.9946577 ]]\n",
            "[[0.44254434 0.55745566]]\n",
            "[[0.9445079  0.05549209]]\n",
            "[[0.7392331  0.26076686]]\n",
            "[[0.5351668  0.46483317]]\n",
            "[[0.08492535 0.91507465]]\n",
            "[[0.99794346 0.0020565 ]]\n",
            "[[0.9977857  0.00221434]]\n",
            "[[0.8451087  0.15489134]]\n",
            "[[0.90878856 0.09121142]]\n",
            "[[0.9450226 0.0549774]]\n",
            "[[0.8322766 0.1677234]]\n",
            "[[0.99533325 0.00466677]]\n",
            "[[0.7292847 0.2707153]]\n",
            "[[0.24287161 0.75712836]]\n",
            "[[0.9726906  0.02730937]]\n",
            "[[0.76838577 0.23161423]]\n",
            "[[9.9965024e-01 3.4972586e-04]]\n",
            "[[0.9694717  0.03052826]]\n",
            "[[0.8926946  0.10730545]]\n",
            "[[0.9874928  0.01250716]]\n",
            "[[0.9943007  0.00569927]]\n",
            "[[0.9843226 0.0156774]]\n",
            "[[0.99497694 0.00502304]]\n",
            "[[0.6939042 0.3060958]]\n",
            "[[0.99600035 0.00399961]]\n",
            "[[0.9594621  0.04053793]]\n",
            "[[9.9972123e-01 2.7879863e-04]]\n",
            "[[0.767541   0.23245905]]\n",
            "[[0.97167206 0.02832793]]\n",
            "[[0.7846591  0.21534097]]\n",
            "[[0.98413426 0.01586569]]\n",
            "[[0.998417   0.00158302]]\n",
            "[[0.64426446 0.35573554]]\n",
            "[[0.99389005 0.00610994]]\n",
            "[[9.99887824e-01 1.12132366e-04]]\n",
            "[[0.9655984  0.03440157]]\n",
            "[[0.6457887  0.35421136]]\n",
            "[[0.7833212  0.21667877]]\n",
            "[[0.9871105 0.0128895]]\n",
            "[[0.99685425 0.00314577]]\n",
            "[[0.5759014  0.42409867]]\n",
            "[[0.7584757  0.24152428]]\n",
            "[[0.9655808  0.03441913]]\n",
            "[[0.97917455 0.02082543]]\n",
            "[[0.8592151  0.14078492]]\n",
            "[[9.9927586e-01 7.2417955e-04]]\n",
            "[[9.9922466e-01 7.7534223e-04]]\n",
            "[[0.9945451  0.00545491]]\n",
            "[[0.8893492  0.11065072]]\n",
            "[[0.8962192  0.10378078]]\n",
            "[[9.998764e-01 1.236174e-04]]\n",
            "[[0.11037207 0.88962793]]\n",
            "[[0.80949146 0.1905085 ]]\n",
            "[[0.38668436 0.6133156 ]]\n",
            "[[0.44971672 0.5502833 ]]\n",
            "[[0.96276355 0.0372364 ]]\n",
            "[[0.992117   0.00788303]]\n",
            "[[0.7287126  0.27128738]]\n",
            "[[0.94216853 0.05783144]]\n",
            "[[0.8737109  0.12628911]]\n",
            "[[9.9952972e-01 4.7031598e-04]]\n",
            "[[0.99784625 0.0021537 ]]\n",
            "[[0.7683758 0.2316242]]\n",
            "[[0.7762782  0.22372177]]\n",
            "[[0.85997695 0.1400231 ]]\n",
            "[[0.968315   0.03168502]]\n",
            "[[0.48810667 0.5118934 ]]\n",
            "[[0.8703309  0.12966909]]\n",
            "[[0.77546024 0.22453974]]\n",
            "[[0.99513406 0.00486588]]\n",
            "[[0.98240626 0.01759368]]\n",
            "[[0.44219267 0.5578073 ]]\n",
            "[[0.9988657  0.00113425]]\n",
            "[[0.8118432  0.18815677]]\n",
            "[[0.897733   0.10226708]]\n",
            "[[0.9963904  0.00360958]]\n",
            "[[0.99883765 0.00116233]]\n",
            "[[0.47438136 0.5256187 ]]\n",
            "[[0.99015105 0.00984901]]\n",
            "[[9.999285e-01 7.151647e-05]]\n",
            "[[0.99244726 0.00755277]]\n",
            "[[9.9959177e-01 4.0823012e-04]]\n",
            "[[0.50190514 0.4980948 ]]\n",
            "[[0.40679678 0.5932032 ]]\n",
            "[[0.6461432  0.35385677]]\n",
            "[[0.9936168  0.00638318]]\n",
            "[[0.8078622  0.19213778]]\n",
            "[[0.99791795 0.00208209]]\n",
            "[[0.9987851  0.00121496]]\n",
            "[[0.44967148 0.5503286 ]]\n",
            "[[0.23995216 0.7600478 ]]\n",
            "[[0.68707174 0.31292823]]\n",
            "[[0.8923004  0.10769959]]\n",
            "[[0.3799412 0.6200588]]\n",
            "[[0.99871695 0.00128312]]\n",
            "[[0.9833462  0.01665376]]\n",
            "[[0.67063177 0.32936826]]\n",
            "[[0.9920128  0.00798716]]\n",
            "[[0.9691584  0.03084158]]\n",
            "[[0.94860005 0.05139993]]\n",
            "[[0.37975875 0.6202413 ]]\n",
            "[[0.88221395 0.1177861 ]]\n",
            "[[0.7872203  0.21277969]]\n",
            "[[0.53231514 0.4676849 ]]\n",
            "[[0.9861773  0.01382267]]\n",
            "[[0.00576291 0.99423707]]\n",
            "[[0.71719384 0.28280616]]\n",
            "[[0.09801864 0.90198135]]\n",
            "[[9.9995863e-01 4.1408388e-05]]\n",
            "[[0.03334083 0.9666591 ]]\n",
            "[[0.99554765 0.00445235]]\n",
            "[[0.72942865 0.27057138]]\n",
            "[[0.9880227  0.01197733]]\n",
            "[[0.9839929  0.01600716]]\n",
            "[[0.95631546 0.04368448]]\n",
            "[[0.9936255  0.00637451]]\n",
            "[[0.7874516  0.21254838]]\n",
            "[[0.9925633  0.00743667]]\n",
            "[[0.2581627 0.7418373]]\n",
            "[[0.25461984 0.74538016]]\n",
            "[[9.99270e-01 7.29987e-04]]\n",
            "[[0.9859546  0.01404548]]\n",
            "[[0.6128281  0.38717192]]\n",
            "[[0.10475085 0.8952491 ]]\n",
            "[[0.08597168 0.9140283 ]]\n",
            "[[0.9957652  0.00423473]]\n",
            "[[9.99882221e-01 1.17788055e-04]]\n",
            "[[0.7363535  0.26364645]]\n",
            "[[0.26745477 0.73254526]]\n",
            "[[0.9911246  0.00887541]]\n",
            "[[0.3212067 0.6787933]]\n",
            "[[0.15945606 0.8405439 ]]\n",
            "[[0.9956298  0.00437023]]\n",
            "[[0.9520092  0.04799087]]\n",
            "[[0.995535   0.00446504]]\n",
            "[[0.64836794 0.35163203]]\n",
            "[[0.17453304 0.82546693]]\n",
            "[[0.988451   0.01154898]]\n",
            "[[9.9942386e-01 5.7610904e-04]]\n",
            "[[0.90966254 0.09033749]]\n",
            "[[0.9626706  0.03732941]]\n",
            "[[0.29107454 0.7089254 ]]\n",
            "[[0.8654104  0.13458961]]\n",
            "[[0.7054583  0.29454175]]\n",
            "[[0.95078117 0.04921881]]\n",
            "[[9.9943525e-01 5.6474633e-04]]\n",
            "[[0.65943605 0.34056398]]\n",
            "[[0.97224706 0.02775298]]\n",
            "[[9.9979454e-01 2.0542837e-04]]\n",
            "[[0.8380749  0.16192505]]\n",
            "[[0.95443386 0.04556618]]\n",
            "[[0.77186394 0.22813606]]\n",
            "[[0.9921153 0.0078847]]\n",
            "[[0.95924884 0.04075119]]\n",
            "[[0.5939055 0.4060944]]\n",
            "[[0.9872371  0.01276294]]\n",
            "[[0.6471491  0.35285094]]\n",
            "[[0.0289595 0.9710405]]\n",
            "[[0.9935887  0.00641133]]\n",
            "[[0.67406064 0.32593933]]\n",
            "[[9.9989760e-01 1.0240693e-04]]\n",
            "[[9.9995518e-01 4.4791694e-05]]\n",
            "[[0.5951292 0.4048708]]\n",
            "[[0.83525425 0.16474575]]\n",
            "[[9.990452e-01 9.548013e-04]]\n",
            "[[0.5864318  0.41356826]]\n",
            "[[0.89284086 0.10715914]]\n",
            "[[0.9803017  0.01969835]]\n",
            "[[0.9788785  0.02112152]]\n",
            "[[9.992667e-01 7.332663e-04]]\n",
            "[[0.9669388  0.03306121]]\n",
            "[[0.9584632  0.04153676]]\n",
            "[[0.94950664 0.05049342]]\n",
            "[[9.9978679e-01 2.1318471e-04]]\n",
            "[[0.93372273 0.06627721]]\n",
            "[[0.58981854 0.41018143]]\n",
            "[[0.4142018 0.5857982]]\n",
            "[[9.9981517e-01 1.8487638e-04]]\n",
            "[[9.9998271e-01 1.7318624e-05]]\n",
            "[[0.828857   0.17114297]]\n",
            "[[0.39203352 0.6079664 ]]\n",
            "[[0.98540384 0.01459613]]\n",
            "[[0.98131806 0.01868199]]\n",
            "[[0.9766558  0.02334424]]\n",
            "[[0.30542973 0.69457024]]\n",
            "[[0.97714543 0.02285452]]\n",
            "[[0.5191555  0.48084444]]\n",
            "[[0.9970061  0.00299386]]\n",
            "[[0.9980793  0.00192069]]\n",
            "[[0.9089019  0.09109811]]\n",
            "[[0.98396504 0.01603497]]\n",
            "[[0.83775127 0.16224867]]\n",
            "[[0.99222624 0.00777373]]\n",
            "[[0.9146923  0.08530774]]\n",
            "[[0.55337584 0.44662416]]\n",
            "[[9.999933e-01 6.668201e-06]]\n",
            "[[0.8682675  0.13173255]]\n",
            "[[0.62375706 0.3762429 ]]\n",
            "[[0.03749279 0.9625072 ]]\n",
            "[[0.99264985 0.00735015]]\n",
            "[[0.99627894 0.00372105]]\n",
            "[[0.89243275 0.10756725]]\n",
            "[[0.55021864 0.44978142]]\n",
            "[[0.764178   0.23582198]]\n",
            "[[0.42404583 0.5759542 ]]\n",
            "[[0.92623484 0.07376518]]\n",
            "[[9.9976355e-01 2.3645912e-04]]\n",
            "[[0.439305 0.560695]]\n",
            "[[0.7565263  0.24347368]]\n",
            "[[0.98675275 0.01324723]]\n",
            "[[0.6942087  0.30579132]]\n",
            "[[0.14904034 0.85095966]]\n",
            "[[0.72083014 0.27916986]]\n",
            "[[0.84711725 0.15288277]]\n",
            "[[0.9514849  0.04851504]]\n",
            "[[9.991479e-01 8.520980e-04]]\n",
            "[[0.96237457 0.03762547]]\n",
            "[[0.0706766 0.9293234]]\n",
            "[[0.98958594 0.01041406]]\n",
            "[[0.99570185 0.00429808]]\n",
            "[[0.9952361  0.00476389]]\n",
            "[[0.78285587 0.21714415]]\n",
            "[[0.89664924 0.10335079]]\n",
            "[[0.99238026 0.00761972]]\n",
            "[[0.9974262  0.00257377]]\n",
            "[[0.99318105 0.00681893]]\n",
            "[[9.9935836e-01 6.4159982e-04]]\n",
            "[[9.9988329e-01 1.1674689e-04]]\n",
            "[[0.37569112 0.6243089 ]]\n",
            "[[0.99855906 0.001441  ]]\n",
            "[[0.47926825 0.52073175]]\n",
            "[[0.9651536  0.03484643]]\n",
            "[[0.9894196  0.01058042]]\n",
            "[[9.9981946e-01 1.8060519e-04]]\n",
            "[[9.9999309e-01 6.9523503e-06]]\n",
            "[[0.95419425 0.04580579]]\n",
            "[[0.9978672  0.00213273]]\n",
            "[[0.97855157 0.0214484 ]]\n",
            "[[0.99480516 0.0051948 ]]\n",
            "[[0.8748571  0.12514286]]\n",
            "[[0.9704109  0.02958913]]\n",
            "[[0.99452966 0.00547036]]\n",
            "[[0.45892224 0.5410778 ]]\n",
            "[[0.93219537 0.06780463]]\n",
            "[[0.7450377  0.25496233]]\n",
            "[[9.9991155e-01 8.8450994e-05]]\n",
            "[[0.23913147 0.76086855]]\n",
            "[[0.60418147 0.39581856]]\n",
            "[[0.8443393  0.15566075]]\n",
            "[[9.9959737e-01 4.0262417e-04]]\n",
            "[[0.3749515  0.62504846]]\n",
            "[[0.982778   0.01722199]]\n",
            "[[0.7640398  0.23596014]]\n",
            "[[0.9569612  0.04303879]]\n",
            "[[0.9048526  0.09514738]]\n",
            "[[0.89906365 0.10093638]]\n",
            "[[0.6514357  0.34856436]]\n",
            "[[0.9288811  0.07111897]]\n",
            "[[0.31835634 0.68164366]]\n",
            "[[0.28771842 0.7122815 ]]\n",
            "[[0.4388836 0.5611164]]\n",
            "[[0.99565804 0.00434193]]\n",
            "[[0.88117164 0.11882829]]\n",
            "[[0.9197516  0.08024848]]\n",
            "[[0.98933    0.01067002]]\n",
            "[[0.8331088  0.16689123]]\n",
            "[[0.8755981  0.12440192]]\n",
            "[[0.9970577 0.0029423]]\n",
            "[[0.97639096 0.02360905]]\n",
            "[[0.04634958 0.9536505 ]]\n",
            "[[0.9972178  0.00278222]]\n",
            "[[0.9419469  0.05805306]]\n",
            "[[9.9999642e-01 3.6187341e-06]]\n",
            "[[0.9685628  0.03143716]]\n",
            "[[0.9415106  0.05848943]]\n",
            "[[0.9976216  0.00237842]]\n",
            "[[0.9927186  0.00728142]]\n",
            "[[0.02709904 0.972901  ]]\n",
            "[[0.8011953  0.19880463]]\n",
            "[[0.91972667 0.08027331]]\n",
            "[[0.91179246 0.08820753]]\n",
            "[[0.98039186 0.0196082 ]]\n",
            "[[9.9961168e-01 3.8829388e-04]]\n",
            "[[0.9422717  0.05772832]]\n",
            "[[0.99675745 0.00324252]]\n",
            "[[0.9954656  0.00453444]]\n",
            "[[0.7783142  0.22168577]]\n",
            "[[0.00819016 0.9918099 ]]\n",
            "[[0.9981166  0.00188346]]\n",
            "[[0.96059567 0.03940435]]\n",
            "[[0.8690154  0.13098463]]\n",
            "[[0.9983333 0.0016667]]\n",
            "[[0.13900372 0.8609963 ]]\n",
            "[[0.41256583 0.58743423]]\n",
            "[[0.9913777  0.00862229]]\n",
            "[[0.9008899  0.09911013]]\n",
            "[[0.9928035  0.00719648]]\n",
            "[[0.99833244 0.00166753]]\n",
            "[[0.9472553 0.0527447]]\n",
            "[[0.8480958 0.1519042]]\n",
            "[[0.5584243 0.4415758]]\n",
            "[[0.988951   0.01104896]]\n",
            "[[0.9888553  0.01114474]]\n",
            "[[0.99868053 0.0013194 ]]\n",
            "[[0.5283209  0.47167903]]\n",
            "[[0.7708211 0.2291789]]\n",
            "[[0.24175602 0.758244  ]]\n",
            "[[0.9972156  0.00278435]]\n",
            "[[0.898198   0.10180203]]\n",
            "[[0.07361357 0.9263864 ]]\n",
            "[[0.9775792  0.02242077]]\n",
            "[[0.99622715 0.00377285]]\n",
            "[[0.46340516 0.53659487]]\n",
            "[[0.26417416 0.73582584]]\n",
            "[[0.53732944 0.4626705 ]]\n",
            "[[0.6489421 0.3510579]]\n",
            "[[0.7841321  0.21586788]]\n",
            "[[0.9342688  0.06573118]]\n",
            "[[0.91304606 0.08695392]]\n",
            "[[0.93583    0.06417005]]\n",
            "[[9.9962878e-01 3.7117483e-04]]\n",
            "[[0.9112657  0.08873429]]\n",
            "[[9.9975950e-01 2.4048667e-04]]\n",
            "[[0.8935592  0.10644078]]\n",
            "[[0.9951409  0.00485908]]\n",
            "[[0.978018   0.02198202]]\n",
            "[[0.70248604 0.29751396]]\n",
            "[[0.7021744  0.29782563]]\n",
            "[[0.99223834 0.00776169]]\n",
            "[[0.64644617 0.35355383]]\n",
            "[[0.8458863  0.15411367]]\n",
            "[[0.99436885 0.00563109]]\n",
            "[[0.9963742 0.0036258]]\n",
            "[[0.8957476  0.10425242]]\n",
            "[[0.99825746 0.00174251]]\n",
            "[[0.9943461  0.00565394]]\n",
            "[[0.9849422  0.01505781]]\n",
            "[[0.8469849  0.15301505]]\n",
            "[[0.98756194 0.012438  ]]\n",
            "[[0.90630776 0.09369218]]\n",
            "[[0.4656413  0.53435874]]\n",
            "[[0.93477297 0.06522702]]\n",
            "[[0.99634486 0.00365518]]\n",
            "[[0.8590122 0.1409878]]\n",
            "[[0.96441644 0.03558359]]\n",
            "[[0.98405844 0.01594162]]\n",
            "[[0.9735537  0.02644625]]\n",
            "[[0.9647508  0.03524913]]\n",
            "[[0.9804616  0.01953841]]\n",
            "[[9.9998236e-01 1.7612985e-05]]\n",
            "[[9.9956638e-01 4.3363313e-04]]\n",
            "[[9.9977320e-01 2.2681402e-04]]\n",
            "[[0.99734515 0.00265493]]\n",
            "[[9.993411e-01 6.589549e-04]]\n",
            "[[0.8108912  0.18910882]]\n",
            "[[0.99614847 0.00385156]]\n",
            "[[0.6436814  0.35631862]]\n",
            "[[0.48916876 0.5108313 ]]\n",
            "[[0.5907933 0.4092067]]\n",
            "[[9.999367e-01 6.324970e-05]]\n",
            "[[0.8606343  0.13936569]]\n",
            "[[0.88148963 0.1185104 ]]\n",
            "[[0.1212343 0.8787657]]\n",
            "[[0.05122392 0.9487761 ]]\n",
            "[[0.62361944 0.37638062]]\n",
            "[[9.999089e-01 9.106427e-05]]\n",
            "[[0.9870964  0.01290353]]\n",
            "[[0.9947884  0.00521155]]\n",
            "[[0.1392553 0.8607447]]\n",
            "[[0.966965   0.03303502]]\n",
            "[[0.5891835 0.4108165]]\n",
            "[[0.9734727  0.02652734]]\n",
            "[[0.97078806 0.02921189]]\n",
            "[[0.8729131 0.1270869]]\n",
            "[[0.9813121  0.01868787]]\n",
            "[[0.9068611  0.09313885]]\n",
            "[[0.00228264 0.9977174 ]]\n",
            "[[0.87141603 0.12858398]]\n",
            "[[0.4878951 0.512105 ]]\n",
            "[[0.9577632  0.04223688]]\n",
            "[[0.924238   0.07576192]]\n",
            "[[9.998418e-01 1.581645e-04]]\n",
            "[[9.990349e-01 9.651631e-04]]\n",
            "[[9.9985015e-01 1.4986900e-04]]\n",
            "[[0.74229586 0.25770414]]\n",
            "[[0.99826056 0.00173946]]\n",
            "[[0.8081158 0.1918842]]\n",
            "[[0.9936207  0.00637925]]\n",
            "[[0.94695055 0.05304941]]\n",
            "[[0.8743408 0.1256592]]\n",
            "[[9.9917573e-01 8.2427904e-04]]\n",
            "[[0.9338989  0.06610107]]\n",
            "[[0.6343777  0.36562228]]\n",
            "[[9.9970835e-01 2.9159765e-04]]\n",
            "[[0.79958135 0.2004186 ]]\n",
            "[[0.8693648  0.13063519]]\n",
            "[[0.88679206 0.11320794]]\n",
            "[[0.39770332 0.60229665]]\n",
            "[[0.91333574 0.08666424]]\n",
            "[[0.9927322  0.00726782]]\n",
            "[[9.9955553e-01 4.4439366e-04]]\n",
            "[[0.7392883  0.26071176]]\n",
            "[[0.98850775 0.0114923 ]]\n",
            "[[0.9739034  0.02609655]]\n",
            "[[0.47603974 0.5239602 ]]\n",
            "[[0.14926945 0.85073054]]\n",
            "[[0.9891046  0.01089536]]\n",
            "[[0.9790364  0.02096363]]\n",
            "[[0.6409308  0.35906932]]\n",
            "[[0.9806008  0.01939919]]\n",
            "[[0.88922423 0.11077579]]\n",
            "[[9.9981922e-01 1.8078677e-04]]\n",
            "[[0.46942586 0.5305742 ]]\n",
            "[[0.69719297 0.30280694]]\n",
            "[[0.9317969  0.06820302]]\n",
            "[[9.9999344e-01 6.5025733e-06]]\n",
            "[[0.34560886 0.65439117]]\n",
            "[[0.82098293 0.17901711]]\n",
            "[[0.5448116  0.45518836]]\n",
            "[[9.9943656e-01 5.6343846e-04]]\n",
            "[[0.9931358  0.00686423]]\n",
            "[[0.9860945  0.01390555]]\n",
            "[[0.98168486 0.01831517]]\n",
            "[[0.99466515 0.0053348 ]]\n",
            "[[0.9964301  0.00356993]]\n",
            "[[0.9975885  0.00241147]]\n",
            "[[0.9199043  0.08009569]]\n",
            "[[0.62270254 0.37729746]]\n",
            "[[0.04887557 0.95112437]]\n",
            "[[0.7239258 0.2760742]]\n",
            "[[0.47998247 0.52001756]]\n",
            "[[9.993907e-01 6.093050e-04]]\n",
            "[[0.7144007  0.28559932]]\n",
            "[[0.91148895 0.08851105]]\n",
            "[[0.99501437 0.0049856 ]]\n",
            "[[0.981093   0.01890707]]\n",
            "[[0.94232327 0.05767675]]\n",
            "[[0.93199587 0.06800408]]\n",
            "[[0.93765974 0.06234027]]\n",
            "[[0.94484735 0.05515268]]\n",
            "[[0.9278125  0.07218748]]\n",
            "[[0.9972844  0.00271563]]\n",
            "[[0.99381614 0.00618383]]\n",
            "[[9.9943334e-01 5.6665071e-04]]\n",
            "[[0.97315234 0.02684764]]\n",
            "[[0.9756537 0.0243463]]\n",
            "[[0.59093636 0.40906358]]\n",
            "[[0.99824464 0.00175531]]\n",
            "[[0.904971   0.09502896]]\n",
            "[[0.4943974  0.50560266]]\n",
            "[[0.8470039  0.15299612]]\n",
            "[[0.6218688 0.3781312]]\n",
            "[[0.88013184 0.11986813]]\n",
            "[[0.98466855 0.01533147]]\n",
            "[[0.9791309  0.02086912]]\n",
            "[[0.02297062 0.9770294 ]]\n",
            "[[0.98380166 0.01619833]]\n",
            "[[0.8380258  0.16197425]]\n",
            "[[9.9925381e-01 7.4616756e-04]]\n",
            "[[9.9996281e-01 3.7142545e-05]]\n",
            "[[9.9974138e-01 2.5865962e-04]]\n",
            "[[0.9745968  0.02540324]]\n",
            "[[0.8433742  0.15662575]]\n",
            "[[0.7218901  0.27810994]]\n",
            "[[0.99823135 0.00176868]]\n",
            "[[0.9972772  0.00272281]]\n",
            "[[0.8644395  0.13556054]]\n",
            "[[0.9988669  0.00113312]]\n",
            "[[0.7759346 0.2240654]]\n",
            "[[0.5715864 0.4284137]]\n",
            "[[0.22370261 0.77629745]]\n",
            "[[0.994793   0.00520697]]\n",
            "[[0.27694452 0.7230555 ]]\n",
            "[[0.9625485  0.03745152]]\n",
            "[[0.9747286  0.02527138]]\n",
            "[[0.622083   0.37791702]]\n",
            "[[0.09860557 0.9013944 ]]\n",
            "[[9.994616e-01 5.383872e-04]]\n",
            "[[0.9804254  0.01957456]]\n",
            "[[0.7744352  0.22556482]]\n",
            "[[0.994743   0.00525698]]\n",
            "[[0.9225854  0.07741462]]\n",
            "[[0.9978669  0.00213316]]\n",
            "[[0.9912248  0.00877517]]\n",
            "[[0.99278426 0.0072158 ]]\n",
            "[[0.57759327 0.42240676]]\n",
            "[[0.99505264 0.00494743]]\n",
            "[[0.97243226 0.02756772]]\n",
            "[[0.9661991 0.0338009]]\n",
            "[[0.96060556 0.0393944 ]]\n",
            "[[0.99025774 0.00974228]]\n",
            "[[9.9932075e-01 6.7932287e-04]]\n",
            "[[0.9187013  0.08129871]]\n",
            "[[9.995567e-01 4.432290e-04]]\n",
            "[[0.687061   0.31293902]]\n",
            "[[0.8894966  0.11050337]]\n",
            "[[0.9079079 0.0920921]]\n",
            "[[0.6291365 0.3708635]]\n",
            "[[0.9802663  0.01973369]]\n",
            "[[0.97103304 0.02896699]]\n",
            "[[0.89913166 0.1008683 ]]\n",
            "[[0.98544925 0.01455072]]\n",
            "[[0.27743632 0.7225637 ]]\n",
            "[[0.9887693 0.0112307]]\n",
            "[[0.53332216 0.46667787]]\n",
            "[[0.7869144  0.21308568]]\n",
            "[[0.7997724  0.20022759]]\n",
            "[[0.99521506 0.00478495]]\n",
            "[[0.34150717 0.6584928 ]]\n",
            "[[0.3424204  0.65757966]]\n",
            "[[0.9620129  0.03798716]]\n",
            "[[0.79341966 0.2065803 ]]\n",
            "[[0.9698835  0.03011642]]\n",
            "[[0.69030577 0.3096942 ]]\n",
            "[[0.97986996 0.02013001]]\n",
            "[[0.92124134 0.07875866]]\n",
            "[[0.74524784 0.25475216]]\n",
            "[[9.9944955e-01 5.5048463e-04]]\n",
            "[[0.634912 0.365088]]\n",
            "[[0.6046379  0.39536208]]\n",
            "[[0.71174014 0.28825977]]\n",
            "[[0.1000911 0.8999089]]\n",
            "[[0.9680177  0.03198228]]\n",
            "[[9.9935573e-01 6.4429757e-04]]\n",
            "[[9.9999464e-01 5.3812755e-06]]\n",
            "[[0.59398794 0.406012  ]]\n",
            "[[0.40843704 0.591563  ]]\n",
            "[[0.6454661  0.35453394]]\n",
            "[[0.9979425  0.00205752]]\n",
            "[[9.9925357e-01 7.4638275e-04]]\n",
            "[[0.96370006 0.03629992]]\n",
            "[[0.95264566 0.04735437]]\n",
            "[[0.86976105 0.13023889]]\n",
            "[[0.7049581  0.29504192]]\n",
            "[[0.9829632  0.01703685]]\n",
            "[[0.3794063  0.62059367]]\n",
            "[[0.5923838 0.4076163]]\n",
            "[[9.9997187e-01 2.8159055e-05]]\n",
            "[[0.9985415  0.00145848]]\n",
            "[[0.99886847 0.00113158]]\n",
            "[[0.9866905  0.01330952]]\n",
            "[[9.9997807e-01 2.1951773e-05]]\n",
            "[[0.93691057 0.0630895 ]]\n",
            "[[0.99428856 0.00571136]]\n",
            "[[9.996123e-01 3.877407e-04]]\n",
            "[[0.39284265 0.6071573 ]]\n",
            "[[0.96733946 0.03266058]]\n",
            "[[0.00814309 0.99185693]]\n",
            "[[0.9936273  0.00637266]]\n",
            "[[0.95453423 0.04546583]]\n",
            "[[0.9844631  0.01553697]]\n",
            "[[0.3185894 0.6814107]]\n",
            "[[0.9893764  0.01062359]]\n",
            "[[0.5615074  0.43849266]]\n",
            "[[0.9894946  0.01050537]]\n",
            "[[0.58979845 0.4102015 ]]\n",
            "[[9.9998355e-01 1.6500222e-05]]\n",
            "[[0.4923035  0.50769645]]\n",
            "[[0.9913523  0.00864773]]\n",
            "[[0.8721666  0.12783341]]\n",
            "[[0.91717315 0.08282682]]\n",
            "[[0.9243853  0.07561472]]\n",
            "[[0.86011237 0.13988766]]\n",
            "[[0.9968374  0.00316264]]\n",
            "[[9.9925119e-01 7.4875104e-04]]\n",
            "[[9.9908125e-01 9.1875368e-04]]\n",
            "[[0.9620951  0.03790489]]\n",
            "[[0.28832105 0.7116789 ]]\n",
            "[[0.40497497 0.595025  ]]\n",
            "[[0.15611371 0.8438863 ]]\n",
            "[[0.7688854  0.23111461]]\n",
            "[[0.9950883  0.00491169]]\n",
            "[[0.00146046 0.99853957]]\n",
            "[[0.979218   0.02078203]]\n",
            "[[9.996215e-01 3.784968e-04]]\n",
            "[[0.3510885 0.6489115]]\n",
            "[[0.7083184  0.29168162]]\n",
            "[[0.99336845 0.00663159]]\n",
            "[[0.9467787  0.05322132]]\n",
            "[[0.7030114  0.29698864]]\n",
            "[[0.90744585 0.09255421]]\n",
            "[[0.92256075 0.07743918]]\n",
            "[[0.87184477 0.1281552 ]]\n",
            "[[0.95066726 0.04933269]]\n",
            "[[0.8999042  0.10009583]]\n",
            "[[0.21451843 0.7854816 ]]\n",
            "[[0.9882632  0.01173689]]\n",
            "[[0.82832384 0.17167619]]\n",
            "[[0.42364672 0.57635325]]\n",
            "[[0.8837863  0.11621373]]\n",
            "[[0.98345584 0.01654418]]\n",
            "[[0.8519377  0.14806235]]\n",
            "[[0.89850056 0.10149939]]\n",
            "[[0.7187718 0.2812282]]\n",
            "[[0.5203976 0.4796024]]\n",
            "[[9.990294e-01 9.706718e-04]]\n",
            "[[0.98187166 0.0181283 ]]\n",
            "[[0.83262974 0.16737022]]\n",
            "[[0.9982292  0.00177084]]\n",
            "[[0.2568699 0.7431301]]\n",
            "[[0.9452755  0.05472453]]\n",
            "[[0.16294816 0.83705187]]\n",
            "[[9.9973625e-01 2.6376641e-04]]\n",
            "[[0.99504685 0.00495313]]\n",
            "[[0.97241116 0.02758881]]\n",
            "[[0.9379374 0.0620626]]\n",
            "[[0.23656528 0.76343465]]\n",
            "[[0.9989987  0.00100125]]\n",
            "[[0.52336127 0.47663867]]\n",
            "[[0.8511852  0.14881484]]\n",
            "[[0.9807522  0.01924776]]\n",
            "[[9.9919826e-01 8.0178143e-04]]\n",
            "[[0.98761755 0.01238243]]\n",
            "[[0.9876502  0.01234977]]\n",
            "[[0.9605742  0.03942577]]\n",
            "[[0.5807159  0.41928408]]\n",
            "[[0.77272654 0.22727343]]\n",
            "[[0.98137146 0.01862858]]\n",
            "[[0.51619065 0.48380932]]\n",
            "[[0.93500787 0.06499211]]\n",
            "[[0.87529147 0.1247085 ]]\n",
            "[[0.84659135 0.1534086 ]]\n",
            "[[0.92021215 0.07978779]]\n",
            "[[0.9728473  0.02715277]]\n",
            "[[0.6508108  0.34918922]]\n",
            "[[9.999318e-01 6.820958e-05]]\n",
            "[[0.54423475 0.45576522]]\n",
            "[[0.5341187  0.46588132]]\n",
            "[[0.9632974  0.03670252]]\n",
            "[[0.8948109  0.10518913]]\n",
            "[[0.05756274 0.9424373 ]]\n",
            "[[0.5700638  0.42993623]]\n",
            "[[0.33508593 0.6649141 ]]\n",
            "[[0.23285645 0.76714355]]\n",
            "[[9.999199e-01 8.009579e-05]]\n",
            "[[0.8736679  0.12633204]]\n",
            "[[0.8144242  0.18557575]]\n",
            "[[0.96318096 0.03681905]]\n",
            "[[0.99627244 0.00372751]]\n",
            "[[0.98550075 0.01449925]]\n",
            "[[0.88343316 0.11656681]]\n",
            "[[0.987956 0.012044]]\n",
            "[[0.9644608  0.03553918]]\n",
            "[[0.9911069  0.00889303]]\n",
            "[[0.94781893 0.0521811 ]]\n",
            "[[0.9469795  0.05302045]]\n",
            "[[0.25106847 0.7489316 ]]\n",
            "[[0.19551782 0.80448216]]\n",
            "[[0.9919194  0.00808056]]\n",
            "[[0.94706976 0.05293025]]\n",
            "[[0.993504   0.00649599]]\n",
            "[[0.94379026 0.05620972]]\n",
            "[[0.9186851  0.08131495]]\n",
            "[[0.572732   0.42726806]]\n",
            "[[0.13942206 0.86057794]]\n",
            "[[9.9957043e-01 4.2954326e-04]]\n",
            "[[9.9996495e-01 3.5005589e-05]]\n",
            "[[0.99768317 0.00231679]]\n",
            "[[0.8619282  0.13807175]]\n",
            "[[0.99839884 0.00160117]]\n",
            "[[0.8666366  0.13336338]]\n",
            "[[0.24019144 0.7598085 ]]\n",
            "[[9.9962878e-01 3.7114843e-04]]\n",
            "[[0.12820704 0.871793  ]]\n",
            "[[9.9999034e-01 9.6500462e-06]]\n",
            "[[0.32689843 0.6731016 ]]\n",
            "[[0.975903 0.024097]]\n",
            "[[0.49255425 0.5074457 ]]\n",
            "[[0.99743795 0.00256199]]\n",
            "[[0.9844057  0.01559438]]\n",
            "[[0.89083546 0.10916462]]\n",
            "[[0.9242393 0.0757608]]\n",
            "[[0.8422534  0.15774658]]\n",
            "[[0.6463208  0.35367918]]\n",
            "[[0.98443687 0.01556312]]\n",
            "[[0.70189196 0.298108  ]]\n",
            "[[0.992388   0.00761203]]\n",
            "[[9.9932301e-01 6.7702937e-04]]\n",
            "[[0.94703877 0.0529612 ]]\n",
            "[[0.67306376 0.32693627]]\n",
            "[[0.99680626 0.00319372]]\n",
            "[[0.96528727 0.03471278]]\n",
            "[[0.7692577  0.23074225]]\n",
            "[[0.67104256 0.3289574 ]]\n",
            "[[0.97897094 0.02102903]]\n",
            "[[0.91392463 0.0860754 ]]\n",
            "[[0.99198127 0.00801874]]\n",
            "[[0.18014716 0.8198528 ]]\n",
            "[[0.20519777 0.7948022 ]]\n",
            "[[0.7634775  0.23652257]]\n",
            "[[0.99886715 0.00113289]]\n",
            "[[0.04644549 0.9535545 ]]\n",
            "[[0.7221558  0.27784416]]\n",
            "[[0.8950947  0.10490532]]\n",
            "[[0.94760275 0.05239724]]\n",
            "[[0.9970632  0.00293672]]\n",
            "[[0.930738   0.06926209]]\n",
            "[[0.9978606  0.00213943]]\n",
            "[[0.31668624 0.6833138 ]]\n",
            "[[0.7599155  0.24008451]]\n",
            "[[0.7220863  0.27791366]]\n",
            "[[0.2789812 0.7210188]]\n",
            "[[0.97451437 0.02548563]]\n",
            "[[0.5685162 0.4314838]]\n",
            "[[0.9872442  0.01275581]]\n",
            "[[0.9271733  0.07282663]]\n",
            "[[0.9911589  0.00884113]]\n",
            "[[0.72703207 0.2729679 ]]\n",
            "[[0.7185907  0.28140923]]\n",
            "[[0.660294 0.339706]]\n",
            "[[0.9927273 0.0072727]]\n",
            "[[3.0148344e-04 9.9969852e-01]]\n",
            "[[0.67130417 0.32869586]]\n",
            "[[0.82564664 0.17435333]]\n",
            "[[0.7058048 0.2941952]]\n",
            "[[0.99878436 0.00121571]]\n",
            "[[0.8731383  0.12686172]]\n",
            "[[0.9896034  0.01039656]]\n",
            "[[0.870695   0.12930506]]\n",
            "[[0.99604803 0.00395191]]\n",
            "[[9.9973315e-01 2.6684222e-04]]\n",
            "[[0.9541047  0.04589527]]\n",
            "[[0.99739623 0.00260381]]\n",
            "[[0.9526423  0.04735765]]\n",
            "[[0.36812505 0.631875  ]]\n",
            "[[0.9602802  0.03971982]]\n",
            "[[9.991351e-01 8.649972e-04]]\n",
            "[[9.9980134e-01 1.9870418e-04]]\n",
            "[[0.40745276 0.59254724]]\n",
            "[[0.99418455 0.00581549]]\n",
            "[[0.9917292  0.00827084]]\n",
            "[[0.9833802  0.01661983]]\n",
            "[[9.9993515e-01 6.4900305e-05]]\n",
            "[[0.99804544 0.00195458]]\n",
            "[[0.9965779  0.00342205]]\n",
            "[[0.98514384 0.01485623]]\n",
            "[[0.8255057 0.1744943]]\n",
            "[[0.8125353  0.18746467]]\n",
            "[[9.9999249e-01 7.5174007e-06]]\n",
            "[[0.25918075 0.7408192 ]]\n",
            "[[9.9994624e-01 5.3770160e-05]]\n",
            "[[0.99625325 0.00374676]]\n",
            "[[0.93868256 0.06131744]]\n",
            "[[0.9936113  0.00638871]]\n",
            "[[0.9821722  0.01782776]]\n",
            "[[0.99860954 0.00139047]]\n",
            "[[0.25011826 0.7498818 ]]\n",
            "[[9.9958664e-01 4.1336694e-04]]\n",
            "[[0.5299433 0.4700567]]\n",
            "[[0.37335792 0.62664205]]\n",
            "[[0.72919965 0.2708003 ]]\n",
            "[[0.8615219  0.13847801]]\n",
            "[[0.9941341  0.00586594]]\n",
            "[[0.81848717 0.1815128 ]]\n",
            "[[0.9875557  0.01244429]]\n",
            "[[0.9736371  0.02636294]]\n",
            "[[0.77692825 0.22307177]]\n",
            "[[0.9594077  0.04059231]]\n",
            "[[0.7554275 0.2445726]]\n",
            "[[0.6483716  0.35162848]]\n",
            "[[0.9426005  0.05739948]]\n",
            "[[9.9990141e-01 9.8598146e-05]]\n",
            "[[0.9980198  0.00198021]]\n",
            "[[9.9964833e-01 3.5172151e-04]]\n",
            "[[0.9063962  0.09360383]]\n",
            "[[9.9947995e-01 5.2002183e-04]]\n",
            "[[0.8890705 0.1109295]]\n",
            "[[0.9045908  0.09540916]]\n",
            "[[0.67613256 0.3238674 ]]\n",
            "[[0.99574995 0.00425006]]\n",
            "[[0.9464915  0.05350853]]\n",
            "[[0.94216156 0.0578384 ]]\n",
            "[[9.9978715e-01 2.1282928e-04]]\n",
            "[[0.8711501  0.12884995]]\n",
            "[[0.99162865 0.00837135]]\n",
            "[[0.9828069  0.01719311]]\n",
            "[[0.99676114 0.00323893]]\n",
            "[[0.9074746  0.09252546]]\n",
            "[[0.85576427 0.14423569]]\n",
            "[[0.9852688  0.01473122]]\n",
            "[[0.8718158  0.12818423]]\n",
            "[[0.94688386 0.05311617]]\n",
            "[[0.96912766 0.0308723 ]]\n",
            "[[0.26176682 0.7382332 ]]\n",
            "[[0.9821799  0.01782014]]\n",
            "[[0.996673   0.00332695]]\n",
            "[[0.98173887 0.01826118]]\n",
            "[[0.9047654  0.09523459]]\n",
            "[[0.99425286 0.00574707]]\n",
            "[[9.991621e-01 8.378847e-04]]\n",
            "[[0.7652387  0.23476124]]\n",
            "[[0.9799866  0.02001336]]\n",
            "[[0.81865156 0.18134838]]\n",
            "[[0.94662946 0.05337055]]\n",
            "[[0.9930443  0.00695562]]\n",
            "[[0.87199134 0.1280087 ]]\n",
            "[[0.863271 0.136729]]\n",
            "[[0.8908059  0.10919408]]\n",
            "[[0.9983486  0.00165145]]\n",
            "[[0.98767763 0.01232236]]\n",
            "[[0.4794646  0.52053547]]\n",
            "[[0.31531784 0.68468213]]\n",
            "[[0.2001021 0.7998979]]\n",
            "[[0.94973797 0.05026208]]\n",
            "[[0.99817777 0.0018223 ]]\n",
            "[[0.9329775  0.06702245]]\n",
            "[[9.997086e-01 2.913895e-04]]\n",
            "[[0.8768083 0.1231917]]\n",
            "[[0.9893606  0.01063947]]\n",
            "[[0.90692264 0.09307741]]\n",
            "[[0.6949522 0.3050478]]\n",
            "[[0.6594091  0.34059098]]\n",
            "[[0.9595629  0.04043707]]\n",
            "[[0.9986941  0.00130592]]\n",
            "[[0.37825808 0.62174195]]\n",
            "[[0.9918914  0.00810859]]\n",
            "[[0.9609033  0.03909674]]\n",
            "[[0.04523288 0.95476717]]\n",
            "[[0.886225   0.11377498]]\n",
            "[[0.9874982  0.01250176]]\n",
            "[[0.9827277  0.01727233]]\n",
            "[[0.85918915 0.14081083]]\n",
            "[[0.9964097  0.00359023]]\n",
            "[[0.61867005 0.38132992]]\n",
            "[[9.9946064e-01 5.3934107e-04]]\n",
            "[[0.9679576  0.03204235]]\n",
            "[[0.9420221  0.05797784]]\n",
            "[[0.7515855  0.24841455]]\n",
            "[[0.9978288  0.00217118]]\n",
            "[[0.91828793 0.08171213]]\n",
            "[[0.8174673  0.18253274]]\n",
            "[[0.99894744 0.00105261]]\n",
            "[[0.47230512 0.5276949 ]]\n",
            "[[0.08258633 0.9174137 ]]\n",
            "[[9.9951661e-01 4.8341666e-04]]\n",
            "[[0.9982382 0.0017618]]\n",
            "[[9.994404e-01 5.595567e-04]]\n",
            "[[0.89315724 0.1068427 ]]\n",
            "[[0.48540404 0.514596  ]]\n",
            "[[9.996754e-01 3.245664e-04]]\n",
            "[[0.4895819 0.5104181]]\n",
            "[[0.86569476 0.13430525]]\n",
            "[[0.9768514  0.02314857]]\n",
            "[[9.9998021e-01 1.9761925e-05]]\n",
            "[[0.61649513 0.38350487]]\n",
            "[[0.91175985 0.08824018]]\n",
            "[[9.9994850e-01 5.1534425e-05]]\n",
            "[[0.94254816 0.05745181]]\n",
            "[[0.9366543  0.06334568]]\n",
            "[[9.999826e-01 1.742302e-05]]\n",
            "[[0.9986664  0.00133357]]\n",
            "[[0.59266114 0.4073388 ]]\n",
            "[[1.4095672e-04 9.9985898e-01]]\n",
            "[[0.9987758  0.00122423]]\n",
            "[[0.20387946 0.7961205 ]]\n",
            "[[0.0722644 0.9277356]]\n",
            "[[0.8079226  0.19207734]]\n",
            "[[9.9994695e-01 5.3060503e-05]]\n",
            "[[0.6650616  0.33493838]]\n",
            "[[0.6774331  0.32256687]]\n",
            "[[0.8736567  0.12634327]]\n",
            "[[0.7842648  0.21573511]]\n",
            "[[0.8190974  0.18090262]]\n",
            "[[0.49424574 0.50575423]]\n",
            "[[0.9938718  0.00612817]]\n",
            "[[0.9978192  0.00218083]]\n",
            "[[0.9980556  0.00194443]]\n",
            "[[0.9467956  0.05320441]]\n",
            "[[0.91919446 0.08080558]]\n",
            "[[0.3461908 0.6538092]]\n",
            "[[0.9978277  0.00217227]]\n",
            "[[0.11757273 0.8824273 ]]\n",
            "[[0.652486   0.34751394]]\n",
            "[[0.21403754 0.78596246]]\n",
            "[[0.84970516 0.15029487]]\n",
            "[[0.9974763 0.0025237]]\n",
            "[[0.9967127  0.00328734]]\n",
            "[[0.604952   0.39504802]]\n",
            "[[0.99737644 0.0026236 ]]\n",
            "[[0.99788934 0.00211069]]\n",
            "[[0.56410897 0.43589106]]\n",
            "[[0.19761656 0.8023834 ]]\n",
            "[[0.98573154 0.01426841]]\n",
            "[[9.9950016e-01 4.9982226e-04]]\n",
            "[[9.9954480e-01 4.5518242e-04]]\n",
            "[[0.8838377  0.11616232]]\n",
            "[[0.99898237 0.0010176 ]]\n",
            "[[0.7452264  0.25477365]]\n",
            "[[9.9980897e-01 1.9108465e-04]]\n",
            "[[0.3347775  0.66522247]]\n",
            "[[0.781554   0.21844602]]\n",
            "[[0.8580335  0.14196657]]\n",
            "[[0.77069557 0.22930445]]\n",
            "[[0.996627   0.00337309]]\n",
            "[[0.92295635 0.07704362]]\n",
            "[[0.45886222 0.54113775]]\n",
            "[[0.9831202  0.01687973]]\n",
            "[[0.99839514 0.00160489]]\n",
            "[[0.8537573  0.14624266]]\n",
            "[[0.98898053 0.01101953]]\n",
            "[[0.9956657  0.00433434]]\n",
            "[[9.9973756e-01 2.6242089e-04]]\n",
            "[[0.9907126  0.00928739]]\n",
            "[[0.99758446 0.0024155 ]]\n",
            "[[0.9989256  0.00107435]]\n",
            "[[0.9548032  0.04519676]]\n",
            "[[0.93386304 0.06613699]]\n",
            "[[0.7699933  0.23000671]]\n",
            "[[9.9949336e-01 5.0657097e-04]]\n",
            "[[0.96498334 0.0350166 ]]\n",
            "[[0.9967529  0.00324708]]\n",
            "[[0.95510143 0.04489857]]\n",
            "[[0.62147135 0.37852862]]\n",
            "[[9.9950314e-01 4.9683137e-04]]\n",
            "[[0.6918242  0.30817577]]\n",
            "[[0.9190484  0.08095153]]\n",
            "[[0.99795216 0.00204782]]\n",
            "[[0.626186 0.373814]]\n",
            "[[0.3944659 0.6055341]]\n",
            "[[0.5551733  0.44482675]]\n",
            "[[0.98228884 0.01771119]]\n",
            "[[0.47303665 0.5269634 ]]\n",
            "[[0.34795693 0.65204304]]\n",
            "[[0.85149866 0.14850134]]\n",
            "[[0.61507684 0.38492316]]\n",
            "[[0.1189621  0.88103795]]\n",
            "[[0.9970463  0.00295366]]\n",
            "[[9.9982375e-01 1.7631626e-04]]\n",
            "[[0.9986777  0.00132234]]\n",
            "[[0.9979054 0.0020947]]\n",
            "[[9.9983490e-01 1.6509756e-04]]\n",
            "[[0.83320624 0.16679375]]\n",
            "[[0.22182547 0.7781745 ]]\n",
            "[[0.7349543  0.26504576]]\n",
            "[[0.816013   0.18398699]]\n",
            "[[9.9997449e-01 2.5481952e-05]]\n",
            "[[0.5352627  0.46473736]]\n",
            "[[9.994017e-01 5.982883e-04]]\n",
            "[[0.83456415 0.16543584]]\n",
            "[[0.36179426 0.6382057 ]]\n",
            "[[0.9870402 0.0129598]]\n",
            "[[0.99119085 0.00880912]]\n",
            "[[0.99207497 0.00792503]]\n",
            "[[0.4005075 0.5994925]]\n",
            "[[0.95451975 0.04548022]]\n",
            "[[0.59479654 0.40520352]]\n",
            "[[9.9956745e-01 4.3254200e-04]]\n",
            "[[0.67457074 0.32542926]]\n",
            "[[0.8483011  0.15169887]]\n",
            "[[0.13593227 0.86406773]]\n",
            "[[0.99284583 0.00715423]]\n",
            "[[0.95787746 0.04212256]]\n",
            "[[0.9918366 0.0081634]]\n",
            "[[0.99521446 0.00478558]]\n",
            "[[0.04845612 0.95154387]]\n",
            "[[0.512812   0.48718795]]\n",
            "[[0.10320862 0.8967914 ]]\n",
            "[[0.84214187 0.1578581 ]]\n",
            "[[0.97571987 0.02428011]]\n",
            "[[0.9939569  0.00604306]]\n",
            "[[0.8888892  0.11111086]]\n",
            "[[0.47494167 0.5250583 ]]\n",
            "[[0.9634749  0.03652506]]\n",
            "[[0.96805555 0.03194441]]\n",
            "[[0.9449771  0.05502293]]\n",
            "[[9.995783e-01 4.216607e-04]]\n",
            "[[0.29228172 0.7077183 ]]\n",
            "[[0.98840684 0.01159311]]\n",
            "[[0.99629873 0.00370131]]\n",
            "[[0.940809 0.059191]]\n",
            "[[0.9953459 0.0046541]]\n",
            "[[0.98937875 0.01062128]]\n",
            "[[0.4402546 0.5597454]]\n",
            "[[9.9997902e-01 2.0958098e-05]]\n",
            "[[0.8774274  0.12257258]]\n",
            "[[0.9907957 0.0092043]]\n",
            "[[0.8644388  0.13556123]]\n",
            "[[0.9861661  0.01383383]]\n",
            "[[0.9989335  0.00106649]]\n",
            "[[0.97531915 0.02468085]]\n",
            "[[0.00226234 0.99773765]]\n",
            "[[0.28815433 0.71184564]]\n",
            "[[0.9914731  0.00852694]]\n",
            "[[0.60391605 0.39608398]]\n",
            "[[0.9808272  0.01917279]]\n",
            "[[9.992361e-01 7.638926e-04]]\n",
            "[[0.64436305 0.35563695]]\n",
            "[[0.90984726 0.09015272]]\n",
            "[[0.70012873 0.29987124]]\n",
            "[[0.99355054 0.00644943]]\n",
            "[[0.99737287 0.00262714]]\n",
            "[[0.9797173  0.02028273]]\n",
            "[[0.96161246 0.03838759]]\n",
            "[[0.9406663  0.05933377]]\n",
            "[[0.9985039  0.00149614]]\n",
            "[[0.9979545 0.0020455]]\n",
            "[[9.9993443e-01 6.5556655e-05]]\n",
            "[[0.00297831 0.99702173]]\n",
            "[[0.4525013  0.54749864]]\n",
            "[[9.9998009e-01 1.9939305e-05]]\n",
            "[[9.9999487e-01 5.1632674e-06]]\n",
            "[[0.8838205  0.11617949]]\n",
            "[[0.91547155 0.08452843]]\n",
            "[[0.86449367 0.13550632]]\n",
            "[[0.88852453 0.11147549]]\n",
            "[[0.97180635 0.02819363]]\n",
            "[[0.41606602 0.58393395]]\n",
            "[[0.7490226  0.25097743]]\n",
            "[[0.43027264 0.5697274 ]]\n",
            "[[0.01472876 0.9852712 ]]\n",
            "[[0.9587371  0.04126299]]\n",
            "[[0.9451646  0.05483538]]\n",
            "[[0.05838396 0.94161606]]\n",
            "[[0.96518654 0.03481342]]\n",
            "[[0.15722354 0.84277654]]\n",
            "[[9.9986005e-01 1.3990382e-04]]\n",
            "[[0.7244829 0.2755171]]\n",
            "[[0.8245893 0.1754107]]\n",
            "[[0.76170135 0.23829861]]\n",
            "[[0.9612726  0.03872741]]\n",
            "[[0.986686   0.01331397]]\n",
            "[[0.9976035  0.00239661]]\n",
            "[[0.99200845 0.00799151]]\n",
            "[[0.98966146 0.01033858]]\n",
            "[[0.17471763 0.82528234]]\n",
            "[[0.79945934 0.20054063]]\n",
            "[[0.9988519  0.00114815]]\n",
            "[[0.10437984 0.89562017]]\n",
            "[[0.9799635  0.02003655]]\n",
            "[[0.97874546 0.02125454]]\n",
            "[[0.97397625 0.02602372]]\n",
            "[[9.9965143e-01 3.4862288e-04]]\n",
            "[[0.98978627 0.01021371]]\n",
            "[[0.93154943 0.06845061]]\n",
            "[[0.9639458  0.03605418]]\n",
            "[[0.9950046  0.00499538]]\n",
            "[[0.9788643  0.02113571]]\n",
            "[[9.9903810e-01 9.6187246e-04]]\n",
            "[[0.80469847 0.19530153]]\n",
            "[[0.96021783 0.03978222]]\n",
            "[[0.9636022  0.03639778]]\n",
            "[[0.34290028 0.6570997 ]]\n",
            "[[0.99575007 0.00424992]]\n",
            "[[0.12814043 0.87185955]]\n",
            "[[0.7200722  0.27992782]]\n",
            "[[0.9988708  0.00112916]]\n",
            "[[0.9762042  0.02379576]]\n",
            "[[0.9948867  0.00511336]]\n",
            "[[0.6550246  0.34497538]]\n",
            "[[0.8386     0.16139998]]\n",
            "[[0.98465955 0.01534049]]\n",
            "[[0.9906082  0.00939179]]\n",
            "[[0.5204062  0.47959387]]\n",
            "[[0.97956    0.02044002]]\n",
            "[[0.21034889 0.78965116]]\n",
            "[[0.7820372  0.21796276]]\n",
            "[[0.87944406 0.12055594]]\n",
            "[[9.9927622e-01 7.2377897e-04]]\n",
            "[[0.24591523 0.75408477]]\n",
            "[[0.77543694 0.22456306]]\n",
            "[[0.9963703 0.0036297]]\n",
            "[[0.93172425 0.06827573]]\n",
            "[[0.9787915  0.02120849]]\n",
            "[[0.96654475 0.03345523]]\n",
            "[[0.969271   0.03072905]]\n",
            "[[9.9969041e-01 3.0966938e-04]]\n",
            "[[0.6697525  0.33024758]]\n",
            "[[0.7515242  0.24847582]]\n",
            "[[0.5126546  0.48734537]]\n",
            "[[0.5707235  0.42927656]]\n",
            "[[0.35197005 0.64803   ]]\n",
            "[[0.89318806 0.10681198]]\n",
            "[[0.6816681 0.3183318]]\n",
            "[[0.97463685 0.02536318]]\n",
            "[[0.92899466 0.07100531]]\n",
            "[[0.99847955 0.00152044]]\n",
            "[[0.9414077  0.05859233]]\n",
            "[[0.9589657  0.04103431]]\n",
            "[[0.9524332  0.04756677]]\n",
            "[[9.9985576e-01 1.4426399e-04]]\n",
            "[[0.20968896 0.79031104]]\n",
            "[[0.28132358 0.7186764 ]]\n",
            "[[0.9969195  0.00308054]]\n",
            "[[0.9971072  0.00289279]]\n",
            "[[0.9922271  0.00777296]]\n",
            "[[0.0657737  0.93422633]]\n",
            "[[0.9573121  0.04268792]]\n",
            "[[0.18811211 0.8118879 ]]\n",
            "[[0.7445024 0.2554976]]\n",
            "[[0.01104361 0.9889564 ]]\n",
            "[[0.99420136 0.00579856]]\n",
            "[[0.36496097 0.63503903]]\n",
            "[[9.991986e-01 8.013448e-04]]\n",
            "[[9.9968088e-01 3.1911233e-04]]\n",
            "[[0.98739105 0.01260887]]\n",
            "[[0.92064095 0.07935908]]\n",
            "[[0.5006761  0.49932384]]\n",
            "[[0.9978248  0.00217524]]\n",
            "[[0.8687557  0.13124423]]\n",
            "[[0.97950417 0.02049577]]\n",
            "[[0.99524975 0.00475023]]\n",
            "[[0.85938126 0.14061877]]\n",
            "[[0.9921359  0.00786413]]\n",
            "[[0.9779252  0.02207486]]\n",
            "[[0.8203131  0.17968686]]\n",
            "[[0.78282255 0.2171775 ]]\n",
            "[[0.98164225 0.01835777]]\n",
            "[[0.99790585 0.00209412]]\n",
            "[[0.22919591 0.77080405]]\n",
            "[[0.9582965  0.04170357]]\n",
            "[[0.98199606 0.01800394]]\n",
            "[[0.54355055 0.45644948]]\n",
            "[[0.9779995  0.02200048]]\n",
            "[[0.9843003  0.01569971]]\n",
            "[[0.99177104 0.00822894]]\n",
            "[[0.9883109  0.01168915]]\n",
            "[[9.9957484e-01 4.2511206e-04]]\n",
            "[[0.83745867 0.16254134]]\n",
            "[[0.99608815 0.00391188]]\n",
            "[[9.9914455e-01 8.5539429e-04]]\n",
            "[[0.99764746 0.00235252]]\n",
            "[[0.62460876 0.37539127]]\n",
            "[[0.93685836 0.0631417 ]]\n",
            "[[0.59747875 0.4025212 ]]\n",
            "[[0.07265129 0.9273487 ]]\n",
            "[[0.90570784 0.09429222]]\n",
            "[[0.9940455  0.00595451]]\n",
            "[[0.96222574 0.03777425]]\n",
            "[[0.41052243 0.58947754]]\n",
            "[[0.9139793  0.08602076]]\n",
            "[[0.33320355 0.6667964 ]]\n",
            "[[0.982129   0.01787099]]\n",
            "[[0.9938653  0.00613462]]\n",
            "[[0.99100846 0.00899152]]\n",
            "[[0.556809   0.44319102]]\n",
            "[[0.9867951  0.01320493]]\n",
            "[[9.9975604e-01 2.4392935e-04]]\n",
            "[[0.998735   0.00126496]]\n",
            "[[0.80630517 0.19369482]]\n",
            "[[0.35703966 0.64296037]]\n",
            "[[0.58989686 0.41010308]]\n",
            "[[9.9953187e-01 4.6814620e-04]]\n",
            "[[0.41216645 0.5878335 ]]\n",
            "[[5.2685558e-04 9.9947315e-01]]\n",
            "[[0.98565525 0.01434474]]\n",
            "[[0.8602083  0.13979176]]\n",
            "[[0.82382476 0.17617527]]\n",
            "[[9.9994314e-01 5.6814482e-05]]\n",
            "[[0.91562617 0.08437388]]\n",
            "[[0.5726388  0.42736116]]\n",
            "[[0.99805534 0.00194469]]\n",
            "[[9.9983549e-01 1.6444585e-04]]\n",
            "[[0.19618063 0.80381936]]\n",
            "[[0.2496464 0.7503536]]\n",
            "[[0.8355655  0.16443446]]\n",
            "[[0.8248843  0.17511576]]\n",
            "[[0.11696687 0.8830331 ]]\n",
            "[[0.9956945  0.00430549]]\n",
            "[[0.9848252  0.01517482]]\n",
            "[[0.80079496 0.19920506]]\n",
            "[[0.99513274 0.00486725]]\n",
            "[[0.9807931 0.0192069]]\n",
            "[[9.9997950e-01 2.0523265e-05]]\n",
            "[[0.9891206  0.01087946]]\n",
            "[[0.9872376  0.01276238]]\n",
            "[[0.97102416 0.02897578]]\n",
            "[[9.9907196e-01 9.2795939e-04]]\n",
            "[[0.9291451  0.07085495]]\n",
            "[[0.788034 0.211966]]\n",
            "[[0.91466177 0.08533829]]\n",
            "[[0.8548732 0.1451268]]\n",
            "[[0.9617966  0.03820342]]\n",
            "[[0.9319895  0.06801049]]\n",
            "[[0.83966416 0.16033582]]\n",
            "[[0.9851841  0.01481592]]\n",
            "[[0.9745038  0.02549615]]\n",
            "[[0.95284647 0.04715348]]\n",
            "[[0.9800197  0.01998032]]\n",
            "[[0.99246114 0.00753886]]\n",
            "[[0.9912152 0.0087847]]\n",
            "[[0.9964929  0.00350713]]\n",
            "[[0.90212345 0.09787651]]\n",
            "[[0.9215289  0.07847111]]\n",
            "[[0.9968292  0.00317079]]\n",
            "[[0.9953017  0.00469831]]\n",
            "[[0.38362023 0.61637974]]\n",
            "[[0.9955369  0.00446312]]\n",
            "[[0.34160057 0.65839946]]\n",
            "[[0.9599569  0.04004309]]\n",
            "[[0.9657945  0.03420553]]\n",
            "[[0.967101 0.032899]]\n",
            "[[0.4598201  0.54017997]]\n",
            "[[0.9105019  0.08949811]]\n",
            "[[0.9241357  0.07586434]]\n",
            "[[0.40020245 0.59979755]]\n",
            "[[0.52324927 0.4767507 ]]\n",
            "[[0.9677398 0.0322601]]\n",
            "[[0.99780387 0.00219613]]\n",
            "[[9.9982458e-01 1.7550008e-04]]\n",
            "[[0.98055696 0.01944302]]\n",
            "[[0.7869053  0.21309468]]\n",
            "[[9.9994850e-01 5.1501313e-05]]\n",
            "[[0.39483246 0.6051675 ]]\n",
            "[[0.9858451  0.01415489]]\n",
            "[[9.9997520e-01 2.4814413e-05]]\n",
            "[[9.9909091e-01 9.0912875e-04]]\n",
            "[[0.37933925 0.6206608 ]]\n",
            "[[0.9743128  0.02568727]]\n",
            "[[0.7356372  0.26436272]]\n",
            "[[0.4530648 0.5469352]]\n",
            "[[0.35322168 0.6467783 ]]\n",
            "[[0.9691869  0.03081308]]\n",
            "[[0.60719943 0.3928006 ]]\n",
            "[[0.99021685 0.00978318]]\n",
            "[[0.48038664 0.5196134 ]]\n",
            "[[9.999950e-01 5.024846e-06]]\n",
            "[[9.9975914e-01 2.4084510e-04]]\n",
            "[[0.8784003  0.12159968]]\n",
            "[[0.9544068  0.04559321]]\n",
            "[[0.9501374  0.04986265]]\n",
            "[[0.9786942  0.02130587]]\n",
            "[[9.9960476e-01 3.9528194e-04]]\n",
            "[[0.9456184  0.05438164]]\n",
            "[[0.9223961  0.07760385]]\n",
            "[[0.9982685  0.00173152]]\n",
            "[[0.69411266 0.30588737]]\n",
            "[[0.3467316 0.6532684]]\n",
            "[[0.8591591  0.14084092]]\n",
            "[[0.99873155 0.00126839]]\n",
            "[[0.8069756  0.19302447]]\n",
            "[[0.7658151  0.23418497]]\n",
            "[[0.62531084 0.37468916]]\n",
            "[[0.9495289  0.05047116]]\n",
            "[[0.9621902  0.03780983]]\n",
            "[[0.5690415 0.4309585]]\n",
            "[[0.8827798  0.11722014]]\n",
            "[[0.58297646 0.41702357]]\n",
            "[[0.48665082 0.5133492 ]]\n",
            "[[0.07704668 0.9229533 ]]\n",
            "[[0.95344096 0.046559  ]]\n",
            "[[0.7726703  0.22732975]]\n",
            "[[0.9604047  0.03959532]]\n",
            "[[0.9812589 0.0187411]]\n",
            "[[0.80894    0.19106005]]\n",
            "[[0.9732776  0.02672231]]\n",
            "[[9.995875e-01 4.124765e-04]]\n",
            "[[0.552882 0.447118]]\n",
            "[[0.00520215 0.9947978 ]]\n",
            "[[9.5296494e-04 9.9904698e-01]]\n",
            "[[2.3942956e-05 9.9997604e-01]]\n",
            "[[0.1312823 0.8687177]]\n",
            "[[5.3691625e-04 9.9946314e-01]]\n",
            "[[4.1161653e-05 9.9995887e-01]]\n",
            "[[0.8805523  0.11944767]]\n",
            "[[1.5662985e-05 9.9998438e-01]]\n",
            "[[0.00236417 0.99763584]]\n",
            "[[0.0140904  0.98590964]]\n",
            "[[2.751036e-04 9.997249e-01]]\n",
            "[[0.01905724 0.9809428 ]]\n",
            "[[0.05588089 0.94411916]]\n",
            "[[0.00603075 0.9939692 ]]\n",
            "[[0.26870602 0.731294  ]]\n",
            "[[7.2275376e-04 9.9927729e-01]]\n",
            "[[0.00296849 0.99703145]]\n",
            "[[0.00170122 0.9982988 ]]\n",
            "[[0.0129273 0.9870727]]\n",
            "[[2.6446013e-04 9.9973553e-01]]\n",
            "[[0.00199982 0.9980002 ]]\n",
            "[[0.00391091 0.9960891 ]]\n",
            "[[3.0629098e-04 9.9969375e-01]]\n",
            "[[0.00459676 0.9954033 ]]\n",
            "[[2.5375954e-05 9.9997461e-01]]\n",
            "[[1.2301647e-05 9.9998772e-01]]\n",
            "[[0.11119366 0.8888064 ]]\n",
            "[[0.14372699 0.85627306]]\n",
            "[[3.5879083e-04 9.9964118e-01]]\n",
            "[[9.7097000e-06 9.9999034e-01]]\n",
            "[[3.2692289e-05 9.9996734e-01]]\n",
            "[[0.02330697 0.976693  ]]\n",
            "[[1.1894711e-04 9.9988103e-01]]\n",
            "[[0.15708232 0.8429177 ]]\n",
            "[[0.00477234 0.9952277 ]]\n",
            "[[0.0012158  0.99878424]]\n",
            "[[1.3270428e-04 9.9986732e-01]]\n",
            "[[2.6696638e-04 9.9973303e-01]]\n",
            "[[0.00935297 0.990647  ]]\n",
            "[[0.00360811 0.99639183]]\n",
            "[[0.00738012 0.99261993]]\n",
            "[[7.289814e-04 9.992710e-01]]\n",
            "[[7.6051586e-04 9.9923944e-01]]\n",
            "[[0.13318057 0.86681944]]\n",
            "[[3.1780134e-04 9.9968219e-01]]\n",
            "[[4.1818676e-05 9.9995816e-01]]\n",
            "[[4.2150295e-04 9.9957854e-01]]\n",
            "[[0.10019315 0.89980686]]\n",
            "[[0.5235713 0.4764287]]\n",
            "[[0.76025957 0.2397405 ]]\n",
            "[[2.740774e-05 9.999726e-01]]\n",
            "[[0.04446582 0.9555342 ]]\n",
            "[[0.01459754 0.98540246]]\n",
            "[[0.40108246 0.59891754]]\n",
            "[[0.00873185 0.99126816]]\n",
            "[[0.0011706 0.9988294]]\n",
            "[[1.8740122e-04 9.9981266e-01]]\n",
            "[[5.208487e-07 9.999995e-01]]\n",
            "[[0.02676923 0.9732308 ]]\n",
            "[[0.03614284 0.9638572 ]]\n",
            "[[0.01141844 0.9885816 ]]\n",
            "[[2.5052883e-04 9.9974948e-01]]\n",
            "[[1.0381693e-05 9.9998963e-01]]\n",
            "[[0.00100514 0.9989949 ]]\n",
            "[[0.00174908 0.9982509 ]]\n",
            "[[0.00830838 0.9916916 ]]\n",
            "[[4.852051e-04 9.995148e-01]]\n",
            "[[8.575198e-06 9.999914e-01]]\n",
            "[[0.00415347 0.99584657]]\n",
            "[[0.03400483 0.96599513]]\n",
            "[[9.0289052e-07 9.9999905e-01]]\n",
            "[[0.20246948 0.79753053]]\n",
            "[[0.01974507 0.98025495]]\n",
            "[[0.00911152 0.9908885 ]]\n",
            "[[6.1513974e-06 9.9999380e-01]]\n",
            "[[4.7688943e-04 9.9952316e-01]]\n",
            "[[0.08319646 0.9168036 ]]\n",
            "[[0.00176212 0.99823785]]\n",
            "[[0.5558544  0.44414562]]\n",
            "[[7.4099511e-04 9.9925894e-01]]\n",
            "[[0.02751182 0.97248816]]\n",
            "[[0.00266914 0.9973309 ]]\n",
            "[[1.8756979e-04 9.9981242e-01]]\n",
            "[[0.00149516 0.9985049 ]]\n",
            "[[4.6456582e-05 9.9995351e-01]]\n",
            "[[0.00345944 0.9965405 ]]\n",
            "[[0.04149367 0.95850635]]\n",
            "[[0.00498103 0.99501896]]\n",
            "[[0.02795307 0.972047  ]]\n",
            "[[0.2967222  0.70327777]]\n",
            "[[3.7884084e-04 9.9962115e-01]]\n",
            "[[1.7771378e-04 9.9982232e-01]]\n",
            "[[0.06393503 0.93606496]]\n",
            "[[0.01843013 0.9815699 ]]\n",
            "[[0.00175316 0.9982469 ]]\n",
            "[[0.10988963 0.8901103 ]]\n",
            "[[2.3444991e-04 9.9976557e-01]]\n",
            "[[0.01314553 0.98685443]]\n",
            "[[0.2721553 0.7278447]]\n",
            "[[3.3286153e-04 9.9966717e-01]]\n",
            "[[0.45769048 0.5423095 ]]\n",
            "[[0.02307143 0.97692853]]\n",
            "[[0.0013613 0.9986387]]\n",
            "[[1.2593674e-04 9.9987400e-01]]\n",
            "[[0.01139198 0.988608  ]]\n",
            "[[0.43053123 0.5694688 ]]\n",
            "[[0.02336381 0.97663623]]\n",
            "[[7.933412e-05 9.999206e-01]]\n",
            "[[2.416469e-04 9.997583e-01]]\n",
            "[[5.2614985e-05 9.9994743e-01]]\n",
            "[[0.08002745 0.9199726 ]]\n",
            "[[8.9787151e-05 9.9991024e-01]]\n",
            "[[0.00228585 0.99771416]]\n",
            "[[0.00673791 0.9932621 ]]\n",
            "[[3.8976882e-06 9.9999607e-01]]\n",
            "[[0.18503515 0.8149649 ]]\n",
            "[[2.8623560e-06 9.9999714e-01]]\n",
            "[[0.00764883 0.9923511 ]]\n",
            "[[3.5876993e-04 9.9964118e-01]]\n",
            "[[0.01821401 0.98178595]]\n",
            "[[4.9695907e-05 9.9995029e-01]]\n",
            "[[0.02716531 0.9728347 ]]\n",
            "[[0.02807559 0.9719244 ]]\n",
            "[[0.77009475 0.22990523]]\n",
            "[[0.01947075 0.98052925]]\n",
            "[[0.00923771 0.9907623 ]]\n",
            "[[0.542305   0.45769507]]\n",
            "[[0.11465319 0.88534683]]\n",
            "[[2.3321700e-04 9.9976677e-01]]\n",
            "[[6.9892565e-05 9.9993014e-01]]\n",
            "[[5.6622797e-05 9.9994338e-01]]\n",
            "[[0.04022342 0.95977664]]\n",
            "[[0.00476851 0.99523145]]\n",
            "[[1.8012363e-06 9.9999821e-01]]\n",
            "[[0.00122681 0.99877316]]\n",
            "[[1.7425082e-05 9.9998260e-01]]\n",
            "[[9.9081977e-04 9.9900913e-01]]\n",
            "[[0.00142391 0.99857605]]\n",
            "[[0.07119062 0.9288094 ]]\n",
            "[[4.231408e-06 9.999958e-01]]\n",
            "[[5.3673448e-06 9.9999464e-01]]\n",
            "[[0.00155961 0.99844044]]\n",
            "[[0.03501818 0.9649818 ]]\n",
            "[[0.2677511 0.7322489]]\n",
            "[[0.4886195 0.5113805]]\n",
            "[[3.234707e-04 9.996766e-01]]\n",
            "[[0.03010712 0.96989286]]\n",
            "[[0.05575119 0.9442488 ]]\n",
            "[[0.00164933 0.99835074]]\n",
            "[[0.02981011 0.97018987]]\n",
            "[[0.17516091 0.8248391 ]]\n",
            "[[0.07346888 0.92653114]]\n",
            "[[0.01531539 0.9846846 ]]\n",
            "[[4.3769713e-05 9.9995625e-01]]\n",
            "[[1.1631707e-04 9.9988365e-01]]\n",
            "[[2.8362093e-04 9.9971634e-01]]\n",
            "[[5.495942e-04 9.994504e-01]]\n",
            "[[1.3688317e-04 9.9986315e-01]]\n",
            "[[0.00110502 0.998895  ]]\n",
            "[[0.7350263  0.26497376]]\n",
            "[[0.03020493 0.96979505]]\n",
            "[[3.4539428e-06 9.9999654e-01]]\n",
            "[[0.6542672  0.34573278]]\n",
            "[[4.155384e-05 9.999584e-01]]\n",
            "[[2.5456844e-04 9.9974543e-01]]\n",
            "[[0.00243159 0.9975684 ]]\n",
            "[[0.9479088 0.0520912]]\n",
            "[[0.00284586 0.9971541 ]]\n",
            "[[0.00831413 0.9916858 ]]\n",
            "[[5.1944508e-06 9.9999475e-01]]\n",
            "[[0.09169543 0.9083046 ]]\n",
            "[[0.00574063 0.99425936]]\n",
            "[[8.8508605e-05 9.9991143e-01]]\n",
            "[[0.0099124  0.99008757]]\n",
            "[[0.06771681 0.9322832 ]]\n",
            "[[5.907178e-05 9.999409e-01]]\n",
            "[[4.950926e-06 9.999950e-01]]\n",
            "[[4.1213771e-05 9.9995875e-01]]\n",
            "[[5.0156108e-05 9.9994981e-01]]\n",
            "[[1.7711663e-04 9.9982291e-01]]\n",
            "[[0.00736536 0.9926346 ]]\n",
            "[[0.00819045 0.99180955]]\n",
            "[[0.0027381 0.9972619]]\n",
            "[[0.9715816  0.02841848]]\n",
            "[[0.8079998  0.19200018]]\n",
            "[[0.02276113 0.9772389 ]]\n",
            "[[2.0441003e-04 9.9979562e-01]]\n",
            "[[7.084654e-07 9.999993e-01]]\n",
            "[[4.393078e-06 9.999956e-01]]\n",
            "[[0.14255784 0.8574422 ]]\n",
            "[[0.00281146 0.9971885 ]]\n",
            "[[0.00484876 0.99515116]]\n",
            "[[3.0975022e-07 9.9999964e-01]]\n",
            "[[3.1760748e-04 9.9968243e-01]]\n",
            "[[0.02536877 0.9746312 ]]\n",
            "[[5.5072136e-04 9.9944931e-01]]\n",
            "[[0.01304706 0.9869529 ]]\n",
            "[[0.0719921  0.92800796]]\n",
            "[[3.7714862e-04 9.9962282e-01]]\n",
            "[[0.00164834 0.9983517 ]]\n",
            "[[0.00770185 0.9922982 ]]\n",
            "[[6.258686e-06 9.999937e-01]]\n",
            "[[0.00150957 0.99849045]]\n",
            "[[0.01673343 0.9832666 ]]\n",
            "[[0.0013716 0.9986284]]\n",
            "[[7.7823817e-05 9.9992216e-01]]\n",
            "[[3.6791142e-04 9.9963212e-01]]\n",
            "[[2.9347045e-04 9.9970645e-01]]\n",
            "[[0.00263752 0.99736243]]\n",
            "[[0.00162799 0.998372  ]]\n",
            "[[0.0328205 0.9671795]]\n",
            "[[6.2117164e-05 9.9993789e-01]]\n",
            "[[4.287419e-05 9.999571e-01]]\n",
            "[[0.0326159  0.96738404]]\n",
            "[[7.8256580e-06 9.9999213e-01]]\n",
            "[[4.463823e-05 9.999554e-01]]\n",
            "[[0.0158933  0.98410666]]\n",
            "[[4.7428138e-04 9.9952579e-01]]\n",
            "[[3.6595526e-04 9.9963403e-01]]\n",
            "[[9.412281e-04 9.990588e-01]]\n",
            "[[1.3645622e-04 9.9986351e-01]]\n",
            "[[0.07933468 0.9206653 ]]\n",
            "[[3.6369127e-04 9.9963629e-01]]\n",
            "[[0.4752379  0.52476203]]\n",
            "[[5.114860e-04 9.994885e-01]]\n",
            "[[0.00675562 0.99324435]]\n",
            "[[2.5937898e-04 9.9974066e-01]]\n",
            "[[8.8673882e-04 9.9911326e-01]]\n",
            "[[0.00697438 0.99302566]]\n",
            "[[0.748894   0.25110602]]\n",
            "[[0.02622747 0.9737725 ]]\n",
            "[[0.01318777 0.9868123 ]]\n",
            "[[8.090426e-06 9.999919e-01]]\n",
            "[[0.03112004 0.96887994]]\n",
            "[[0.06731862 0.9326814 ]]\n",
            "[[5.8424828e-04 9.9941576e-01]]\n",
            "[[0.2753317 0.7246683]]\n",
            "[[0.3958252 0.6041748]]\n",
            "[[0.01443566 0.9855643 ]]\n",
            "[[3.7605414e-04 9.9962389e-01]]\n",
            "[[0.00958278 0.99041724]]\n",
            "[[3.0804074e-07 9.9999964e-01]]\n",
            "[[9.6246926e-04 9.9903750e-01]]\n",
            "[[0.00159141 0.9984086 ]]\n",
            "[[0.00135449 0.9986455 ]]\n",
            "[[3.9111461e-07 9.9999964e-01]]\n",
            "[[0.07678805 0.9232119 ]]\n",
            "[[4.668986e-04 9.995332e-01]]\n",
            "[[0.010754   0.98924595]]\n",
            "[[0.01063119 0.98936886]]\n",
            "[[0.02442839 0.97557163]]\n",
            "[[0.00650255 0.9934975 ]]\n",
            "[[4.347655e-05 9.999565e-01]]\n",
            "[[1.5151016e-04 9.9984848e-01]]\n",
            "[[4.6833688e-06 9.9999535e-01]]\n",
            "[[0.00271875 0.9972812 ]]\n",
            "[[0.00979191 0.9902081 ]]\n",
            "[[0.00829839 0.99170166]]\n",
            "[[0.80600744 0.19399253]]\n",
            "[[0.0012261 0.9987739]]\n",
            "[[0.00525242 0.9947476 ]]\n",
            "[[0.23854584 0.7614541 ]]\n",
            "[[0.07826328 0.9217367 ]]\n",
            "[[0.22628473 0.7737153 ]]\n",
            "[[0.30006865 0.6999314 ]]\n",
            "[[0.00332122 0.9966788 ]]\n",
            "[[6.8101013e-04 9.9931896e-01]]\n",
            "[[0.31022084 0.68977916]]\n",
            "[[1.1099525e-04 9.9988902e-01]]\n",
            "[[4.273415e-04 9.995727e-01]]\n",
            "[[0.3843421 0.6156579]]\n",
            "[[0.03335348 0.96664655]]\n",
            "[[0.17281376 0.8271862 ]]\n",
            "[[0.86497116 0.13502878]]\n",
            "[[0.19133718 0.80866283]]\n",
            "[[0.6531708 0.3468292]]\n",
            "[[0.23321733 0.7667827 ]]\n",
            "[[1.3925698e-04 9.9986076e-01]]\n",
            "[[0.00243666 0.99756336]]\n",
            "[[8.711816e-06 9.999913e-01]]\n",
            "[[0.39525115 0.60474885]]\n",
            "[[0.00106752 0.9989324 ]]\n",
            "[[7.1297714e-04 9.9928707e-01]]\n",
            "[[0.0367944  0.96320564]]\n",
            "[[0.27271688 0.72728306]]\n",
            "[[0.0069519  0.99304813]]\n",
            "[[7.143448e-05 9.999286e-01]]\n",
            "[[0.0028635 0.9971365]]\n",
            "[[0.00247871 0.9975212 ]]\n",
            "[[2.1479887e-05 9.9997854e-01]]\n",
            "[[2.8661787e-04 9.9971336e-01]]\n",
            "[[0.05575554 0.94424444]]\n",
            "[[7.0785044e-04 9.9929214e-01]]\n",
            "[[2.6431555e-06 9.9999738e-01]]\n",
            "[[0.00472409 0.99527586]]\n",
            "[[6.527911e-05 9.999347e-01]]\n",
            "[[0.02014586 0.97985417]]\n",
            "[[0.02460224 0.97539777]]\n",
            "[[9.3429827e-04 9.9906570e-01]]\n",
            "[[0.40669936 0.59330064]]\n",
            "[[0.7091366 0.2908634]]\n",
            "[[2.9418995e-06 9.9999702e-01]]\n",
            "[[1.4756495e-04 9.9985242e-01]]\n",
            "[[0.08320538 0.9167946 ]]\n",
            "[[0.2602986 0.7397014]]\n",
            "[[6.262752e-06 9.999937e-01]]\n",
            "[[0.01274417 0.9872558 ]]\n",
            "[[3.2665933e-04 9.9967337e-01]]\n",
            "[[1.2395102e-05 9.9998760e-01]]\n",
            "[[1.3893402e-04 9.9986100e-01]]\n",
            "[[0.86992705 0.13007301]]\n",
            "[[0.02091216 0.9790878 ]]\n",
            "[[0.00185295 0.99814713]]\n",
            "[[3.282105e-05 9.999672e-01]]\n",
            "[[0.0027333  0.99726677]]\n",
            "[[9.1827055e-04 9.9908173e-01]]\n",
            "[[1.3672818e-04 9.9986327e-01]]\n",
            "[[7.7483736e-07 9.9999928e-01]]\n",
            "[[1.3452605e-04 9.9986541e-01]]\n",
            "[[0.00102951 0.9989705 ]]\n",
            "[[0.01069511 0.9893049 ]]\n",
            "[[0.05414796 0.9458521 ]]\n",
            "[[1.0167971e-05 9.9998987e-01]]\n",
            "[[1.7931226e-06 9.9999821e-01]]\n",
            "[[1.6772006e-04 9.9983227e-01]]\n",
            "[[0.05467559 0.94532436]]\n",
            "[[0.8741326  0.12586747]]\n",
            "[[2.6377758e-05 9.9997365e-01]]\n",
            "[[1.2792713e-06 9.9999869e-01]]\n",
            "[[2.5417624e-04 9.9974579e-01]]\n",
            "[[1.0573717e-05 9.9998939e-01]]\n",
            "[[1.3051638e-05 9.9998701e-01]]\n",
            "[[0.00582535 0.99417466]]\n",
            "[[8.642890e-06 9.999913e-01]]\n",
            "[[0.0548025 0.9451975]]\n",
            "[[0.07563915 0.9243609 ]]\n",
            "[[0.00211461 0.9978854 ]]\n",
            "[[0.00169654 0.9983034 ]]\n",
            "[[0.00419229 0.99580765]]\n",
            "[[0.01193808 0.9880619 ]]\n",
            "[[0.37032622 0.6296738 ]]\n",
            "[[0.4757591 0.524241 ]]\n",
            "[[0.00141281 0.99858725]]\n",
            "[[0.07174968 0.92825025]]\n",
            "[[0.33942866 0.66057134]]\n",
            "[[0.00360766 0.9963923 ]]\n",
            "[[5.394930e-04 9.994605e-01]]\n",
            "[[6.330896e-05 9.999367e-01]]\n",
            "[[9.5608644e-05 9.9990439e-01]]\n",
            "[[3.9277427e-04 9.9960726e-01]]\n",
            "[[0.00587066 0.9941294 ]]\n",
            "[[2.133832e-04 9.997867e-01]]\n",
            "[[0.07256495 0.92743504]]\n",
            "[[0.69292337 0.3070766 ]]\n",
            "[[8.2060316e-04 9.9917942e-01]]\n",
            "[[1.2242964e-05 9.9998772e-01]]\n",
            "[[1.50079495e-05 9.99984980e-01]]\n",
            "[[0.00776845 0.9922315 ]]\n",
            "[[0.00822362 0.99177635]]\n",
            "[[1.0113851e-05 9.9998987e-01]]\n",
            "[[5.1817806e-06 9.9999487e-01]]\n",
            "[[1.9149228e-04 9.9980849e-01]]\n",
            "[[0.20369026 0.79630977]]\n",
            "[[1.4091372e-04 9.9985909e-01]]\n",
            "[[0.01666359 0.98333645]]\n",
            "[[0.00326467 0.9967353 ]]\n",
            "[[5.840678e-05 9.999416e-01]]\n",
            "[[0.01146498 0.988535  ]]\n",
            "[[0.00191449 0.9980855 ]]\n",
            "[[6.0129994e-05 9.9993992e-01]]\n",
            "[[0.19942811 0.8005719 ]]\n",
            "[[8.1560034e-07 9.9999917e-01]]\n",
            "[[5.0230033e-04 9.9949765e-01]]\n",
            "[[0.1318491 0.8681509]]\n",
            "[[0.01137124 0.98862875]]\n",
            "[[0.11342361 0.88657635]]\n",
            "[[0.93717915 0.06282084]]\n",
            "[[0.31444475 0.6855553 ]]\n",
            "[[1.2734796e-04 9.9987268e-01]]\n",
            "[[0.00212207 0.99787796]]\n",
            "[[0.0054898 0.9945102]]\n",
            "[[0.13674723 0.86325276]]\n",
            "[[0.0014073 0.9985927]]\n",
            "[[0.00166295 0.99833703]]\n",
            "[[5.2614225e-04 9.9947387e-01]]\n",
            "[[0.02650354 0.97349644]]\n",
            "[[1.8371240e-04 9.9981636e-01]]\n",
            "[[3.1918168e-04 9.9968088e-01]]\n",
            "[[0.8996394  0.10036059]]\n",
            "[[0.78076506 0.21923493]]\n",
            "[[1.3749221e-04 9.9986243e-01]]\n",
            "[[0.02270879 0.97729117]]\n",
            "[[1.0962568e-06 9.9999893e-01]]\n",
            "[[4.679294e-04 9.995321e-01]]\n",
            "[[0.00105196 0.99894804]]\n",
            "[[0.05334671 0.94665325]]\n",
            "[[0.0013504  0.99864966]]\n",
            "[[0.25553313 0.74446684]]\n",
            "[[0.00440256 0.9955974 ]]\n",
            "[[0.01320199 0.986798  ]]\n",
            "[[0.02455602 0.975444  ]]\n",
            "[[5.9618746e-05 9.9994040e-01]]\n",
            "[[0.0022556  0.99774444]]\n",
            "[[0.00122207 0.9987779 ]]\n",
            "[[8.9768064e-04 9.9910235e-01]]\n",
            "[[0.00542934 0.9945707 ]]\n",
            "[[5.332509e-05 9.999467e-01]]\n",
            "[[1.3897791e-06 9.9999857e-01]]\n",
            "[[0.00414878 0.9958513 ]]\n",
            "[[0.00227427 0.9977258 ]]\n",
            "[[0.00689585 0.99310416]]\n",
            "[[0.0062889 0.9937111]]\n",
            "[[5.4549816e-04 9.9945456e-01]]\n",
            "[[1.0990560e-05 9.9998903e-01]]\n",
            "[[0.02130524 0.97869474]]\n",
            "[[3.4790068e-05 9.9996519e-01]]\n",
            "[[0.5221457 0.4778543]]\n",
            "[[0.01151466 0.98848534]]\n",
            "[[6.733124e-04 9.993267e-01]]\n",
            "[[3.222723e-05 9.999678e-01]]\n",
            "[[1.5432276e-04 9.9984562e-01]]\n",
            "[[0.11428457 0.8857154 ]]\n",
            "[[8.5337285e-04 9.9914658e-01]]\n",
            "[[0.21028808 0.7897119 ]]\n",
            "[[0.52430284 0.47569716]]\n",
            "[[0.01396029 0.98603976]]\n",
            "[[0.05008079 0.94991916]]\n",
            "[[0.01936149 0.98063844]]\n",
            "[[0.96021104 0.03978897]]\n",
            "[[0.11874729 0.88125277]]\n",
            "[[0.35647747 0.6435225 ]]\n",
            "[[4.5811874e-04 9.9954188e-01]]\n",
            "[[0.36616206 0.63383794]]\n",
            "[[0.45123678 0.54876316]]\n",
            "[[1.9739853e-04 9.9980265e-01]]\n",
            "[[0.44446483 0.5555352 ]]\n",
            "[[1.7590541e-05 9.9998236e-01]]\n",
            "[[0.08702178 0.9129782 ]]\n",
            "[[0.22724494 0.772755  ]]\n",
            "[[5.2337695e-05 9.9994767e-01]]\n",
            "[[0.0127223 0.9872777]]\n",
            "[[0.07815159 0.92184836]]\n",
            "[[0.25409415 0.7459059 ]]\n",
            "[[0.8748034 0.1251966]]\n",
            "[[0.04489397 0.95510596]]\n",
            "[[0.00566506 0.99433494]]\n",
            "[[0.01943898 0.98056096]]\n",
            "[[0.002874   0.99712604]]\n",
            "[[0.00608645 0.9939136 ]]\n",
            "[[8.1451255e-04 9.9918550e-01]]\n",
            "[[4.9192226e-04 9.9950802e-01]]\n",
            "[[0.20594816 0.7940518 ]]\n",
            "[[0.00326405 0.996736  ]]\n",
            "[[9.128455e-06 9.999908e-01]]\n",
            "[[0.08672263 0.9132774 ]]\n",
            "[[0.00156374 0.9984363 ]]\n",
            "[[0.00300204 0.99699795]]\n",
            "[[1.4955248e-04 9.9985039e-01]]\n",
            "[[8.9626337e-05 9.9991035e-01]]\n",
            "[[4.807649e-04 9.995192e-01]]\n",
            "[[0.01895284 0.9810472 ]]\n",
            "[[0.00159415 0.9984059 ]]\n",
            "[[0.06287806 0.937122  ]]\n",
            "[[0.06194194 0.9380581 ]]\n",
            "[[0.00112497 0.99887496]]\n",
            "[[9.4262225e-04 9.9905735e-01]]\n",
            "[[0.08182253 0.9181775 ]]\n",
            "[[0.00855964 0.9914404 ]]\n",
            "[[0.02521869 0.9747813 ]]\n",
            "[[1.835744e-04 9.998165e-01]]\n",
            "[[0.00715948 0.9928405 ]]\n",
            "[[0.01442979 0.9855702 ]]\n",
            "[[0.6624441  0.33755592]]\n",
            "[[0.00122701 0.9987729 ]]\n",
            "[[3.0745569e-04 9.9969256e-01]]\n",
            "[[0.05529184 0.94470817]]\n",
            "[[0.00470816 0.9952918 ]]\n",
            "[[2.3890268e-06 9.9999762e-01]]\n",
            "[[0.00110941 0.9988906 ]]\n",
            "[[0.02970967 0.9702903 ]]\n",
            "[[0.0124248  0.98757523]]\n",
            "[[2.3226856e-04 9.9976772e-01]]\n",
            "[[0.00633511 0.99366486]]\n",
            "[[0.06609988 0.93390006]]\n",
            "[[0.8097296  0.19027041]]\n",
            "[[0.7885058  0.21149422]]\n",
            "[[0.9648841  0.03511586]]\n",
            "[[0.04600072 0.9539993 ]]\n",
            "[[3.9075836e-05 9.9996090e-01]]\n",
            "[[0.00378716 0.99621284]]\n",
            "[[1.4681428e-06 9.9999857e-01]]\n",
            "[[0.62646025 0.37353975]]\n",
            "[[9.566220e-04 9.990434e-01]]\n",
            "[[0.0264066  0.97359335]]\n",
            "[[0.00192191 0.9980781 ]]\n",
            "[[1.8423114e-05 9.9998152e-01]]\n",
            "[[0.04294441 0.9570556 ]]\n",
            "[[0.00617515 0.99382484]]\n",
            "[[5.6790655e-06 9.9999428e-01]]\n",
            "[[8.1868326e-05 9.9991810e-01]]\n",
            "[[9.4260155e-05 9.9990571e-01]]\n",
            "[[1.217313e-04 9.998783e-01]]\n",
            "[[9.6211602e-07 9.9999905e-01]]\n",
            "[[0.00109972 0.9989003 ]]\n",
            "[[0.05908319 0.9409168 ]]\n",
            "[[0.01064978 0.98935026]]\n",
            "[[1.5113627e-06 9.9999845e-01]]\n",
            "[[0.05104585 0.9489542 ]]\n",
            "[[3.855952e-05 9.999615e-01]]\n",
            "[[0.00344538 0.9965546 ]]\n",
            "[[1.7450108e-04 9.9982554e-01]]\n",
            "[[0.00515205 0.994848  ]]\n",
            "[[0.47636548 0.52363455]]\n",
            "[[0.28589416 0.7141058 ]]\n",
            "[[7.335212e-06 9.999926e-01]]\n",
            "[[0.00563602 0.994364  ]]\n",
            "[[0.46619782 0.5338022 ]]\n",
            "[[0.45927474 0.54072523]]\n",
            "[[0.18567291 0.81432706]]\n",
            "[[2.9744842e-04 9.9970251e-01]]\n",
            "[[4.582124e-05 9.999542e-01]]\n",
            "[[0.0011728  0.99882716]]\n",
            "[[0.01003942 0.98996055]]\n",
            "[[0.04171991 0.95828015]]\n",
            "[[1.610891e-04 9.998388e-01]]\n",
            "[[0.02063734 0.9793626 ]]\n",
            "[[6.063490e-04 9.993936e-01]]\n",
            "[[0.00693272 0.99306726]]\n",
            "[[0.7113729 0.2886271]]\n",
            "[[2.4444712e-04 9.9975556e-01]]\n",
            "[[1.8683134e-04 9.9981314e-01]]\n",
            "[[3.666600e-05 9.999633e-01]]\n",
            "[[0.68994266 0.31005734]]\n",
            "[[9.8441064e-04 9.9901557e-01]]\n",
            "[[6.6189957e-04 9.9933809e-01]]\n",
            "[[0.04622123 0.95377874]]\n",
            "[[0.0034612  0.99653876]]\n",
            "[[0.00265302 0.997347  ]]\n",
            "[[0.00148308 0.998517  ]]\n",
            "[[0.00369067 0.99630934]]\n",
            "[[0.04691254 0.95308745]]\n",
            "[[0.0090101 0.9909899]]\n",
            "[[4.2666503e-05 9.9995732e-01]]\n",
            "[[0.07440467 0.9255953 ]]\n",
            "[[0.00230379 0.9976962 ]]\n",
            "[[7.2187941e-05 9.9992776e-01]]\n",
            "[[0.06855966 0.9314404 ]]\n",
            "[[0.00265046 0.9973495 ]]\n",
            "[[0.08289114 0.9171089 ]]\n",
            "[[0.01695119 0.9830488 ]]\n",
            "[[0.00361891 0.9963811 ]]\n",
            "[[0.00388795 0.99611205]]\n",
            "[[5.5711247e-05 9.9994433e-01]]\n",
            "[[0.4392671  0.56073284]]\n",
            "[[0.00312007 0.99687994]]\n",
            "[[0.80395687 0.19604316]]\n",
            "[[0.9517185  0.04828142]]\n",
            "[[5.6348270e-04 9.9943644e-01]]\n",
            "[[5.0218013e-04 9.9949777e-01]]\n",
            "[[6.228393e-04 9.993772e-01]]\n",
            "[[9.8628912e-04 9.9901366e-01]]\n",
            "[[7.1122905e-04 9.9928880e-01]]\n",
            "[[0.00815974 0.99184024]]\n",
            "[[5.8681167e-07 9.9999940e-01]]\n",
            "[[0.00691388 0.9930861 ]]\n",
            "[[0.27937102 0.7206289 ]]\n",
            "[[0.0010384  0.99896157]]\n",
            "[[6.7971506e-08 9.9999988e-01]]\n",
            "[[4.3194843e-04 9.9956805e-01]]\n",
            "[[1.8945373e-05 9.9998105e-01]]\n",
            "[[9.112212e-05 9.999089e-01]]\n",
            "[[0.00266486 0.99733514]]\n",
            "[[0.01001842 0.9899816 ]]\n",
            "[[0.00840168 0.99159825]]\n",
            "[[0.13164295 0.86835706]]\n",
            "[[0.26111335 0.7388867 ]]\n",
            "[[0.0037089  0.99629116]]\n",
            "[[0.00138902 0.998611  ]]\n",
            "[[0.36820605 0.6317939 ]]\n",
            "[[0.00434912 0.99565095]]\n",
            "[[1.2391101e-04 9.9987602e-01]]\n",
            "[[0.8122219  0.18777813]]\n",
            "[[6.2307830e-05 9.9993765e-01]]\n",
            "[[0.03008718 0.9699128 ]]\n",
            "[[0.00114696 0.9988531 ]]\n",
            "[[0.3458991  0.65410084]]\n",
            "[[0.09999266 0.9000073 ]]\n",
            "[[4.2031973e-04 9.9957973e-01]]\n",
            "[[0.02831503 0.971685  ]]\n",
            "[[2.5633160e-05 9.9997437e-01]]\n",
            "[[9.3065755e-05 9.9990690e-01]]\n",
            "[[0.00655262 0.99344736]]\n",
            "[[0.07433003 0.92566997]]\n",
            "[[0.0262433 0.9737567]]\n",
            "[[0.01600412 0.98399585]]\n",
            "[[6.40495e-05 9.99936e-01]]\n",
            "[[0.00124485 0.9987551 ]]\n",
            "[[0.03431704 0.965683  ]]\n",
            "[[1.4364973e-04 9.9985635e-01]]\n",
            "[[4.902037e-07 9.999995e-01]]\n",
            "[[0.0568194 0.9431806]]\n",
            "[[0.3035032  0.69649684]]\n",
            "[[0.00640724 0.9935928 ]]\n",
            "[[0.02244394 0.97755605]]\n",
            "[[0.20292307 0.79707694]]\n",
            "[[3.3534503e-05 9.9996650e-01]]\n",
            "[[0.11665046 0.88334954]]\n",
            "[[0.04854469 0.95145535]]\n",
            "[[0.01566975 0.98433024]]\n",
            "[[0.00946933 0.99053067]]\n",
            "[[0.0479874  0.95201266]]\n",
            "[[0.31681386 0.6831862 ]]\n",
            "[[3.2997588e-04 9.9967003e-01]]\n",
            "[[0.00214363 0.9978563 ]]\n",
            "[[1.9838853e-05 9.9998021e-01]]\n",
            "[[0.01111061 0.98888934]]\n",
            "[[4.011530e-05 9.999598e-01]]\n",
            "[[0.01534846 0.98465157]]\n",
            "[[0.00630693 0.9936931 ]]\n",
            "[[0.79787016 0.20212984]]\n",
            "[[0.02060553 0.9793945 ]]\n",
            "[[0.01833985 0.9816601 ]]\n",
            "[[0.01241999 0.98758   ]]\n",
            "[[0.01358023 0.98641974]]\n",
            "[[4.6895817e-04 9.9953103e-01]]\n",
            "[[0.00110831 0.9988918 ]]\n",
            "[[4.3829056e-04 9.9956173e-01]]\n",
            "[[5.095131e-05 9.999491e-01]]\n",
            "[[3.5928097e-04 9.9964070e-01]]\n",
            "[[0.00402955 0.9959705 ]]\n",
            "[[0.14718105 0.85281897]]\n",
            "[[6.578530e-05 9.999342e-01]]\n",
            "[[1.0018928e-05 9.9998999e-01]]\n",
            "[[0.27489126 0.7251088 ]]\n",
            "[[0.8037016  0.19629839]]\n",
            "[[0.00224015 0.9977598 ]]\n",
            "[[1.2808669e-04 9.9987185e-01]]\n",
            "[[1.172686e-04 9.998827e-01]]\n",
            "[[0.03875609 0.96124387]]\n",
            "[[0.02926389 0.9707361 ]]\n",
            "[[0.10213024 0.89786977]]\n",
            "[[2.0986267e-04 9.9979013e-01]]\n",
            "[[0.01068357 0.98931646]]\n",
            "[[0.05069736 0.9493027 ]]\n",
            "[[0.00105211 0.9989479 ]]\n",
            "[[0.34404817 0.65595186]]\n",
            "[[0.01713372 0.9828662 ]]\n",
            "[[3.2518379e-05 9.9996746e-01]]\n",
            "[[7.208022e-05 9.999279e-01]]\n",
            "[[0.00218857 0.9978115 ]]\n",
            "[[1.2699871e-04 9.9987304e-01]]\n",
            "[[0.0602309 0.9397691]]\n",
            "[[0.02325974 0.9767403 ]]\n",
            "[[8.4580315e-06 9.9999154e-01]]\n",
            "[[4.3416503e-04 9.9956578e-01]]\n",
            "[[0.1836782  0.81632185]]\n",
            "[[0.00106826 0.9989317 ]]\n",
            "[[0.00247768 0.9975223 ]]\n",
            "[[0.00239609 0.99760395]]\n",
            "[[0.02388572 0.9761143 ]]\n",
            "[[0.00108236 0.9989177 ]]\n",
            "[[0.00677032 0.9932296 ]]\n",
            "[[2.665010e-05 9.999733e-01]]\n",
            "[[0.3235477  0.67645234]]\n",
            "[[0.34673885 0.65326107]]\n",
            "[[0.04181251 0.9581875 ]]\n",
            "[[0.09631656 0.9036835 ]]\n",
            "[[1.9018597e-04 9.9980980e-01]]\n",
            "[[5.4992644e-05 9.9994504e-01]]\n",
            "[[0.00771804 0.992282  ]]\n",
            "[[0.01198223 0.9880178 ]]\n",
            "[[0.07405596 0.9259441 ]]\n",
            "[[0.00447698 0.995523  ]]\n",
            "[[0.01969745 0.9803026 ]]\n",
            "[[0.0019491 0.9980509]]\n",
            "[[0.00992662 0.99007344]]\n",
            "[[0.04437397 0.9556261 ]]\n",
            "[[3.7071211e-04 9.9962926e-01]]\n",
            "[[0.00146408 0.9985359 ]]\n",
            "[[6.5039058e-04 9.9934965e-01]]\n",
            "[[0.00379968 0.99620026]]\n",
            "[[0.00122861 0.99877137]]\n",
            "[[0.02551241 0.97448754]]\n",
            "[[0.00142055 0.9985795 ]]\n",
            "[[4.6996825e-05 9.9995303e-01]]\n",
            "[[0.02971864 0.9702813 ]]\n",
            "[[6.270007e-04 9.993730e-01]]\n",
            "[[0.01543795 0.984562  ]]\n",
            "[[1.8011358e-05 9.9998200e-01]]\n",
            "[[0.01741387 0.98258615]]\n",
            "[[0.00126187 0.9987381 ]]\n",
            "[[0.01032897 0.98967105]]\n",
            "[[0.01690573 0.9830943 ]]\n",
            "[[0.01330415 0.9866959 ]]\n",
            "[[0.17366923 0.8263307 ]]\n",
            "[[0.03287651 0.9671235 ]]\n",
            "[[7.3340331e-04 9.9926656e-01]]\n",
            "[[0.00678389 0.9932161 ]]\n",
            "[[0.50344336 0.49655667]]\n",
            "[[2.7573723e-08 1.0000000e+00]]\n",
            "[[0.07643809 0.92356193]]\n",
            "[[9.6858206e-04 9.9903142e-01]]\n",
            "[[9.867995e-04 9.990132e-01]]\n",
            "[[4.3263194e-06 9.9999571e-01]]\n",
            "[[2.4420765e-04 9.9975580e-01]]\n",
            "[[3.3954513e-04 9.9966049e-01]]\n",
            "[[0.04134428 0.9586558 ]]\n",
            "[[1.6234507e-04 9.9983764e-01]]\n",
            "[[6.812646e-04 9.993187e-01]]\n",
            "[[3.7513073e-05 9.9996245e-01]]\n",
            "[[7.7556964e-05 9.9992239e-01]]\n",
            "[[0.09717394 0.902826  ]]\n",
            "[[0.02051034 0.9794897 ]]\n",
            "[[1.1022843e-04 9.9988973e-01]]\n",
            "[[6.9048587e-04 9.9930954e-01]]\n",
            "[[0.00375615 0.99624383]]\n",
            "[[0.34179723 0.6582027 ]]\n",
            "[[0.00154629 0.99845374]]\n",
            "[[0.00625041 0.9937496 ]]\n",
            "[[0.00105536 0.9989447 ]]\n",
            "[[6.0200346e-06 9.9999392e-01]]\n",
            "[[4.097527e-05 9.999590e-01]]\n",
            "[[0.02086673 0.9791333 ]]\n",
            "[[6.6241504e-05 9.9993372e-01]]\n",
            "[[9.657237e-04 9.990343e-01]]\n",
            "[[0.03730934 0.96269065]]\n",
            "[[3.1728314e-06 9.9999678e-01]]\n",
            "[[0.00597549 0.9940246 ]]\n",
            "[[0.36197248 0.6380275 ]]\n",
            "[[0.03572659 0.96427345]]\n",
            "[[2.3747687e-05 9.9997628e-01]]\n",
            "[[5.9273345e-05 9.9994075e-01]]\n",
            "[[0.24533312 0.75466686]]\n",
            "[[6.1563443e-04 9.9938440e-01]]\n",
            "[[8.3809144e-05 9.9991620e-01]]\n",
            "[[0.13729061 0.86270934]]\n",
            "[[0.05850404 0.9414959 ]]\n",
            "[[0.10790736 0.8920926 ]]\n",
            "[[0.00212019 0.99787986]]\n",
            "[[0.0227354  0.97726464]]\n",
            "[[0.4287255  0.57127446]]\n",
            "[[0.0859355 0.9140645]]\n",
            "[[0.45842078 0.5415792 ]]\n",
            "[[0.0175069  0.98249316]]\n",
            "[[0.03064479 0.9693552 ]]\n",
            "[[0.32584018 0.67415977]]\n",
            "[[6.1889738e-04 9.9938107e-01]]\n",
            "[[0.03320471 0.96679527]]\n",
            "[[3.6197185e-04 9.9963796e-01]]\n",
            "[[0.02887099 0.97112906]]\n",
            "[[5.9121372e-05 9.9994087e-01]]\n",
            "[[5.509266e-04 9.994491e-01]]\n",
            "[[9.2549104e-04 9.9907446e-01]]\n",
            "[[0.01649379 0.98350626]]\n",
            "[[7.714934e-05 9.999229e-01]]\n",
            "[[0.01819169 0.98180825]]\n",
            "[[0.8647189  0.13528109]]\n",
            "[[0.770589   0.22941099]]\n",
            "[[0.8463767  0.15362328]]\n",
            "[[8.275439e-06 9.999918e-01]]\n",
            "[[0.00248004 0.9975199 ]]\n",
            "[[0.00792626 0.9920738 ]]\n",
            "[[3.5529619e-04 9.9964476e-01]]\n",
            "[[0.01936112 0.9806389 ]]\n",
            "[[0.02138642 0.97861356]]\n",
            "[[1.9943032e-05 9.9998009e-01]]\n",
            "[[2.1375397e-06 9.9999785e-01]]\n",
            "[[0.12887184 0.87112814]]\n",
            "[[0.06215503 0.937845  ]]\n",
            "[[0.03089506 0.9691049 ]]\n",
            "[[0.3452969 0.654703 ]]\n",
            "[[4.6761274e-05 9.9995327e-01]]\n",
            "[[2.1621106e-04 9.9978381e-01]]\n",
            "[[0.04317032 0.95682967]]\n",
            "[[0.02398534 0.9760147 ]]\n",
            "[[0.0014898 0.9985102]]\n",
            "[[1.5025331e-04 9.9984968e-01]]\n",
            "[[0.23412108 0.765879  ]]\n",
            "[[0.00566233 0.9943376 ]]\n",
            "[[8.7124441e-04 9.9912876e-01]]\n",
            "[[0.02994641 0.97005355]]\n",
            "[[0.05297108 0.94702893]]\n",
            "[[0.00918449 0.9908155 ]]\n",
            "[[0.37286708 0.62713295]]\n",
            "[[8.9104839e-07 9.9999917e-01]]\n",
            "[[0.00321807 0.99678195]]\n",
            "[[4.7536741e-06 9.9999523e-01]]\n",
            "[[9.7345893e-04 9.9902654e-01]]\n",
            "[[4.0363735e-05 9.9995959e-01]]\n",
            "[[0.01046002 0.9895399 ]]\n",
            "[[4.267374e-04 9.995733e-01]]\n",
            "[[0.12443236 0.8755677 ]]\n",
            "[[0.05673672 0.94326323]]\n",
            "[[0.00324166 0.9967584 ]]\n",
            "[[0.8573422  0.14265783]]\n",
            "[[9.627907e-05 9.999037e-01]]\n",
            "[[0.01401536 0.98598456]]\n",
            "[[0.4113159  0.58868414]]\n",
            "[[5.966111e-05 9.999403e-01]]\n",
            "[[0.04965907 0.9503409 ]]\n",
            "[[0.09726953 0.9027304 ]]\n",
            "[[3.9694682e-04 9.9960309e-01]]\n",
            "[[0.00297879 0.99702114]]\n",
            "[[1.5479443e-05 9.9998450e-01]]\n",
            "[[0.07974713 0.92025286]]\n",
            "[[0.21082577 0.78917426]]\n",
            "[[0.15125659 0.8487434 ]]\n",
            "[[0.00170737 0.9982926 ]]\n",
            "[[1.3710823e-04 9.9986291e-01]]\n",
            "[[0.03690901 0.96309096]]\n",
            "[[0.00203125 0.99796873]]\n",
            "[[2.4025707e-07 9.9999976e-01]]\n",
            "[[7.2561507e-04 9.9927443e-01]]\n",
            "[[0.6936719  0.30632812]]\n",
            "[[4.4116922e-04 9.9955887e-01]]\n",
            "[[9.295161e-05 9.999070e-01]]\n",
            "[[0.00456739 0.9954326 ]]\n",
            "[[0.00534507 0.9946549 ]]\n",
            "[[0.01419835 0.98580164]]\n",
            "[[1.3609602e-04 9.9986386e-01]]\n",
            "[[7.6842205e-08 9.9999988e-01]]\n",
            "[[0.25502878 0.7449713 ]]\n",
            "[[2.5760337e-06 9.9999738e-01]]\n",
            "[[0.06662666 0.93337333]]\n",
            "[[0.00335622 0.9966438 ]]\n",
            "[[0.75323945 0.24676058]]\n",
            "[[0.00242485 0.9975751 ]]\n",
            "[[0.00908205 0.990918  ]]\n",
            "[[0.08831597 0.91168404]]\n",
            "[[0.05574182 0.94425815]]\n",
            "[[0.05477671 0.9452233 ]]\n",
            "[[0.0549023  0.94509774]]\n",
            "[[2.4772435e-04 9.9975222e-01]]\n",
            "[[9.0802911e-05 9.9990916e-01]]\n",
            "[[0.01690057 0.98309946]]\n",
            "[[0.00260335 0.9973967 ]]\n",
            "[[0.00362373 0.9963762 ]]\n",
            "[[0.10797144 0.89202857]]\n",
            "[[0.21274897 0.787251  ]]\n",
            "[[0.1699415 0.8300585]]\n",
            "[[3.6236182e-05 9.9996376e-01]]\n",
            "[[9.8839251e-04 9.9901164e-01]]\n",
            "[[0.00190385 0.99809617]]\n",
            "[[0.5919872  0.40801278]]\n",
            "[[0.0437525  0.95624745]]\n",
            "[[0.01031797 0.989682  ]]\n",
            "[[0.00294968 0.9970503 ]]\n",
            "[[0.0022074 0.9977926]]\n",
            "[[0.18273745 0.81726253]]\n",
            "[[0.1593534  0.84064656]]\n",
            "[[0.27987483 0.72012514]]\n",
            "[[0.00884993 0.99115014]]\n",
            "[[0.33218825 0.66781175]]\n",
            "[[0.01832848 0.9816715 ]]\n",
            "[[0.14422117 0.8557789 ]]\n",
            "[[8.199529e-05 9.999180e-01]]\n",
            "[[0.06699872 0.9330013 ]]\n",
            "[[3.1612756e-06 9.9999678e-01]]\n",
            "[[0.00500648 0.9949935 ]]\n",
            "[[0.2607893  0.73921067]]\n",
            "[[3.1220425e-05 9.9996877e-01]]\n",
            "[[1.1567697e-06 9.9999881e-01]]\n",
            "[[5.2457186e-04 9.9947542e-01]]\n",
            "[[1.2169373e-05 9.9998784e-01]]\n",
            "[[0.0075326  0.99246734]]\n",
            "[[0.07469279 0.9253073 ]]\n",
            "[[0.928427 0.071573]]\n",
            "[[1.8769982e-06 9.9999809e-01]]\n",
            "[[0.00872339 0.9912766 ]]\n",
            "[[0.00316336 0.99683666]]\n",
            "[[0.4819948  0.51800525]]\n",
            "[[0.04130985 0.95869017]]\n",
            "[[0.0095663 0.9904337]]\n",
            "[[4.3539287e-05 9.9995649e-01]]\n",
            "[[0.07763022 0.9223698 ]]\n",
            "[[1.2955134e-05 9.9998701e-01]]\n",
            "[[3.6419133e-05 9.9996352e-01]]\n",
            "[[1.4355456e-04 9.9985647e-01]]\n",
            "[[4.428873e-04 9.995571e-01]]\n",
            "[[3.6357535e-06 9.9999642e-01]]\n",
            "[[0.00135663 0.99864334]]\n",
            "[[0.00104872 0.9989512 ]]\n",
            "[[0.39281246 0.60718757]]\n",
            "[[0.00855844 0.99144155]]\n",
            "[[0.2787037 0.7212963]]\n",
            "[[0.24415565 0.7558443 ]]\n",
            "[[0.31633058 0.6836694 ]]\n",
            "[[2.4314754e-04 9.9975687e-01]]\n",
            "[[0.00427344 0.9957266 ]]\n",
            "[[0.31942034 0.6805796 ]]\n",
            "[[0.04492023 0.9550798 ]]\n",
            "[[0.00907482 0.9909252 ]]\n",
            "[[0.00667536 0.9933247 ]]\n",
            "[[0.864806   0.13519396]]\n",
            "[[3.7890073e-04 9.9962103e-01]]\n",
            "[[0.01469007 0.98530996]]\n",
            "[[0.00381864 0.99618137]]\n",
            "[[1.619013e-05 9.999838e-01]]\n",
            "[[0.00407561 0.99592435]]\n",
            "[[0.00387824 0.9961217 ]]\n",
            "[[4.838974e-05 9.999516e-01]]\n",
            "[[2.966785e-05 9.999703e-01]]\n",
            "[[4.909509e-05 9.999509e-01]]\n",
            "[[0.00519223 0.9948078 ]]\n",
            "[[0.05437938 0.9456206 ]]\n",
            "[[0.03206411 0.9679359 ]]\n",
            "[[0.21975324 0.7802468 ]]\n",
            "[[1.8231662e-05 9.9998176e-01]]\n",
            "[[2.0375542e-04 9.9979621e-01]]\n",
            "[[0.00102629 0.9989737 ]]\n",
            "[[0.11215158 0.88784844]]\n",
            "[[0.66206884 0.3379312 ]]\n",
            "[[0.00133979 0.9986602 ]]\n",
            "[[1.7898888e-04 9.9982101e-01]]\n",
            "[[0.8026374  0.19736265]]\n",
            "[[0.00732962 0.9926704 ]]\n",
            "[[8.374442e-05 9.999162e-01]]\n",
            "[[2.7025488e-04 9.9972969e-01]]\n",
            "[[0.86707366 0.1329264 ]]\n",
            "[[4.5618282e-05 9.9995434e-01]]\n",
            "[[0.96905017 0.03094984]]\n",
            "[[9.6569885e-04 9.9903429e-01]]\n",
            "[[7.609117e-04 9.992391e-01]]\n",
            "[[0.01228271 0.98771733]]\n",
            "[[0.0010778 0.9989222]]\n",
            "[[0.02076963 0.97923034]]\n",
            "[[6.3946281e-06 9.9999356e-01]]\n",
            "[[1.9036632e-04 9.9980968e-01]]\n",
            "[[0.00474318 0.99525684]]\n",
            "[[0.00485709 0.99514294]]\n",
            "[[4.5583895e-04 9.9954408e-01]]\n",
            "[[0.00281712 0.99718297]]\n",
            "[[0.1340288  0.86597127]]\n",
            "[[0.00170522 0.9982948 ]]\n",
            "[[3.9863580e-06 9.9999607e-01]]\n",
            "[[0.69409996 0.30590004]]\n",
            "[[2.1917007e-05 9.9997807e-01]]\n",
            "[[0.2617057 0.7382943]]\n",
            "[[0.00155555 0.9984445 ]]\n",
            "[[0.0040685  0.99593157]]\n",
            "[[0.49846962 0.5015304 ]]\n",
            "[[0.00708627 0.9929137 ]]\n",
            "[[0.75850946 0.24149056]]\n",
            "[[3.2582265e-04 9.9967420e-01]]\n",
            "[[0.9038451  0.09615496]]\n",
            "[[0.36623546 0.63376445]]\n",
            "[[0.1714967  0.82850325]]\n",
            "[[6.692081e-05 9.999331e-01]]\n",
            "[[0.37086764 0.62913233]]\n",
            "[[0.0018879 0.9981121]]\n",
            "[[0.40854287 0.5914571 ]]\n",
            "[[0.7256054  0.27439463]]\n",
            "[[0.0010547 0.9989453]]\n",
            "[[9.2440139e-04 9.9907553e-01]]\n",
            "[[0.08560546 0.91439456]]\n",
            "[[0.16959897 0.830401  ]]\n",
            "[[0.00332421 0.99667585]]\n",
            "[[0.9412327  0.05876741]]\n",
            "[[0.02536849 0.97463155]]\n",
            "[[3.0123916e-05 9.9996984e-01]]\n",
            "[[0.05475287 0.9452471 ]]\n",
            "[[0.02126921 0.97873086]]\n",
            "[[0.00195001 0.99805   ]]\n",
            "[[0.22103597 0.77896404]]\n",
            "[[0.00145063 0.9985494 ]]\n",
            "[[0.2269276 0.7730724]]\n",
            "[[0.99771833 0.00228171]]\n",
            "[[5.7595153e-04 9.9942410e-01]]\n",
            "[[0.25984716 0.74015284]]\n",
            "[[6.952482e-05 9.999305e-01]]\n",
            "[[0.43244013 0.56755984]]\n",
            "[[2.604001e-04 9.997396e-01]]\n",
            "[[0.03674586 0.9632542 ]]\n",
            "[[5.2759057e-04 9.9947244e-01]]\n",
            "[[6.580401e-07 9.999993e-01]]\n",
            "[[0.9694922  0.03050776]]\n",
            "[[6.6700806e-05 9.9993324e-01]]\n",
            "[[2.5566397e-04 9.9974436e-01]]\n",
            "[[0.45368773 0.5463123 ]]\n",
            "[[0.7016735  0.29832652]]\n",
            "[[0.00615808 0.9938419 ]]\n",
            "[[0.00204552 0.9979545 ]]\n",
            "[[0.00869054 0.9913094 ]]\n",
            "[[0.00586022 0.9941398 ]]\n",
            "[[5.7095150e-04 9.9942905e-01]]\n",
            "[[0.00572498 0.99427503]]\n",
            "[[0.00212379 0.99787617]]\n",
            "[[0.02556996 0.97443   ]]\n",
            "[[0.03748276 0.9625172 ]]\n",
            "[[0.01144312 0.98855686]]\n",
            "[[1.4408967e-04 9.9985588e-01]]\n",
            "[[0.00796186 0.99203813]]\n",
            "[[0.06888708 0.93111295]]\n",
            "[[0.0772287  0.92277133]]\n",
            "[[2.00309e-05 9.99980e-01]]\n",
            "[[0.0024084 0.9975916]]\n",
            "[[0.00423948 0.9957605 ]]\n",
            "[[2.2619334e-04 9.9977380e-01]]\n",
            "[[4.6759134e-05 9.9995327e-01]]\n",
            "[[0.25801784 0.7419821 ]]\n",
            "[[0.01827351 0.98172647]]\n",
            "[[6.824723e-04 9.993175e-01]]\n",
            "[[8.8900797e-06 9.9999106e-01]]\n",
            "[[0.02382398 0.9761761 ]]\n",
            "[[0.04257866 0.95742136]]\n",
            "[[0.00167518 0.9983248 ]]\n",
            "[[0.29151857 0.70848143]]\n",
            "[[1.1783189e-06 9.9999881e-01]]\n",
            "[[0.12513502 0.874865  ]]\n",
            "[[2.5865458e-05 9.9997413e-01]]\n",
            "[[0.01213466 0.9878654 ]]\n",
            "[[8.2738632e-05 9.9991727e-01]]\n",
            "[[1.735086e-05 9.999826e-01]]\n",
            "[[0.00396299 0.99603707]]\n",
            "[[0.940465   0.05953503]]\n",
            "[[0.00136744 0.9986325 ]]\n",
            "[[0.03892919 0.96107084]]\n",
            "[[0.34667823 0.6533218 ]]\n",
            "[[0.27696648 0.7230335 ]]\n",
            "[[0.03530032 0.9646997 ]]\n",
            "[[0.04711085 0.95288914]]\n",
            "[[0.5180546 0.4819454]]\n",
            "[[3.247701e-05 9.999676e-01]]\n",
            "[[8.4810785e-04 9.9915195e-01]]\n",
            "[[2.8424922e-05 9.9997163e-01]]\n",
            "[[0.00511204 0.994888  ]]\n",
            "[[0.00120392 0.9987961 ]]\n",
            "[[0.95766705 0.0423329 ]]\n",
            "[[0.10649507 0.893505  ]]\n",
            "[[0.79352945 0.20647056]]\n",
            "[[7.7574323e-06 9.9999225e-01]]\n",
            "[[0.02494052 0.97505945]]\n",
            "[[0.00417929 0.99582064]]\n",
            "[[0.21148619 0.7885138 ]]\n",
            "[[6.4280670e-05 9.9993575e-01]]\n",
            "[[1.5150080e-06 9.9999845e-01]]\n",
            "[[0.04026777 0.9597323 ]]\n",
            "[[0.00664087 0.99335915]]\n",
            "[[6.1235146e-04 9.9938762e-01]]\n",
            "[[3.5836591e-04 9.9964166e-01]]\n",
            "[[0.00210576 0.9978942 ]]\n",
            "[[0.9604113  0.03958872]]\n",
            "[[8.193826e-04 9.991806e-01]]\n",
            "[[9.202755e-04 9.990797e-01]]\n",
            "[[6.286983e-05 9.999372e-01]]\n",
            "[[2.584491e-04 9.997415e-01]]\n",
            "[[8.002403e-04 9.991998e-01]]\n",
            "[[0.02794892 0.972051  ]]\n",
            "[[0.03917547 0.9608245 ]]\n",
            "[[0.00105687 0.99894315]]\n",
            "[[3.3655604e-05 9.9996638e-01]]\n",
            "[[1.854812e-06 9.999981e-01]]\n",
            "[[0.00386669 0.9961333 ]]\n",
            "[[3.334704e-04 9.996666e-01]]\n",
            "[[5.9791264e-04 9.9940205e-01]]\n",
            "[[0.353388  0.6466119]]\n",
            "[[0.01459822 0.98540175]]\n",
            "[[0.01162331 0.9883767 ]]\n",
            "[[9.1990241e-04 9.9908006e-01]]\n",
            "[[0.03754752 0.96245253]]\n",
            "[[3.0319332e-04 9.9969685e-01]]\n",
            "[[0.03042503 0.96957505]]\n",
            "[[3.108359e-05 9.999689e-01]]\n",
            "[[0.01244147 0.9875586 ]]\n",
            "[[0.00272595 0.9972741 ]]\n",
            "[[0.8652525  0.13474753]]\n",
            "[[2.839118e-04 9.997161e-01]]\n",
            "[[0.03827539 0.9617246 ]]\n",
            "[[0.6388068  0.36119315]]\n",
            "[[0.00389235 0.99610764]]\n",
            "[[0.00113638 0.9988637 ]]\n",
            "[[7.209686e-04 9.992791e-01]]\n",
            "[[2.627669e-04 9.997372e-01]]\n",
            "[[0.3835065  0.61649346]]\n",
            "[[0.16441864 0.8355814 ]]\n",
            "[[0.05103718 0.9489628 ]]\n",
            "[[1.01898244e-04 9.99898076e-01]]\n",
            "[[7.834857e-05 9.999217e-01]]\n",
            "[[0.76449066 0.23550935]]\n",
            "[[0.09806558 0.90193444]]\n",
            "[[4.9573573e-04 9.9950421e-01]]\n",
            "[[0.37886706 0.62113297]]\n",
            "[[0.9373788 0.0626212]]\n",
            "[[5.786205e-05 9.999422e-01]]\n",
            "[[0.3656391  0.63436085]]\n",
            "[[9.3044026e-04 9.9906963e-01]]\n",
            "[[5.2669457e-06 9.9999475e-01]]\n",
            "[[1.9389443e-05 9.9998057e-01]]\n",
            "[[0.00242416 0.9975758 ]]\n",
            "[[0.15828286 0.84171706]]\n",
            "[[5.7724933e-04 9.9942279e-01]]\n",
            "[[0.01814393 0.98185605]]\n",
            "[[9.0473815e-04 9.9909532e-01]]\n",
            "[[0.0061965 0.9938035]]\n",
            "[[0.4136907 0.5863093]]\n",
            "[[0.0371462  0.96285385]]\n",
            "[[6.6240097e-04 9.9933761e-01]]\n",
            "[[3.2218083e-04 9.9967778e-01]]\n",
            "[[7.495881e-06 9.999925e-01]]\n",
            "[[0.12208986 0.87791014]]\n",
            "[[0.13575378 0.8642462 ]]\n",
            "[[7.8726566e-04 9.9921274e-01]]\n",
            "[[6.1521877e-04 9.9938476e-01]]\n",
            "[[1.5159296e-04 9.9984837e-01]]\n",
            "[[2.2047324e-04 9.9977952e-01]]\n",
            "[[0.05035139 0.9496486 ]]\n",
            "[[2.4940842e-04 9.9975055e-01]]\n",
            "[[3.3817152e-04 9.9966180e-01]]\n",
            "[[4.5170769e-04 9.9954826e-01]]\n",
            "[[0.00213699 0.997863  ]]\n",
            "[[3.494857e-04 9.996505e-01]]\n",
            "[[0.589526   0.41047403]]\n",
            "[[0.6455185  0.35448155]]\n",
            "[[1.0446984e-04 9.9989557e-01]]\n",
            "[[0.83381826 0.16618167]]\n",
            "[[8.9013753e-05 9.9991095e-01]]\n",
            "[[5.6953145e-06 9.9999428e-01]]\n",
            "[[8.5774518e-05 9.9991417e-01]]\n",
            "[[0.01102006 0.98897994]]\n",
            "[[0.01133315 0.9886669 ]]\n",
            "[[0.0041836 0.9958164]]\n",
            "[[9.4908144e-04 9.9905092e-01]]\n",
            "[[0.02487701 0.97512305]]\n",
            "[[0.14313376 0.85686624]]\n",
            "[[0.00539725 0.99460274]]\n",
            "[[0.00276314 0.99723685]]\n",
            "[[0.46726656 0.5327335 ]]\n",
            "[[0.01439726 0.98560274]]\n",
            "[[0.05365789 0.94634205]]\n",
            "[[0.01814882 0.9818512 ]]\n",
            "[[2.6272376e-06 9.9999738e-01]]\n",
            "[[8.124808e-04 9.991875e-01]]\n",
            "[[0.002909   0.99709094]]\n",
            "[[8.345572e-04 9.991654e-01]]\n",
            "[[0.03389721 0.96610284]]\n",
            "[[0.00121922 0.9987808 ]]\n",
            "[[1.8793516e-04 9.9981207e-01]]\n",
            "[[4.1834475e-05 9.9995816e-01]]\n",
            "[[0.0012097 0.9987903]]\n",
            "[[0.02337767 0.97662234]]\n",
            "[[0.3238594 0.6761406]]\n",
            "[[0.07968549 0.92031455]]\n",
            "[[0.11808478 0.8819152 ]]\n",
            "[[0.32136136 0.6786387 ]]\n",
            "[[0.03509099 0.96490896]]\n",
            "[[7.8540616e-04 9.9921453e-01]]\n",
            "[[0.00331396 0.99668604]]\n",
            "[[0.00556182 0.9944382 ]]\n",
            "[[4.264816e-05 9.999573e-01]]\n",
            "[[0.5167682  0.48323175]]\n",
            "[[0.00441886 0.9955811 ]]\n",
            "[[0.01864682 0.98135316]]\n",
            "[[2.802700e-04 9.997197e-01]]\n",
            "[[0.01082097 0.989179  ]]\n",
            "[[0.04166728 0.95833266]]\n",
            "[[0.0017518 0.9982482]]\n",
            "[[0.15480469 0.8451953 ]]\n",
            "[[1.9294089e-04 9.9980706e-01]]\n",
            "[[7.7940733e-04 9.9922061e-01]]\n",
            "[[0.6915431 0.3084569]]\n",
            "[[6.1768180e-05 9.9993825e-01]]\n",
            "[[0.623771 0.376229]]\n",
            "[[0.0768038  0.92319614]]\n",
            "[[7.554387e-04 9.992446e-01]]\n",
            "[[0.02770103 0.97229904]]\n",
            "[[0.00375162 0.9962484 ]]\n",
            "[[0.01541376 0.98458624]]\n",
            "[[0.00408141 0.9959186 ]]\n",
            "[[0.01500164 0.9849984 ]]\n",
            "[[0.04321579 0.95678425]]\n",
            "[[0.01864072 0.98135924]]\n",
            "[[0.6549096  0.34509042]]\n",
            "[[0.00479063 0.9952094 ]]\n",
            "[[0.00454787 0.9954521 ]]\n",
            "[[0.00958102 0.990419  ]]\n",
            "[[5.4362480e-04 9.9945635e-01]]\n",
            "[[0.2970346  0.70296544]]\n",
            "[[8.8958396e-04 9.9911040e-01]]\n",
            "[[0.6531191  0.34688094]]\n",
            "[[8.121546e-04 9.991879e-01]]\n",
            "[[0.01330365 0.9866963 ]]\n",
            "[[8.7540306e-05 9.9991250e-01]]\n",
            "[[0.31246966 0.6875303 ]]\n",
            "[[0.07107516 0.9289249 ]]\n",
            "[[2.5736567e-06 9.9999738e-01]]\n",
            "[[0.01236854 0.9876315 ]]\n",
            "[[0.00293674 0.9970632 ]]\n",
            "[[7.2857965e-06 9.9999273e-01]]\n",
            "[[7.9680460e-05 9.9992037e-01]]\n",
            "[[0.00172059 0.99827945]]\n",
            "[[0.00449687 0.9955031 ]]\n",
            "[[0.00766378 0.9923362 ]]\n",
            "[[0.12309615 0.8769039 ]]\n",
            "[[0.00101316 0.9989868 ]]\n",
            "[[0.02109064 0.9789094 ]]\n",
            "[[0.23216517 0.76783484]]\n",
            "[[3.1104524e-04 9.9968898e-01]]\n",
            "[[0.00731645 0.9926836 ]]\n",
            "[[0.00107322 0.9989267 ]]\n",
            "[[0.04576546 0.95423454]]\n",
            "[[0.05989988 0.9401001 ]]\n",
            "[[0.52500963 0.4749903 ]]\n",
            "[[0.03830599 0.96169406]]\n",
            "[[0.00354883 0.99645114]]\n",
            "[[0.07453056 0.92546946]]\n",
            "[[0.00221541 0.9977847 ]]\n",
            "[[0.00418774 0.99581224]]\n",
            "[[0.03250841 0.9674916 ]]\n",
            "[[0.19733809 0.8026619 ]]\n",
            "[[0.01888287 0.98111707]]\n",
            "[[0.02979095 0.97020906]]\n",
            "[[0.88525826 0.11474176]]\n",
            "[[1.6237634e-04 9.9983764e-01]]\n",
            "[[0.0082331  0.99176687]]\n",
            "[[0.01458818 0.9854118 ]]\n",
            "[[6.9709247e-05 9.9993026e-01]]\n",
            "[[0.26922908 0.7307709 ]]\n",
            "[[0.11890496 0.88109505]]\n",
            "[[0.05625314 0.94374686]]\n",
            "[[0.00514209 0.9948579 ]]\n",
            "[[2.2013833e-06 9.9999785e-01]]\n",
            "[[0.17711337 0.8228866 ]]\n",
            "[[0.3550141 0.6449859]]\n",
            "[[0.00751247 0.99248755]]\n",
            "[[3.3085002e-04 9.9966919e-01]]\n",
            "[[0.03100212 0.9689979 ]]\n",
            "[[6.1345322e-04 9.9938655e-01]]\n",
            "[[7.833317e-04 9.992167e-01]]\n",
            "[[0.00498455 0.99501544]]\n",
            "[[9.677022e-05 9.999032e-01]]\n",
            "[[0.00531024 0.9946898 ]]\n",
            "[[0.00145531 0.9985447 ]]\n",
            "[[2.2616636e-05 9.9997735e-01]]\n",
            "[[0.23464584 0.7653542 ]]\n",
            "[[0.11874631 0.88125366]]\n",
            "[[4.7657266e-04 9.9952340e-01]]\n",
            "[[6.2511557e-05 9.9993753e-01]]\n",
            "[[0.02852772 0.9714723 ]]\n",
            "[[4.3857694e-06 9.9999559e-01]]\n",
            "[[0.01479052 0.9852094 ]]\n",
            "[[0.00196569 0.9980343 ]]\n",
            "[[0.01200486 0.9879951 ]]\n",
            "[[0.05227582 0.94772416]]\n",
            "[[3.7035163e-04 9.9962962e-01]]\n",
            "[[0.54106706 0.45893294]]\n",
            "[[0.06891265 0.9310873 ]]\n",
            "[[0.00502147 0.9949785 ]]\n",
            "[[0.5013829  0.49861705]]\n",
            "[[0.00602951 0.9939705 ]]\n",
            "[[0.20352137 0.79647857]]\n",
            "[[0.01326277 0.9867372 ]]\n",
            "[[2.345023e-06 9.999976e-01]]\n",
            "[[0.00373156 0.9962684 ]]\n",
            "[[0.20631993 0.7936801 ]]\n",
            "[[2.1141516e-04 9.9978858e-01]]\n",
            "[[5.018898e-05 9.999498e-01]]\n",
            "[[0.837195   0.16280511]]\n",
            "[[0.02106307 0.97893685]]\n",
            "[[1.7608518e-05 9.9998236e-01]]\n",
            "[[0.00907246 0.9909276 ]]\n",
            "[[0.0461448 0.9538552]]\n",
            "[[0.01378533 0.9862147 ]]\n",
            "[[0.00239281 0.99760723]]\n",
            "[[0.12694392 0.87305605]]\n",
            "[[0.00454414 0.99545586]]\n",
            "[[0.00311123 0.9968888 ]]\n",
            "[[0.019353   0.98064697]]\n",
            "[[0.00824509 0.9917549 ]]\n",
            "[[0.0798971  0.92010295]]\n",
            "[[0.00154946 0.9984505 ]]\n",
            "[[0.00258005 0.99741995]]\n",
            "[[0.05803613 0.94196385]]\n",
            "[[0.24725543 0.75274456]]\n",
            "[[0.012216   0.98778397]]\n",
            "[[1.4850845e-04 9.9985147e-01]]\n",
            "[[3.3597415e-04 9.9966407e-01]]\n",
            "[[0.00912329 0.99087673]]\n",
            "[[0.00472429 0.99527574]]\n",
            "[[4.8086640e-05 9.9995196e-01]]\n",
            "[[0.00697136 0.9930286 ]]\n",
            "[[0.00765736 0.99234265]]\n",
            "[[0.05842441 0.9415756 ]]\n",
            "[[0.03213388 0.9678661 ]]\n",
            "[[0.00459985 0.99540013]]\n",
            "[[7.1617571e-04 9.9928385e-01]]\n",
            "[[0.5209     0.47910002]]\n",
            "[[4.490885e-04 9.995509e-01]]\n",
            "[[0.00327772 0.9967223 ]]\n",
            "[[0.13049325 0.8695067 ]]\n",
            "[[5.369861e-04 9.994630e-01]]\n",
            "[[6.635783e-05 9.999336e-01]]\n",
            "[[0.00391193 0.996088  ]]\n",
            "[[0.00477431 0.99522567]]\n",
            "[[0.02910708 0.9708929 ]]\n",
            "[[6.3660147e-05 9.9993634e-01]]\n",
            "[[0.00155525 0.99844474]]\n",
            "[[0.03772075 0.96227926]]\n",
            "[[0.00286802 0.99713194]]\n",
            "[[0.03866519 0.9613349 ]]\n",
            "[[6.0707764e-05 9.9993932e-01]]\n",
            "[[3.2926324e-05 9.9996710e-01]]\n",
            "[[0.54723585 0.4527642 ]]\n",
            "[[0.00312336 0.9968766 ]]\n",
            "[[0.00658219 0.99341786]]\n",
            "[[0.00134471 0.9986553 ]]\n",
            "[[2.9306283e-04 9.9970692e-01]]\n",
            "[[1.3830676e-06 9.9999857e-01]]\n",
            "[[6.7289970e-05 9.9993265e-01]]\n",
            "[[9.715961e-06 9.999902e-01]]\n",
            "[[0.00364885 0.9963511 ]]\n",
            "[[2.7414544e-06 9.9999726e-01]]\n",
            "[[0.8590787  0.14092125]]\n",
            "[[0.00195308 0.9980469 ]]\n",
            "[[3.5997877e-05 9.9996400e-01]]\n",
            "[[0.00119748 0.99880254]]\n",
            "[[1.9641602e-05 9.9998033e-01]]\n",
            "[[0.0011761  0.99882394]]\n",
            "[[1.6275424e-04 9.9983716e-01]]\n",
            "[[0.00221294 0.997787  ]]\n",
            "[[2.944608e-06 9.999970e-01]]\n",
            "[[0.05455428 0.9454457 ]]\n",
            "[[1.3703096e-05 9.9998629e-01]]\n",
            "[[0.01426291 0.9857371 ]]\n",
            "[[1.7731432e-04 9.9982268e-01]]\n",
            "[[0.0554707  0.94452924]]\n",
            "[[7.1697701e-05 9.9992836e-01]]\n",
            "[[0.01412635 0.9858737 ]]\n",
            "[[0.06042875 0.93957126]]\n",
            "[[0.9590833  0.04091667]]\n",
            "[[1.09814144e-04 9.99890208e-01]]\n",
            "[[0.00114457 0.9988555 ]]\n",
            "[[3.1963453e-05 9.9996805e-01]]\n",
            "[[0.00809353 0.9919065 ]]\n",
            "[[0.02587939 0.97412056]]\n",
            "[[0.0018392 0.9981608]]\n",
            "[[5.7235616e-04 9.9942762e-01]]\n",
            "[[5.4793720e-05 9.9994516e-01]]\n",
            "[[0.00373078 0.9962692 ]]\n",
            "[[2.6047113e-04 9.9973947e-01]]\n",
            "[[3.846063e-04 9.996153e-01]]\n",
            "[[8.6007130e-05 9.9991393e-01]]\n",
            "[[0.7925223  0.20747767]]\n",
            "[[0.43175274 0.56824726]]\n",
            "[[0.00405113 0.9959488 ]]\n",
            "[[8.419368e-04 9.991580e-01]]\n",
            "[[3.9718770e-06 9.9999607e-01]]\n",
            "[[1.6791302e-05 9.9998319e-01]]\n",
            "[[0.00105356 0.9989465 ]]\n",
            "[[1.5274221e-05 9.9998474e-01]]\n",
            "[[0.00848736 0.9915126 ]]\n",
            "[[0.10938606 0.8906139 ]]\n",
            "[[0.34136206 0.65863794]]\n",
            "[[0.85878766 0.14121233]]\n",
            "[[1.6575563e-05 9.9998343e-01]]\n",
            "[[0.0053569 0.9946431]]\n",
            "[[0.00568012 0.9943199 ]]\n",
            "[[0.05482418 0.94517577]]\n",
            "[[0.10899056 0.89100945]]\n",
            "[[5.462676e-04 9.994537e-01]]\n",
            "[[0.00291617 0.99708384]]\n",
            "[[9.4503157e-05 9.9990547e-01]]\n",
            "[[9.676395e-05 9.999032e-01]]\n",
            "[[4.1271959e-04 9.9958724e-01]]\n",
            "[[0.23876806 0.76123196]]\n",
            "[[0.01378461 0.98621535]]\n",
            "[[0.3064848  0.69351524]]\n",
            "[[0.12630153 0.8736985 ]]\n",
            "[[1.3065177e-04 9.9986935e-01]]\n",
            "[[0.00280326 0.9971967 ]]\n",
            "[[0.7081865  0.29181346]]\n",
            "[[9.8899087e-05 9.9990106e-01]]\n",
            "[[4.0148420e-06 9.9999595e-01]]\n",
            "[[0.18606095 0.81393903]]\n",
            "[[6.612999e-04 9.993387e-01]]\n",
            "[[0.3417644 0.6582356]]\n",
            "[[5.2195083e-04 9.9947804e-01]]\n",
            "[[1.9372879e-04 9.9980634e-01]]\n",
            "[[0.0024568 0.9975432]]\n",
            "[[0.02126539 0.9787346 ]]\n",
            "[[1.4990228e-04 9.9985003e-01]]\n",
            "[[7.2612270e-04 9.9927384e-01]]\n",
            "[[0.00697141 0.9930286 ]]\n",
            "[[0.01441271 0.98558724]]\n",
            "[[0.00465299 0.99534696]]\n",
            "[[0.13383424 0.8661658 ]]\n",
            "[[4.1464697e-05 9.9995852e-01]]\n",
            "[[0.0022941 0.9977059]]\n",
            "[[0.02974106 0.9702589 ]]\n",
            "[[1.1427846e-05 9.9998856e-01]]\n",
            "[[6.772962e-05 9.999323e-01]]\n",
            "[[3.3905071e-05 9.9996614e-01]]\n",
            "[[0.01998145 0.98001856]]\n",
            "[[5.826681e-05 9.999417e-01]]\n",
            "[[1.00928584e-04 9.99899030e-01]]\n",
            "[[0.03944671 0.9605533 ]]\n",
            "[[0.23371969 0.76628035]]\n",
            "[[0.00541229 0.99458766]]\n",
            "[[4.8383084e-04 9.9951613e-01]]\n",
            "[[0.02402013 0.9759798 ]]\n",
            "[[9.4895704e-06 9.9999046e-01]]\n",
            "[[0.4049229  0.59507704]]\n",
            "[[1.1719058e-05 9.9998832e-01]]\n",
            "[[0.19168504 0.808315  ]]\n",
            "[[5.0953447e-05 9.9994910e-01]]\n",
            "[[0.00129053 0.99870944]]\n",
            "[[0.00768472 0.9923153 ]]\n",
            "[[2.9365316e-05 9.9997067e-01]]\n",
            "[[0.00184415 0.9981559 ]]\n",
            "[[0.00523858 0.9947614 ]]\n",
            "[[0.0625421 0.9374579]]\n",
            "[[2.2155132e-04 9.9977845e-01]]\n",
            "[[0.99826837 0.00173165]]\n",
            "[[4.8637623e-05 9.9995136e-01]]\n",
            "[[0.14158508 0.8584149 ]]\n",
            "[[0.02905217 0.97094786]]\n",
            "[[0.35775083 0.6422491 ]]\n",
            "[[4.532455e-05 9.999547e-01]]\n",
            "[[0.00263769 0.9973623 ]]\n",
            "[[0.34944656 0.6505534 ]]\n",
            "[[0.04852911 0.9514709 ]]\n",
            "[[0.04296923 0.9570308 ]]\n",
            "[[4.4309717e-04 9.9955684e-01]]\n",
            "[[0.02899719 0.9710028 ]]\n",
            "[[1.3537948e-04 9.9986458e-01]]\n",
            "[[0.00150292 0.9984971 ]]\n",
            "[[0.08555572 0.91444427]]\n",
            "[[6.4173584e-08 9.9999988e-01]]\n",
            "[[0.7594534  0.24054654]]\n",
            "[[6.1147533e-05 9.9993885e-01]]\n",
            "[[0.0098081  0.99019194]]\n",
            "[[5.6320099e-05 9.9994373e-01]]\n",
            "[[0.7091387  0.29086128]]\n",
            "[[0.00429614 0.9957039 ]]\n",
            "[[0.01105053 0.98894954]]\n",
            "[[1.3989834e-04 9.9986005e-01]]\n",
            "[[0.9502807  0.04971921]]\n",
            "[[1.3530465e-05 9.9998641e-01]]\n",
            "[[0.08763396 0.91236603]]\n",
            "[[0.00113959 0.99886036]]\n",
            "[[1.5820736e-04 9.9984181e-01]]\n",
            "[[6.1517570e-04 9.9938476e-01]]\n",
            "[[0.0012523 0.9987477]]\n",
            "[[3.4064965e-06 9.9999654e-01]]\n",
            "[[0.8826072  0.11739277]]\n",
            "[[1.3133237e-04 9.9986863e-01]]\n",
            "[[1.9055809e-04 9.9980944e-01]]\n",
            "[[2.1223312e-04 9.9978775e-01]]\n",
            "[[1.0005561e-04 9.9989998e-01]]\n",
            "[[8.014812e-04 9.991985e-01]]\n",
            "[[0.01646966 0.98353034]]\n",
            "[[6.1286584e-04 9.9938715e-01]]\n",
            "[[0.43124413 0.5687559 ]]\n",
            "[[8.4669719e-04 9.9915326e-01]]\n",
            "[[0.03879731 0.9612027 ]]\n",
            "[[0.00558412 0.9944159 ]]\n",
            "[[0.0128157 0.9871842]]\n",
            "[[0.41649503 0.58350503]]\n",
            "[[0.00119577 0.9988042 ]]\n",
            "[[2.1480661e-04 9.9978524e-01]]\n",
            "[[3.0989115e-05 9.9996901e-01]]\n",
            "[[0.1676085  0.83239144]]\n",
            "[[0.01697863 0.9830214 ]]\n",
            "[[0.00107393 0.9989261 ]]\n",
            "[[1.4120802e-04 9.9985874e-01]]\n",
            "[[0.10369755 0.8963024 ]]\n",
            "[[0.02441209 0.9755879 ]]\n",
            "[[0.02746344 0.9725365 ]]\n",
            "[[0.22937928 0.7706207 ]]\n",
            "[[5.5865843e-05 9.9994409e-01]]\n",
            "[[4.1663907e-06 9.9999583e-01]]\n",
            "[[2.0844578e-04 9.9979156e-01]]\n",
            "[[0.01192755 0.9880725 ]]\n",
            "[[5.1764640e-05 9.9994826e-01]]\n",
            "[[0.10262101 0.897379  ]]\n",
            "[[0.05241722 0.9475827 ]]\n",
            "[[0.38821486 0.6117851 ]]\n",
            "[[0.00104677 0.9989532 ]]\n",
            "[[4.601707e-05 9.999540e-01]]\n",
            "[[1.8862662e-05 9.9998116e-01]]\n",
            "[[0.7959197  0.20408024]]\n",
            "[[3.8383534e-04 9.9961615e-01]]\n",
            "[[1.3225632e-04 9.9986768e-01]]\n",
            "[[0.00433007 0.99566996]]\n",
            "[[2.7103737e-05 9.9997294e-01]]\n",
            "[[0.02670994 0.9732901 ]]\n",
            "[[0.01380964 0.9861903 ]]\n",
            "[[6.618964e-07 9.999993e-01]]\n",
            "[[4.6721895e-04 9.9953282e-01]]\n",
            "[[0.016836 0.983164]]\n",
            "[[0.06077955 0.9392204 ]]\n",
            "[[0.45933998 0.54066   ]]\n",
            "[[1.2753521e-04 9.9987245e-01]]\n",
            "[[0.06129617 0.9387038 ]]\n",
            "[[0.00315547 0.99684453]]\n",
            "[[6.1457154e-06 9.9999380e-01]]\n",
            "[[0.00369132 0.9963086 ]]\n",
            "[[2.1595065e-04 9.9978405e-01]]\n",
            "[[0.0106231 0.9893769]]\n",
            "[[7.3717954e-04 9.9926287e-01]]\n",
            "[[0.00353308 0.9964669 ]]\n",
            "[[3.4746603e-04 9.9965250e-01]]\n",
            "[[0.00242343 0.99757653]]\n",
            "[[7.1625609e-06 9.9999285e-01]]\n",
            "[[2.1319867e-05 9.9997866e-01]]\n",
            "[[4.0266636e-06 9.9999595e-01]]\n",
            "[[0.17979002 0.82021   ]]\n",
            "[[0.82230616 0.17769392]]\n",
            "[[2.6488508e-06 9.9999738e-01]]\n",
            "[[2.2533837e-05 9.9997747e-01]]\n",
            "[[0.00112666 0.9988733 ]]\n",
            "[[0.0437126  0.95628744]]\n",
            "[[0.8608704  0.13912956]]\n",
            "[[2.2558447e-04 9.9977440e-01]]\n",
            "[[4.7795934e-04 9.9952209e-01]]\n",
            "[[0.00238369 0.9976163 ]]\n",
            "[[2.5188594e-05 9.9997485e-01]]\n",
            "[[0.00413589 0.99586415]]\n",
            "[[0.01536243 0.98463756]]\n",
            "[[1.7514035e-04 9.9982494e-01]]\n",
            "[[4.6860136e-04 9.9953139e-01]]\n",
            "[[8.5431134e-04 9.9914563e-01]]\n",
            "[[0.09959587 0.90040416]]\n",
            "[[1.9943584e-05 9.9998009e-01]]\n",
            "[[6.204694e-05 9.999379e-01]]\n",
            "[[0.21663997 0.78336006]]\n",
            "[[0.00391184 0.99608815]]\n",
            "[[0.6979361 0.3020639]]\n",
            "[[0.24864142 0.7513586 ]]\n",
            "[[8.267143e-04 9.991732e-01]]\n",
            "[[4.698101e-04 9.995302e-01]]\n",
            "[[0.00857892 0.99142104]]\n",
            "[[4.6050736e-06 9.9999535e-01]]\n",
            "[[0.01787487 0.98212516]]\n",
            "[[5.333038e-05 9.999467e-01]]\n",
            "[[0.10422236 0.89577764]]\n",
            "[[6.7122414e-04 9.9932885e-01]]\n",
            "[[0.01055925 0.98944074]]\n",
            "[[0.06120714 0.9387929 ]]\n",
            "[[1.6592769e-04 9.9983406e-01]]\n",
            "[[0.45659408 0.54340595]]\n",
            "[[0.8919145  0.10808558]]\n",
            "[[0.01526321 0.98473674]]\n",
            "[[0.08252425 0.9174757 ]]\n",
            "[[2.3370136e-04 9.9976629e-01]]\n",
            "[[0.00439108 0.99560887]]\n",
            "[[0.03647571 0.9635243 ]]\n",
            "[[0.00216729 0.9978327 ]]\n",
            "[[1.1855264e-05 9.9998820e-01]]\n",
            "[[0.20547569 0.79452425]]\n",
            "[[0.00467143 0.99532855]]\n",
            "[[4.8228587e-05 9.9995172e-01]]\n",
            "[[0.5595008  0.44049916]]\n",
            "[[0.00310179 0.9968982 ]]\n",
            "[[0.61931527 0.3806847 ]]\n",
            "[[2.3343167e-04 9.9976653e-01]]\n",
            "[[0.5817688  0.41823116]]\n",
            "[[0.59444594 0.4055541 ]]\n",
            "[[0.7416354  0.25836462]]\n",
            "[[0.07493751 0.9250625 ]]\n",
            "[[0.04835123 0.9516488 ]]\n",
            "[[3.5038272e-06 9.9999654e-01]]\n",
            "[[0.22443311 0.77556694]]\n",
            "[[4.9229304e-04 9.9950767e-01]]\n",
            "[[0.01795011 0.9820499 ]]\n",
            "[[1.1188384e-05 9.9998879e-01]]\n",
            "[[0.76395243 0.23604758]]\n",
            "[[2.0402935e-05 9.9997962e-01]]\n",
            "[[0.00208568 0.9979144 ]]\n",
            "[[3.8358403e-04 9.9961638e-01]]\n",
            "[[0.0017827 0.9982173]]\n",
            "[[0.07166211 0.92833793]]\n",
            "[[0.1516519  0.84834814]]\n",
            "[[0.09341057 0.9065894 ]]\n",
            "[[0.00133678 0.9986632 ]]\n",
            "[[0.0013297  0.99867034]]\n",
            "[[0.01030847 0.98969156]]\n",
            "[[5.2904277e-05 9.9994707e-01]]\n",
            "[[6.5276644e-04 9.9934727e-01]]\n",
            "[[0.00450859 0.99549145]]\n",
            "[[0.01870706 0.9812929 ]]\n",
            "[[0.37212902 0.62787104]]\n",
            "[[0.00638582 0.99361426]]\n",
            "[[0.00395041 0.9960496 ]]\n",
            "[[5.3661265e-06 9.9999464e-01]]\n",
            "[[0.10146136 0.8985387 ]]\n",
            "[[0.19077791 0.80922204]]\n",
            "[[0.38076156 0.61923844]]\n",
            "[[2.6486666e-04 9.9973518e-01]]\n",
            "[[0.9346383  0.06536165]]\n",
            "[[0.00331649 0.99668354]]\n",
            "[[0.570607   0.42939296]]\n",
            "[[0.15032265 0.8496774 ]]\n",
            "[[7.806795e-04 9.992193e-01]]\n",
            "[[3.9706614e-05 9.9996030e-01]]\n",
            "[[2.1310996e-06 9.9999785e-01]]\n",
            "[[7.9873338e-04 9.9920124e-01]]\n",
            "[[0.7315862 0.2684138]]\n",
            "[[0.22244355 0.77755636]]\n",
            "[[0.00624445 0.9937556 ]]\n",
            "[[0.04323282 0.95676714]]\n",
            "[[3.9611838e-05 9.9996042e-01]]\n",
            "[[0.11058027 0.8894198 ]]\n",
            "[[0.00808483 0.9919152 ]]\n",
            "[[0.01476168 0.9852384 ]]\n",
            "[[6.711605e-05 9.999329e-01]]\n",
            "[[0.10472526 0.89527476]]\n",
            "[[0.60195947 0.39804053]]\n",
            "[[0.6626162 0.3373839]]\n",
            "[[0.006226   0.99377394]]\n",
            "[[0.03616637 0.9638336 ]]\n",
            "[[3.884037e-04 9.996117e-01]]\n",
            "[[0.01792121 0.98207885]]\n",
            "[[0.02495684 0.9750431 ]]\n",
            "[[2.6560482e-04 9.9973434e-01]]\n",
            "[[0.25375834 0.7462417 ]]\n",
            "[[2.0973304e-04 9.9979025e-01]]\n",
            "[[0.0023678  0.99763215]]\n",
            "[[4.6245565e-05 9.9995375e-01]]\n",
            "[[0.5878164  0.41218352]]\n",
            "[[0.20378755 0.7962125 ]]\n",
            "[[0.9640171  0.03598289]]\n",
            "[[9.8763616e-04 9.9901235e-01]]\n",
            "[[1.4540677e-05 9.9998546e-01]]\n",
            "[[4.0375230e-06 9.9999595e-01]]\n",
            "[[0.3184877 0.6815123]]\n",
            "[[4.0355002e-04 9.9959642e-01]]\n",
            "[[5.691557e-06 9.999943e-01]]\n",
            "[[0.80422544 0.19577457]]\n",
            "[[0.00727747 0.9927226 ]]\n",
            "[[0.06843213 0.93156785]]\n",
            "[[2.4729723e-04 9.9975270e-01]]\n",
            "[[0.01354734 0.98645264]]\n",
            "[[2.0756250e-05 9.9997926e-01]]\n",
            "[[0.00582905 0.99417096]]\n",
            "[[0.02007614 0.9799239 ]]\n",
            "[[4.1409995e-04 9.9958593e-01]]\n",
            "[[1.9173536e-05 9.9998081e-01]]\n",
            "[[0.10619285 0.8938072 ]]\n",
            "[[0.00106356 0.9989365 ]]\n",
            "[[0.009169 0.990831]]\n",
            "[[9.744977e-06 9.999902e-01]]\n",
            "[[0.00643913 0.9935609 ]]\n",
            "[[2.485574e-06 9.999975e-01]]\n",
            "[[0.07989674 0.92010325]]\n",
            "[[0.2852461 0.7147539]]\n",
            "[[0.50215 0.49785]]\n",
            "[[3.804947e-04 9.996195e-01]]\n",
            "[[6.556422e-04 9.993443e-01]]\n",
            "[[1.3174416e-04 9.9986827e-01]]\n",
            "[[0.00208255 0.9979175 ]]\n",
            "[[4.941136e-04 9.995059e-01]]\n",
            "[[3.9251221e-04 9.9960750e-01]]\n",
            "[[0.00927438 0.9907257 ]]\n",
            "[[0.6896992  0.31030086]]\n",
            "[[0.19233176 0.80766827]]\n",
            "[[0.00248907 0.9975109 ]]\n",
            "[[2.9450754e-04 9.9970549e-01]]\n",
            "[[6.508866e-05 9.999349e-01]]\n",
            "[[7.4975786e-04 9.9925023e-01]]\n",
            "[[0.01871405 0.9812859 ]]\n",
            "[[0.0096968 0.9903032]]\n",
            "[[0.01887829 0.98112166]]\n",
            "[[0.05503498 0.94496506]]\n",
            "[[0.0355232  0.96447676]]\n",
            "[[1.0642897e-04 9.9989355e-01]]\n",
            "[[1.6192671e-04 9.9983799e-01]]\n",
            "[[0.00898398 0.99101603]]\n",
            "[[0.8219031  0.17809682]]\n",
            "[[0.00206567 0.99793434]]\n",
            "[[0.04703838 0.9529617 ]]\n",
            "[[0.02764344 0.9723566 ]]\n",
            "[[0.22438845 0.7756115 ]]\n",
            "[[0.01859185 0.9814082 ]]\n",
            "[[0.0023302  0.99766976]]\n",
            "[[0.002633   0.99736696]]\n",
            "[[9.909470e-06 9.999901e-01]]\n",
            "[[0.02159485 0.9784052 ]]\n",
            "[[2.6125633e-04 9.9973875e-01]]\n",
            "[[0.09286052 0.9071395 ]]\n",
            "[[0.0010984 0.9989016]]\n",
            "[[0.00126316 0.9987368 ]]\n",
            "[[0.1968862 0.8031138]]\n",
            "[[0.00794778 0.9920522 ]]\n",
            "[[0.00168496 0.9983151 ]]\n",
            "[[0.00630827 0.9936917 ]]\n",
            "[[4.943141e-05 9.999505e-01]]\n",
            "[[4.086203e-05 9.999591e-01]]\n",
            "[[0.02335656 0.9766434 ]]\n",
            "[[0.8321743  0.16782576]]\n",
            "[[0.00879894 0.99120104]]\n",
            "[[6.4757252e-05 9.9993527e-01]]\n",
            "[[0.6253876  0.37461242]]\n",
            "[[0.04852428 0.95147574]]\n",
            "[[0.00153145 0.9984686 ]]\n",
            "[[1.3869800e-04 9.9986124e-01]]\n",
            "[[2.1991815e-04 9.9978012e-01]]\n",
            "[[2.5797395e-05 9.9997425e-01]]\n",
            "[[2.581523e-06 9.999974e-01]]\n",
            "[[3.3295166e-04 9.9966705e-01]]\n",
            "[[0.17938125 0.82061875]]\n",
            "[[0.4290652 0.5709348]]\n",
            "[[0.00191166 0.9980883 ]]\n",
            "[[1.7230453e-04 9.9982762e-01]]\n",
            "[[1.6697009e-04 9.9983299e-01]]\n",
            "[[3.795507e-05 9.999621e-01]]\n",
            "[[5.0599570e-04 9.9949396e-01]]\n",
            "[[0.9303535  0.06964644]]\n",
            "[[2.9951295e-06 9.9999702e-01]]\n",
            "[[8.631022e-05 9.999137e-01]]\n",
            "[[0.00236286 0.99763715]]\n",
            "[[0.00335783 0.99664223]]\n",
            "[[9.637586e-04 9.990363e-01]]\n",
            "[[0.01599552 0.9840045 ]]\n",
            "[[0.00105198 0.99894804]]\n",
            "[[6.506807e-04 9.993493e-01]]\n",
            "[[0.00547558 0.9945245 ]]\n",
            "[[1.5093107e-04 9.9984908e-01]]\n",
            "[[6.6150292e-06 9.9999344e-01]]\n",
            "[[0.98676956 0.01323041]]\n",
            "[[0.16188675 0.8381133 ]]\n",
            "[[0.14932129 0.8506787 ]]\n",
            "[[9.1582340e-05 9.9990845e-01]]\n",
            "[[0.58374524 0.41625473]]\n",
            "[[0.22098295 0.77901703]]\n",
            "[[0.29621655 0.70378345]]\n",
            "[[0.00258796 0.997412  ]]\n",
            "[[0.02262292 0.9773771 ]]\n",
            "[[2.9432993e-05 9.9997056e-01]]\n",
            "[[1.9105367e-04 9.9980897e-01]]\n",
            "[[0.21781658 0.7821834 ]]\n",
            "[[4.2700496e-05 9.9995732e-01]]\n",
            "[[0.00396329 0.9960367 ]]\n",
            "[[1.261317e-04 9.998739e-01]]\n",
            "[[1.7054675e-06 9.9999833e-01]]\n",
            "[[7.713043e-04 9.992287e-01]]\n",
            "[[0.03601382 0.9639862 ]]\n",
            "[[7.679362e-07 9.999993e-01]]\n",
            "[[0.01072969 0.9892703 ]]\n",
            "[[5.6115354e-05 9.9994385e-01]]\n",
            "[[0.03388432 0.9661157 ]]\n",
            "[[0.08048036 0.9195196 ]]\n",
            "[[0.68839407 0.31160593]]\n",
            "[[0.9001651  0.09983484]]\n",
            "[[0.38083205 0.619168  ]]\n",
            "[[1.9089306e-04 9.9980909e-01]]\n",
            "[[2.3580305e-05 9.9997640e-01]]\n",
            "[[1.04828185e-04 9.99895215e-01]]\n",
            "[[4.2740788e-04 9.9957258e-01]]\n",
            "[[4.2625918e-05 9.9995732e-01]]\n",
            "[[0.01049761 0.9895023 ]]\n",
            "[[0.00631966 0.99368036]]\n",
            "[[0.11732362 0.8826764 ]]\n",
            "[[0.01267057 0.9873295 ]]\n",
            "[[0.04859011 0.95140994]]\n",
            "[[0.4135232 0.5864768]]\n",
            "[[4.3366777e-05 9.9995661e-01]]\n",
            "[[1.3603001e-06 9.9999869e-01]]\n",
            "[[0.04066277 0.95933723]]\n",
            "[[0.08177579 0.9182242 ]]\n",
            "[[8.9821621e-04 9.9910176e-01]]\n",
            "[[0.00158686 0.9984131 ]]\n",
            "[[0.00244881 0.99755114]]\n",
            "[[9.9345087e-04 9.9900657e-01]]\n",
            "[[0.05916782 0.94083214]]\n",
            "[[4.1243288e-04 9.9958760e-01]]\n",
            "[[0.02460994 0.97539   ]]\n",
            "[[0.00242742 0.9975726 ]]\n",
            "[[0.20822532 0.7917747 ]]\n",
            "[[0.0014689 0.9985311]]\n",
            "[[9.9664659e-04 9.9900335e-01]]\n",
            "[[0.75951684 0.24048316]]\n",
            "[[0.2672235  0.73277646]]\n",
            "[[0.00264554 0.9973545 ]]\n",
            "[[0.00107076 0.9989292 ]]\n",
            "[[1.0523149e-04 9.9989474e-01]]\n",
            "[[7.3420793e-05 9.9992657e-01]]\n",
            "[[0.00511631 0.9948837 ]]\n",
            "[[4.6607433e-06 9.9999535e-01]]\n",
            "[[0.53555846 0.46444154]]\n",
            "[[2.6660242e-05 9.9997330e-01]]\n",
            "[[5.1937369e-04 9.9948066e-01]]\n",
            "[[0.836129   0.16387102]]\n",
            "[[0.10823842 0.8917616 ]]\n",
            "[[0.50257766 0.49742234]]\n",
            "[[4.8217466e-05 9.9995184e-01]]\n",
            "[[0.18892269 0.81107736]]\n",
            "[[0.02227468 0.9777254 ]]\n",
            "[[0.03004031 0.9699597 ]]\n",
            "[[0.26144105 0.7385589 ]]\n",
            "[[2.7888813e-05 9.9997211e-01]]\n",
            "[[0.00711237 0.9928877 ]]\n",
            "[[0.01910175 0.9808983 ]]\n",
            "[[0.01091773 0.98908234]]\n",
            "[[0.0016652 0.9983348]]\n",
            "[[1.8584316e-04 9.9981421e-01]]\n",
            "[[0.04341184 0.9565882 ]]\n",
            "[[7.1859235e-05 9.9992812e-01]]\n",
            "[[3.2436587e-06 9.9999678e-01]]\n",
            "[[0.0271162 0.9728838]]\n",
            "[[0.00140278 0.9985972 ]]\n",
            "[[0.29816198 0.701838  ]]\n",
            "[[0.2653487  0.73465127]]\n",
            "[[7.057168e-07 9.999993e-01]]\n",
            "[[0.00495812 0.99504185]]\n",
            "[[8.279916e-04 9.991721e-01]]\n",
            "[[5.8587131e-05 9.9994147e-01]]\n",
            "[[2.0262221e-05 9.9997973e-01]]\n",
            "[[0.00392006 0.99608   ]]\n",
            "[[0.09525417 0.9047458 ]]\n",
            "[[8.8207837e-04 9.9911791e-01]]\n",
            "[[0.3130584 0.6869416]]\n",
            "[[1.208924e-04 9.998791e-01]]\n",
            "[[9.854306e-05 9.999014e-01]]\n",
            "[[0.33783498 0.662165  ]]\n",
            "[[3.0286223e-04 9.9969721e-01]]\n",
            "[[0.6309296  0.36907044]]\n",
            "[[3.2647437e-04 9.9967349e-01]]\n",
            "[[0.01437815 0.98562187]]\n",
            "[[0.00356174 0.99643826]]\n",
            "[[6.201394e-05 9.999380e-01]]\n",
            "[[0.0163404 0.9836596]]\n",
            "[[0.02736336 0.97263664]]\n",
            "[[0.15351735 0.84648263]]\n",
            "[[0.29341868 0.7065813 ]]\n",
            "[[0.4693034  0.53069663]]\n",
            "[[0.02737572 0.97262424]]\n",
            "[[0.02030191 0.97969806]]\n",
            "[[1.3712169e-05 9.9998629e-01]]\n",
            "[[0.00136718 0.99863285]]\n",
            "[[0.3417039 0.6582961]]\n",
            "[[8.940366e-04 9.991060e-01]]\n",
            "[[0.01071573 0.9892842 ]]\n",
            "[[0.00524882 0.99475116]]\n",
            "[[0.3409801  0.65901995]]\n",
            "[[0.01124486 0.9887552 ]]\n",
            "[[2.4205554e-04 9.9975795e-01]]\n",
            "[[5.0577051e-05 9.9994946e-01]]\n",
            "[[1.625706e-04 9.998374e-01]]\n",
            "[[4.4450347e-04 9.9955553e-01]]\n",
            "[[0.03726488 0.9627352 ]]\n",
            "[[3.0895106e-05 9.9996912e-01]]\n",
            "[[0.00115617 0.9988438 ]]\n",
            "[[0.02785179 0.97214824]]\n",
            "[[0.00201147 0.9979886 ]]\n",
            "[[0.30263272 0.69736725]]\n",
            "[[0.2054817  0.79451823]]\n",
            "[[0.00908339 0.99091655]]\n",
            "[[4.6820660e-05 9.9995315e-01]]\n",
            "[[0.01376137 0.9862386 ]]\n",
            "[[0.00124227 0.9987577 ]]\n",
            "[[4.4920345e-04 9.9955076e-01]]\n",
            "[[0.9724639  0.02753605]]\n",
            "[[0.19070931 0.8092907 ]]\n",
            "[[0.45761135 0.5423886 ]]\n",
            "[[0.0068211  0.99317884]]\n",
            "[[0.00641203 0.993588  ]]\n",
            "[[0.00281854 0.99718153]]\n",
            "[[0.01672115 0.98327893]]\n",
            "[[0.00194856 0.9980514 ]]\n",
            "[[0.00392706 0.996073  ]]\n",
            "[[5.9833983e-06 9.9999404e-01]]\n",
            "[[7.4786018e-05 9.9992526e-01]]\n",
            "[[0.00539461 0.99460536]]\n",
            "[[0.00332353 0.99667645]]\n",
            "[[0.23814087 0.7618592 ]]\n",
            "[[1.9887648e-05 9.9998009e-01]]\n",
            "[[2.571160e-04 9.997429e-01]]\n",
            "[[6.6258563e-06 9.9999332e-01]]\n",
            "[[0.00371713 0.9962829 ]]\n",
            "[[0.1959497  0.80405027]]\n",
            "[[1.12490925e-04 9.99887466e-01]]\n",
            "[[0.2729947 0.7270053]]\n",
            "[[0.00149105 0.998509  ]]\n",
            "[[0.00306689 0.9969331 ]]\n",
            "[[5.6353485e-04 9.9943644e-01]]\n",
            "[[7.1146700e-05 9.9992883e-01]]\n",
            "[[0.00387571 0.9961243 ]]\n",
            "[[2.1618346e-04 9.9978381e-01]]\n",
            "[[4.3548985e-06 9.9999559e-01]]\n",
            "[[0.737811 0.262189]]\n",
            "[[2.8247865e-05 9.9997175e-01]]\n",
            "[[0.00171491 0.9982851 ]]\n",
            "[[0.0019413 0.9980586]]\n",
            "[[5.5944687e-04 9.9944049e-01]]\n",
            "[[0.35875162 0.6412484 ]]\n",
            "[[8.789397e-04 9.991210e-01]]\n",
            "[[0.63873756 0.3612625 ]]\n",
            "[[0.03613878 0.9638613 ]]\n",
            "[[8.1556296e-04 9.9918443e-01]]\n",
            "[[0.5343713  0.46562868]]\n",
            "[[5.8689434e-04 9.9941313e-01]]\n",
            "[[0.75397336 0.24602665]]\n",
            "[[0.10811891 0.8918811 ]]\n",
            "[[0.71346736 0.28653267]]\n",
            "[[0.01593005 0.98406994]]\n",
            "[[0.12980814 0.8701919 ]]\n",
            "[[0.00215809 0.99784184]]\n",
            "[[0.4063466 0.5936534]]\n",
            "[[2.9296616e-05 9.9997067e-01]]\n",
            "[[6.8049959e-04 9.9931955e-01]]\n",
            "[[0.00104997 0.99895006]]\n",
            "[[0.02835681 0.9716432 ]]\n",
            "[[0.05047038 0.9495296 ]]\n",
            "[[0.03856739 0.9614326 ]]\n",
            "[[0.0265421  0.97345793]]\n",
            "[[0.12463862 0.8753614 ]]\n",
            "[[0.7318847 0.2681153]]\n",
            "[[0.01367365 0.98632634]]\n",
            "[[8.20333e-05 9.99918e-01]]\n",
            "[[7.4370287e-04 9.9925631e-01]]\n",
            "[[0.03186586 0.9681341 ]]\n",
            "[[3.1172112e-05 9.9996889e-01]]\n",
            "[[4.5866365e-04 9.9954140e-01]]\n",
            "[[9.896087e-06 9.999901e-01]]\n",
            "[[0.9327801  0.06721987]]\n",
            "[[4.1560856e-05 9.9995840e-01]]\n",
            "[[0.88496286 0.11503712]]\n",
            "[[2.492453e-06 9.999975e-01]]\n",
            "[[7.3859637e-06 9.9999261e-01]]\n",
            "[[2.1233756e-04 9.9978763e-01]]\n",
            "[[1.3199377e-04 9.9986804e-01]]\n",
            "[[3.7473632e-07 9.9999964e-01]]\n",
            "[[3.475803e-05 9.999652e-01]]\n",
            "[[0.00396928 0.99603075]]\n",
            "[[2.6213036e-06 9.9999738e-01]]\n",
            "[[0.00151116 0.9984888 ]]\n",
            "[[0.02158046 0.97841954]]\n",
            "[[4.0397979e-04 9.9959606e-01]]\n",
            "[[2.3386587e-04 9.9976617e-01]]\n",
            "[[2.8625405e-05 9.9997139e-01]]\n",
            "[[2.9171828e-05 9.9997079e-01]]\n",
            "[[4.160457e-04 9.995840e-01]]\n",
            "[[0.06944267 0.93055737]]\n",
            "[[0.00112456 0.99887544]]\n",
            "[[0.10709193 0.8929081 ]]\n",
            "[[1.0702126e-04 9.9989295e-01]]\n",
            "[[0.0131161  0.98688394]]\n",
            "[[0.99120456 0.00879546]]\n",
            "[[0.00786309 0.99213696]]\n",
            "[[1.4215215e-06 9.9999857e-01]]\n",
            "[[0.00692658 0.9930734 ]]\n",
            "[[0.28048572 0.71951425]]\n",
            "[[5.7698624e-05 9.9994230e-01]]\n",
            "[[7.275926e-04 9.992724e-01]]\n",
            "[[0.02243134 0.9775687 ]]\n",
            "[[0.2808007 0.7191993]]\n",
            "[[0.01204781 0.9879522 ]]\n",
            "[[0.00285625 0.9971437 ]]\n",
            "[[0.30760127 0.6923987 ]]\n",
            "[[5.0169503e-05 9.9994981e-01]]\n",
            "[[2.4838298e-06 9.9999750e-01]]\n",
            "[[4.1457064e-05 9.9995852e-01]]\n",
            "[[0.2500025 0.7499975]]\n",
            "[[0.00352245 0.99647754]]\n",
            "[[8.041580e-04 9.991959e-01]]\n",
            "[[0.45395187 0.54604816]]\n",
            "[[0.00358104 0.99641895]]\n",
            "[[0.00202947 0.9979705 ]]\n",
            "[[0.20968619 0.7903138 ]]\n",
            "[[0.0028972 0.9971028]]\n",
            "[[1.8294295e-05 9.9998176e-01]]\n",
            "[[8.495803e-05 9.999150e-01]]\n",
            "[[0.4598622 0.5401378]]\n",
            "[[0.18451244 0.8154876 ]]\n",
            "[[0.01519994 0.9848001 ]]\n",
            "[[0.01131256 0.9886875 ]]\n",
            "[[0.41047797 0.589522  ]]\n",
            "[[0.40136945 0.59863055]]\n",
            "[[0.7506867  0.24931327]]\n",
            "[[0.43392748 0.5660725 ]]\n",
            "[[0.01108496 0.988915  ]]\n",
            "[[0.01318814 0.9868118 ]]\n",
            "[[0.08015607 0.919844  ]]\n",
            "[[2.3738481e-04 9.9976259e-01]]\n",
            "[[1.2985736e-04 9.9987018e-01]]\n",
            "[[0.00146869 0.99853134]]\n",
            "[[0.01155508 0.9884449 ]]\n",
            "[[0.09135186 0.90864813]]\n",
            "[[4.8015107e-05 9.9995196e-01]]\n",
            "[[0.47573972 0.5242603 ]]\n",
            "[[2.7501953e-06 9.9999726e-01]]\n",
            "[[1.2815805e-04 9.9987185e-01]]\n",
            "[[0.20472123 0.7952787 ]]\n",
            "[[9.176863e-05 9.999082e-01]]\n",
            "[[0.00684269 0.9931573 ]]\n",
            "[[0.25131205 0.7486879 ]]\n",
            "[[0.05611519 0.9438848 ]]\n",
            "[[0.06898478 0.9310152 ]]\n",
            "[[4.2637225e-04 9.9957365e-01]]\n",
            "[[0.0131622 0.9868378]]\n",
            "[[0.00966291 0.99033713]]\n",
            "[[5.7560799e-04 9.9942446e-01]]\n",
            "[[0.0184465  0.98155355]]\n",
            "[[0.8699375  0.13006252]]\n",
            "[[0.01337857 0.9866215 ]]\n",
            "[[0.08969922 0.91030073]]\n",
            "[[0.00128774 0.9987123 ]]\n",
            "[[0.00170511 0.9982949 ]]\n",
            "[[0.00168822 0.99831176]]\n",
            "[[0.00147501 0.99852496]]\n",
            "[[0.01085659 0.98914343]]\n",
            "[[0.9303686  0.06963137]]\n",
            "[[0.1946267 0.8053733]]\n",
            "[[0.05105015 0.9489498 ]]\n",
            "[[0.02519097 0.97480905]]\n",
            "[[0.1207157 0.8792843]]\n",
            "[[0.01034693 0.98965305]]\n",
            "[[1.8777282e-05 9.9998116e-01]]\n",
            "[[0.04494715 0.9550528 ]]\n",
            "[[4.2847835e-04 9.9957150e-01]]\n",
            "[[2.2726123e-05 9.9997723e-01]]\n",
            "[[0.9358605  0.06413953]]\n",
            "[[1.7317765e-05 9.9998271e-01]]\n",
            "[[0.7174786  0.28252143]]\n",
            "[[9.803657e-06 9.999902e-01]]\n",
            "[[0.02095706 0.9790429 ]]\n",
            "[[0.06147002 0.93852997]]\n",
            "[[0.3870155 0.6129845]]\n",
            "[[0.02185647 0.9781436 ]]\n",
            "[[0.77699476 0.22300519]]\n",
            "[[0.0631595  0.93684053]]\n",
            "[[3.383676e-04 9.996617e-01]]\n",
            "[[1.2382161e-04 9.9987614e-01]]\n",
            "[[0.06250156 0.9374985 ]]\n",
            "[[0.5503154  0.44968456]]\n",
            "[[0.01506839 0.9849316 ]]\n",
            "[[0.00223874 0.99776125]]\n",
            "[[0.00632323 0.9936767 ]]\n",
            "[[6.816398e-06 9.999932e-01]]\n",
            "[[0.15233423 0.8476658 ]]\n",
            "[[0.31121767 0.6887823 ]]\n",
            "[[1.6205422e-05 9.9998379e-01]]\n",
            "[[0.00270756 0.99729246]]\n",
            "[[0.05379684 0.9462032 ]]\n",
            "[[0.0801743 0.9198257]]\n",
            "[[0.00118442 0.99881566]]\n",
            "[[0.0045944  0.99540555]]\n",
            "[[0.00119874 0.99880123]]\n",
            "[[7.2468174e-06 9.9999273e-01]]\n",
            "[[0.00227904 0.9977209 ]]\n",
            "[[0.23554245 0.7644575 ]]\n",
            "[[0.8798033  0.12019669]]\n",
            "[[0.06472775 0.9352722 ]]\n",
            "[[0.00807372 0.99192625]]\n",
            "[[0.08078724 0.91921276]]\n",
            "[[4.9759774e-04 9.9950242e-01]]\n",
            "[[2.1570371e-04 9.9978429e-01]]\n",
            "[[0.00902821 0.9909718 ]]\n",
            "[[6.4331695e-04 9.9935669e-01]]\n",
            "[[0.06519838 0.93480164]]\n",
            "[[0.8291723  0.17082764]]\n",
            "[[9.449364e-04 9.990551e-01]]\n",
            "[[3.6005353e-04 9.9963999e-01]]\n",
            "[[0.00140166 0.9985984 ]]\n",
            "[[1.6522559e-04 9.9983478e-01]]\n",
            "[[1.3986541e-04 9.9986017e-01]]\n",
            "[[2.6744942e-04 9.9973255e-01]]\n",
            "[[0.5560241  0.44397599]]\n",
            "[[0.26332578 0.7366742 ]]\n",
            "[[0.41954315 0.58045685]]\n",
            "[[0.02262622 0.97737384]]\n",
            "[[0.0078322 0.9921678]]\n",
            "[[0.00192415 0.99807584]]\n",
            "[[0.02922938 0.97077066]]\n",
            "[[0.01000576 0.9899942 ]]\n",
            "[[0.05457327 0.94542676]]\n",
            "[[0.00790223 0.99209774]]\n",
            "[[0.06846055 0.9315395 ]]\n",
            "[[2.3708084e-04 9.9976295e-01]]\n",
            "[[4.1543575e-05 9.9995840e-01]]\n",
            "[[0.24600057 0.75399935]]\n",
            "[[7.8517587e-05 9.9992144e-01]]\n",
            "[[0.00236858 0.99763143]]\n",
            "[[3.8737347e-04 9.9961263e-01]]\n",
            "[[0.582776   0.41722396]]\n",
            "[[0.00189253 0.99810743]]\n",
            "[[0.00183293 0.99816704]]\n",
            "[[0.00567785 0.9943222 ]]\n",
            "[[2.272996e-04 9.997727e-01]]\n",
            "[[1.4022557e-04 9.9985981e-01]]\n",
            "[[0.04625002 0.95375   ]]\n",
            "[[0.00220406 0.99779594]]\n",
            "[[0.01996955 0.9800305 ]]\n",
            "[[0.93255454 0.06744541]]\n",
            "[[0.0027446 0.9972554]]\n",
            "[[2.3158660e-04 9.9976844e-01]]\n",
            "[[0.00621189 0.99378806]]\n",
            "[[0.05761683 0.94238317]]\n",
            "[[0.01303188 0.9869681 ]]\n",
            "[[0.00312512 0.9968748 ]]\n",
            "[[0.08784659 0.9121534 ]]\n",
            "[[0.9527164  0.04728361]]\n",
            "[[0.63807076 0.3619292 ]]\n",
            "[[0.9324853  0.06751468]]\n",
            "[[0.01335492 0.98664504]]\n",
            "[[0.01589921 0.9841008 ]]\n",
            "[[2.0027091e-04 9.9979979e-01]]\n",
            "[[4.1023282e-05 9.9995899e-01]]\n",
            "[[9.1670845e-05 9.9990833e-01]]\n",
            "[[0.00196711 0.99803287]]\n",
            "[[0.58351845 0.41648155]]\n",
            "[[6.0951465e-04 9.9939048e-01]]\n",
            "[[7.1138557e-04 9.9928856e-01]]\n",
            "[[0.09012154 0.90987843]]\n",
            "[[3.2776894e-04 9.9967229e-01]]\n",
            "[[0.00154819 0.9984518 ]]\n",
            "[[0.00961947 0.9903805 ]]\n",
            "[[0.6027555  0.39724448]]\n",
            "[[0.01814971 0.9818503 ]]\n",
            "[[0.02346242 0.9765376 ]]\n",
            "[[0.0014772 0.9985228]]\n",
            "[[0.12105227 0.87894773]]\n",
            "[[0.13827491 0.86172515]]\n",
            "[[3.9220080e-07 9.9999964e-01]]\n",
            "[[0.04148349 0.95851654]]\n",
            "[[8.9581818e-06 9.9999106e-01]]\n",
            "[[0.00488603 0.99511397]]\n",
            "[[0.02840872 0.97159123]]\n",
            "[[0.001446   0.99855405]]\n",
            "[[4.0675548e-04 9.9959320e-01]]\n",
            "[[8.7651315e-05 9.9991238e-01]]\n",
            "[[0.10922045 0.89077955]]\n",
            "[[4.0020921e-05 9.9995995e-01]]\n",
            "[[0.88207126 0.11792869]]\n",
            "[[0.03241585 0.96758413]]\n",
            "[[0.00306215 0.9969379 ]]\n",
            "[[0.26620045 0.7337996 ]]\n",
            "[[0.16537537 0.8346246 ]]\n",
            "[[1.6651547e-05 9.9998331e-01]]\n",
            "[[0.03574568 0.9642544 ]]\n",
            "[[0.3331638 0.6668362]]\n",
            "[[0.08828125 0.9117188 ]]\n",
            "[[0.00476966 0.9952303 ]]\n",
            "[[6.793928e-04 9.993206e-01]]\n",
            "[[0.03731674 0.96268326]]\n",
            "[[4.6756020e-04 9.9953246e-01]]\n",
            "[[0.00501613 0.9949839 ]]\n",
            "[[9.1915985e-04 9.9908078e-01]]\n",
            "[[0.00577594 0.994224  ]]\n",
            "[[0.00456492 0.99543506]]\n",
            "[[0.12130537 0.87869465]]\n",
            "[[0.03817026 0.9618298 ]]\n",
            "[[0.11539077 0.8846092 ]]\n",
            "[[6.2485668e-04 9.9937516e-01]]\n",
            "[[0.06181973 0.9381803 ]]\n",
            "[[6.210604e-04 9.993789e-01]]\n",
            "[[0.21694943 0.7830506 ]]\n",
            "[[1.2520087e-04 9.9987483e-01]]\n",
            "[[0.7198144  0.28018552]]\n",
            "[[4.6398345e-04 9.9953604e-01]]\n",
            "[[0.42125404 0.5787459 ]]\n",
            "[[0.40588847 0.5941115 ]]\n",
            "[[0.00734696 0.992653  ]]\n",
            "[[2.3727065e-05 9.9997628e-01]]\n",
            "[[0.0020615  0.99793845]]\n",
            "[[0.6165473  0.38345268]]\n",
            "[[2.8837330e-06 9.9999714e-01]]\n",
            "[[0.05748059 0.94251937]]\n",
            "[[0.01978229 0.9802177 ]]\n",
            "[[0.699638 0.300362]]\n",
            "[[0.09488471 0.90511525]]\n",
            "[[0.01214329 0.9878566 ]]\n",
            "[[0.22374979 0.7762502 ]]\n",
            "[[0.01410398 0.98589605]]\n",
            "[[1.5981574e-04 9.9984014e-01]]\n",
            "[[8.864443e-05 9.999113e-01]]\n",
            "[[5.4922515e-05 9.9994504e-01]]\n",
            "[[0.00523621 0.99476373]]\n",
            "[[0.08468881 0.91531116]]\n",
            "[[2.3888379e-05 9.9997616e-01]]\n",
            "[[0.00358916 0.9964108 ]]\n",
            "[[0.09818318 0.9018168 ]]\n",
            "[[0.6683313  0.33166867]]\n",
            "[[0.01048421 0.9895158 ]]\n",
            "[[0.03992368 0.9600763 ]]\n",
            "[[5.7622194e-05 9.9994242e-01]]\n",
            "[[2.3319610e-04 9.9976677e-01]]\n",
            "[[0.00136557 0.9986344 ]]\n",
            "[[8.8621535e-05 9.9991143e-01]]\n",
            "[[0.93467444 0.06532557]]\n",
            "[[1.7810100e-05 9.9998224e-01]]\n",
            "[[0.00173638 0.99826366]]\n",
            "[[0.9237485  0.07625149]]\n",
            "[[7.6040789e-04 9.9923956e-01]]\n",
            "[[0.00150351 0.99849653]]\n",
            "[[0.24882635 0.7511736 ]]\n",
            "[[0.9621177  0.03788232]]\n",
            "[[0.00895462 0.9910453 ]]\n",
            "[[9.729656e-04 9.990270e-01]]\n",
            "[[0.00108343 0.9989166 ]]\n",
            "[[0.72188586 0.27811414]]\n",
            "[[6.0463045e-04 9.9939537e-01]]\n",
            "[[5.1708712e-04 9.9948287e-01]]\n",
            "[[0.00202891 0.9979711 ]]\n",
            "[[0.92051065 0.07948937]]\n",
            "[[0.00242734 0.9975726 ]]\n",
            "[[0.04029276 0.95970726]]\n",
            "[[0.07164938 0.9283506 ]]\n",
            "[[0.00257824 0.99742174]]\n",
            "[[2.7511319e-06 9.9999726e-01]]\n",
            "[[0.00227416 0.9977258 ]]\n",
            "[[0.44034097 0.559659  ]]\n",
            "[[0.28928396 0.71071607]]\n",
            "[[0.00628544 0.9937145 ]]\n",
            "[[5.5577526e-05 9.9994445e-01]]\n",
            "[[3.9976972e-04 9.9960023e-01]]\n",
            "[[0.00563345 0.9943666 ]]\n",
            "[[5.4100447e-06 9.9999464e-01]]\n",
            "[[0.00191906 0.99808097]]\n",
            "[[1.5851328e-04 9.9984145e-01]]\n",
            "[[0.00189768 0.99810237]]\n",
            "[[0.0024518  0.99754816]]\n",
            "[[1.3099295e-04 9.9986899e-01]]\n",
            "[[0.8667772  0.13322279]]\n",
            "[[3.3965282e-06 9.9999666e-01]]\n",
            "[[0.04939812 0.9506019 ]]\n",
            "[[4.1460831e-04 9.9958545e-01]]\n",
            "[[2.4405259e-05 9.9997556e-01]]\n",
            "[[0.026428 0.973572]]\n",
            "[[4.1149924e-06 9.9999583e-01]]\n",
            "[[2.1531303e-05 9.9997842e-01]]\n",
            "[[2.8996228e-04 9.9971002e-01]]\n",
            "[[0.01300508 0.9869949 ]]\n",
            "[[5.9321064e-05 9.9994063e-01]]\n",
            "[[0.0117609  0.98823905]]\n",
            "[[0.00220206 0.99779797]]\n",
            "[[0.00790921 0.9920908 ]]\n",
            "[[1.0888061e-06 9.9999893e-01]]\n",
            "[[0.00127252 0.9987275 ]]\n",
            "[[1.4660788e-04 9.9985337e-01]]\n",
            "[[0.22748624 0.77251375]]\n",
            "[[0.12088349 0.87911654]]\n",
            "[[0.03324353 0.9667565 ]]\n",
            "[[0.23394805 0.766052  ]]\n",
            "[[0.01434354 0.9856565 ]]\n",
            "[[0.20746563 0.7925344 ]]\n",
            "[[0.1176329  0.88236713]]\n",
            "[[3.4132402e-04 9.9965870e-01]]\n",
            "[[0.19629835 0.80370164]]\n",
            "[[0.00450913 0.99549085]]\n",
            "[[0.00155747 0.9984426 ]]\n",
            "[[3.5325924e-04 9.9964678e-01]]\n",
            "[[0.2290371 0.7709629]]\n",
            "[[6.0342468e-04 9.9939656e-01]]\n",
            "[[0.03011037 0.9698896 ]]\n",
            "[[8.986001e-04 9.991014e-01]]\n",
            "[[5.2868016e-04 9.9947137e-01]]\n",
            "[[7.156725e-04 9.992843e-01]]\n",
            "[[7.0228818e-04 9.9929774e-01]]\n",
            "[[0.00320258 0.9967975 ]]\n",
            "[[2.7466015e-04 9.9972528e-01]]\n",
            "[[1.726862e-05 9.999827e-01]]\n",
            "[[0.05522169 0.94477826]]\n",
            "[[1.09359156e-04 9.99890685e-01]]\n",
            "[[1.904584e-05 9.999809e-01]]\n",
            "[[0.01111589 0.9888841 ]]\n",
            "[[1.042352e-04 9.998958e-01]]\n",
            "[[0.65243596 0.34756398]]\n",
            "[[0.32689685 0.6731031 ]]\n",
            "[[0.18732238 0.8126776 ]]\n",
            "[[0.4932327  0.50676733]]\n",
            "[[2.1259790e-07 9.9999976e-01]]\n",
            "[[0.01346997 0.98653007]]\n",
            "[[0.0695798 0.9304202]]\n",
            "[[4.093180e-05 9.999591e-01]]\n",
            "[[0.9881071  0.01189296]]\n",
            "[[0.00346903 0.99653095]]\n",
            "[[8.484663e-05 9.999151e-01]]\n",
            "[[0.43123588 0.5687641 ]]\n",
            "[[0.004996 0.995004]]\n",
            "[[0.00122492 0.99877506]]\n",
            "[[9.2158944e-04 9.9907839e-01]]\n",
            "[[0.2979274 0.7020726]]\n",
            "[[4.4771325e-04 9.9955231e-01]]\n",
            "[[0.00505121 0.99494874]]\n",
            "[[1.9603542e-06 9.9999809e-01]]\n",
            "[[1.537470e-04 9.998462e-01]]\n",
            "[[0.00263325 0.9973667 ]]\n",
            "[[0.1266904  0.87330955]]\n",
            "[[0.8965026  0.10349736]]\n",
            "[[3.697304e-04 9.996302e-01]]\n",
            "[[0.60940945 0.39059058]]\n",
            "[[0.11347339 0.8865266 ]]\n",
            "[[0.99251586 0.00748411]]\n",
            "[[9.950219e-06 9.999901e-01]]\n",
            "[[1.128559e-04 9.998871e-01]]\n",
            "[[0.9458876 0.0541124]]\n",
            "[[5.561293e-06 9.999944e-01]]\n",
            "[[0.00403927 0.9959608 ]]\n",
            "[[2.4802462e-06 9.9999750e-01]]\n",
            "[[7.5136522e-06 9.9999249e-01]]\n",
            "[[0.00842193 0.9915781 ]]\n",
            "[[0.5378199  0.46218008]]\n",
            "[[5.5883324e-04 9.9944121e-01]]\n",
            "[[0.5255788  0.47442123]]\n",
            "[[0.00209043 0.9979096 ]]\n",
            "[[0.02052897 0.979471  ]]\n",
            "[[2.0039985e-04 9.9979967e-01]]\n",
            "[[0.00454081 0.9954592 ]]\n",
            "[[0.00124168 0.9987583 ]]\n",
            "[[0.9933119  0.00668819]]\n",
            "[[0.00132916 0.9986708 ]]\n",
            "[[4.236895e-06 9.999957e-01]]\n",
            "[[0.8771727  0.12282733]]\n",
            "[[0.00122872 0.99877125]]\n",
            "[[7.385837e-06 9.999926e-01]]\n",
            "[[0.03182008 0.96817994]]\n",
            "[[0.02522453 0.9747755 ]]\n",
            "[[3.9210869e-04 9.9960786e-01]]\n",
            "[[0.01557294 0.98442703]]\n",
            "[[0.82925    0.17075005]]\n",
            "[[0.57171094 0.4282891 ]]\n",
            "[[2.5540416e-05 9.9997449e-01]]\n",
            "[[0.0965497  0.90345025]]\n",
            "[[4.997634e-04 9.995003e-01]]\n",
            "[[0.00801698 0.991983  ]]\n",
            "[[4.196787e-04 9.995803e-01]]\n",
            "[[0.05964994 0.94035006]]\n",
            "[[0.03776821 0.9622318 ]]\n",
            "[[6.1872524e-05 9.9993813e-01]]\n",
            "[[0.07471234 0.92528766]]\n",
            "[[0.8734112  0.12658885]]\n",
            "[[3.5539237e-04 9.9964464e-01]]\n",
            "[[0.00122428 0.99877566]]\n",
            "[[8.238167e-05 9.999176e-01]]\n",
            "[[0.005368 0.994632]]\n",
            "[[0.00799281 0.99200714]]\n",
            "[[8.9441653e-04 9.9910551e-01]]\n",
            "[[1.5192853e-04 9.9984801e-01]]\n",
            "[[0.02733385 0.9726661 ]]\n",
            "[[1.0125037e-06 9.9999905e-01]]\n",
            "[[0.06189282 0.9381072 ]]\n",
            "[[7.4902993e-05 9.9992514e-01]]\n",
            "[[0.001744 0.998256]]\n",
            "[[0.03203858 0.9679614 ]]\n",
            "[[6.4571947e-07 9.9999940e-01]]\n",
            "[[0.00177614 0.99822384]]\n",
            "[[5.289136e-04 9.994711e-01]]\n",
            "[[4.4499987e-04 9.9955493e-01]]\n",
            "[[0.00222494 0.997775  ]]\n",
            "[[0.43362752 0.56637245]]\n",
            "[[1.0386987e-04 9.9989617e-01]]\n",
            "[[1.6859551e-06 9.9999833e-01]]\n",
            "[[0.06817309 0.93182695]]\n",
            "[[3.3200614e-04 9.9966800e-01]]\n",
            "[[0.01134582 0.98865414]]\n",
            "[[1.552846e-05 9.999845e-01]]\n",
            "[[0.05935511 0.94064486]]\n",
            "[[0.00124851 0.9987515 ]]\n",
            "[[0.1108956 0.8891044]]\n",
            "[[0.01535943 0.9846406 ]]\n",
            "[[6.739823e-04 9.993260e-01]]\n",
            "[[6.5097905e-04 9.9934906e-01]]\n",
            "[[3.2912570e-04 9.9967086e-01]]\n",
            "[[1.9366364e-05 9.9998069e-01]]\n",
            "[[0.00901212 0.99098784]]\n",
            "[[0.00348087 0.9965191 ]]\n",
            "[[9.946329e-06 9.999901e-01]]\n",
            "[[2.6702328e-05 9.9997330e-01]]\n",
            "[[8.066908e-04 9.991934e-01]]\n",
            "[[0.7125039  0.28749612]]\n",
            "[[0.00120764 0.9987923 ]]\n",
            "[[0.0109654  0.98903465]]\n",
            "[[1.06509164e-04 9.99893427e-01]]\n",
            "[[3.785527e-04 9.996214e-01]]\n",
            "[[0.02739823 0.97260183]]\n",
            "[[0.00202294 0.9979771 ]]\n",
            "[[0.02615058 0.97384936]]\n",
            "[[0.86433315 0.13566688]]\n",
            "[[0.5928902  0.40710986]]\n",
            "[[3.2156245e-06 9.9999678e-01]]\n",
            "[[0.5025122 0.4974878]]\n",
            "[[0.32115442 0.67884564]]\n",
            "[[0.00180479 0.99819523]]\n",
            "[[0.01242389 0.9875761 ]]\n",
            "[[0.07647839 0.9235216 ]]\n",
            "[[3.582149e-06 9.999964e-01]]\n",
            "[[9.2169510e-05 9.9990785e-01]]\n",
            "[[1.5966006e-04 9.9984026e-01]]\n",
            "[[1.8570428e-04 9.9981433e-01]]\n",
            "[[0.85069704 0.14930299]]\n",
            "[[5.2464275e-05 9.9994755e-01]]\n",
            "[[0.00615118 0.99384886]]\n",
            "[[8.2345156e-05 9.9991763e-01]]\n",
            "[[0.02976155 0.97023845]]\n",
            "[[0.2337443 0.7662558]]\n",
            "[[0.00113538 0.99886465]]\n",
            "[[7.9713740e-05 9.9992025e-01]]\n",
            "[[0.00851128 0.9914887 ]]\n",
            "[[0.7600142  0.23998585]]\n",
            "[[1.1328709e-04 9.9988675e-01]]\n",
            "[[0.00417812 0.99582183]]\n",
            "[[6.896188e-06 9.999931e-01]]\n",
            "[[0.00417409 0.9958259 ]]\n",
            "[[1.0787993e-05 9.9998927e-01]]\n",
            "[[0.00179394 0.998206  ]]\n",
            "[[0.95625216 0.04374783]]\n",
            "[[0.20480952 0.7951905 ]]\n",
            "[[9.6836803e-04 9.9903166e-01]]\n",
            "[[2.082819e-06 9.999980e-01]]\n",
            "[[0.04565651 0.9543435 ]]\n",
            "[[0.36855537 0.63144463]]\n",
            "[[0.9578625  0.04213752]]\n",
            "[[0.0023069 0.9976931]]\n",
            "[[0.0706774  0.92932266]]\n",
            "[[1.6564500e-05 9.9998343e-01]]\n",
            "[[0.03702912 0.9629709 ]]\n",
            "[[3.2398188e-05 9.9996758e-01]]\n",
            "[[6.672483e-04 9.993327e-01]]\n",
            "[[3.7699763e-04 9.9962294e-01]]\n",
            "[[0.03951686 0.96048313]]\n",
            "[[0.07188614 0.9281139 ]]\n",
            "[[0.00733819 0.99266183]]\n",
            "[[0.00951418 0.99048585]]\n",
            "[[0.04174894 0.958251  ]]\n",
            "[[0.03201519 0.96798486]]\n",
            "[[0.60550785 0.39449218]]\n",
            "[[2.8571158e-05 9.9997139e-01]]\n",
            "[[0.22393559 0.7760644 ]]\n",
            "[[0.01208271 0.98791724]]\n",
            "[[2.4322055e-04 9.9975675e-01]]\n",
            "[[7.8420540e-05 9.9992156e-01]]\n",
            "[[0.08537006 0.91462994]]\n",
            "[[0.0222136  0.97778636]]\n",
            "[[0.18093993 0.8190601 ]]\n",
            "[[0.00283336 0.9971667 ]]\n",
            "[[0.0049477 0.9950523]]\n",
            "[[0.02003567 0.9799643 ]]\n",
            "[[5.4194345e-05 9.9994576e-01]]\n",
            "[[0.00766139 0.99233854]]\n",
            "[[0.00666774 0.9933322 ]]\n",
            "[[0.11964577 0.88035417]]\n",
            "[[5.0176465e-04 9.9949825e-01]]\n",
            "[[0.01527253 0.9847275 ]]\n",
            "[[0.14596063 0.8540394 ]]\n",
            "[[1.310188e-04 9.998690e-01]]\n",
            "[[7.570496e-05 9.999243e-01]]\n",
            "[[0.02039175 0.9796083 ]]\n",
            "[[0.17714186 0.8228581 ]]\n",
            "[[8.7690772e-05 9.9991226e-01]]\n",
            "[[1.8147138e-04 9.9981850e-01]]\n",
            "[[0.0073811  0.99261886]]\n",
            "[[0.00772791 0.99227214]]\n",
            "[[0.00437734 0.9956227 ]]\n",
            "[[0.11163896 0.8883611 ]]\n",
            "[[2.0117135e-04 9.9979883e-01]]\n",
            "[[0.41480258 0.58519745]]\n",
            "[[0.8072559  0.19274405]]\n",
            "[[3.156768e-05 9.999684e-01]]\n",
            "[[8.619227e-06 9.999914e-01]]\n",
            "[[0.24242994 0.75757   ]]\n",
            "[[0.03266535 0.9673346 ]]\n",
            "[[3.0526306e-04 9.9969471e-01]]\n",
            "[[6.6789282e-05 9.9993324e-01]]\n",
            "[[3.114614e-05 9.999689e-01]]\n",
            "[[0.01711092 0.98288906]]\n",
            "[[5.122026e-06 9.999949e-01]]\n",
            "[[0.01019884 0.9898011 ]]\n",
            "[[4.164615e-06 9.999958e-01]]\n",
            "[[4.1156745e-04 9.9958843e-01]]\n",
            "[[0.00122646 0.9987735 ]]\n",
            "[[2.2333847e-05 9.9997771e-01]]\n",
            "[[7.1990135e-04 9.9928015e-01]]\n",
            "[[0.01167527 0.98832476]]\n",
            "[[0.00960416 0.99039584]]\n",
            "[[0.01886414 0.9811359 ]]\n",
            "[[0.00222291 0.99777704]]\n",
            "[[8.4799546e-04 9.9915195e-01]]\n",
            "[[0.01936475 0.9806353 ]]\n",
            "[[0.00106332 0.9989367 ]]\n",
            "[[0.31150925 0.6884907 ]]\n",
            "[[0.18543011 0.81456983]]\n",
            "[[4.3316813e-05 9.9995673e-01]]\n",
            "[[0.00219827 0.9978017 ]]\n",
            "[[0.00472128 0.9952787 ]]\n",
            "[[0.00252522 0.99747473]]\n",
            "[[0.03981704 0.96018296]]\n",
            "[[0.30895457 0.6910454 ]]\n",
            "[[9.055730e-04 9.990945e-01]]\n",
            "[[0.5181183  0.48188174]]\n",
            "[[0.87455887 0.1254411 ]]\n",
            "[[0.45019972 0.5498002 ]]\n",
            "[[0.89724106 0.10275898]]\n",
            "[[0.00606644 0.9939335 ]]\n",
            "[[4.5893301e-04 9.9954104e-01]]\n",
            "[[0.11088192 0.88911813]]\n",
            "[[1.636648e-04 9.998363e-01]]\n",
            "[[0.0337746  0.96622545]]\n",
            "[[0.02926058 0.9707394 ]]\n",
            "[[2.0910065e-05 9.9997914e-01]]\n",
            "[[0.00474812 0.9952519 ]]\n",
            "[[6.7905354e-04 9.9932098e-01]]\n",
            "[[3.3270256e-04 9.9966729e-01]]\n",
            "[[0.33218747 0.6678126 ]]\n",
            "[[1.0177618e-04 9.9989820e-01]]\n",
            "[[0.00174125 0.99825877]]\n",
            "[[2.0605770e-04 9.9979395e-01]]\n",
            "[[0.31095842 0.6890416 ]]\n",
            "[[0.00524104 0.9947589 ]]\n",
            "[[0.2319241 0.7680758]]\n",
            "[[0.17897277 0.8210273 ]]\n",
            "[[0.83871394 0.16128609]]\n",
            "[[0.00119556 0.99880445]]\n",
            "[[8.7582564e-04 9.9912423e-01]]\n",
            "[[0.00104933 0.9989506 ]]\n",
            "[[2.545330e-04 9.997454e-01]]\n",
            "[[1.4809624e-04 9.9985182e-01]]\n",
            "[[0.2058866 0.7941134]]\n",
            "[[8.624842e-05 9.999137e-01]]\n",
            "[[3.7774883e-04 9.9962223e-01]]\n",
            "[[5.290187e-04 9.994710e-01]]\n",
            "[[0.00207127 0.99792874]]\n",
            "[[0.00207192 0.997928  ]]\n",
            "[[6.8849709e-04 9.9931145e-01]]\n",
            "[[0.09046493 0.90953505]]\n",
            "[[0.17182027 0.8281798 ]]\n",
            "[[0.7507257  0.24927433]]\n",
            "[[0.273945   0.72605497]]\n",
            "[[0.00779328 0.99220675]]\n",
            "[[0.00631652 0.99368346]]\n",
            "[[0.8656071  0.13439286]]\n",
            "[[5.5560569e-05 9.9994445e-01]]\n",
            "[[0.0271763 0.9728237]]\n",
            "[[0.00539846 0.9946016 ]]\n",
            "[[0.01367115 0.9863288 ]]\n",
            "[[7.783452e-04 9.992217e-01]]\n",
            "[[0.00196636 0.9980336 ]]\n",
            "[[0.00310505 0.99689496]]\n",
            "[[0.5532558 0.4467442]]\n",
            "[[3.753087e-04 9.996247e-01]]\n",
            "[[0.00988686 0.99011314]]\n",
            "[[0.31133154 0.6886685 ]]\n",
            "[[2.1164608e-04 9.9978834e-01]]\n",
            "[[5.137030e-06 9.999949e-01]]\n",
            "[[0.0015388  0.99846125]]\n",
            "[[2.6127300e-04 9.9973875e-01]]\n",
            "[[6.022700e-07 9.999994e-01]]\n",
            "[[8.4203675e-05 9.9991584e-01]]\n",
            "[[8.583470e-06 9.999914e-01]]\n",
            "[[0.01790321 0.9820968 ]]\n",
            "[[0.01105119 0.9889488 ]]\n",
            "[[0.04030418 0.9596958 ]]\n",
            "[[0.0027625  0.99723744]]\n",
            "[[0.02088251 0.9791175 ]]\n",
            "[[7.8200933e-04 9.9921799e-01]]\n",
            "[[0.07702783 0.9229722 ]]\n",
            "[[1.3733919e-04 9.9986267e-01]]\n",
            "[[0.0728917  0.92710835]]\n",
            "[[0.3185854 0.6814146]]\n",
            "[[0.00576822 0.99423176]]\n",
            "[[3.4316727e-05 9.9996567e-01]]\n",
            "[[0.35465783 0.6453422 ]]\n",
            "[[0.00284565 0.99715436]]\n",
            "[[0.91351736 0.0864826 ]]\n",
            "[[1.1818824e-04 9.9988174e-01]]\n",
            "[[6.377523e-05 9.999362e-01]]\n",
            "[[0.11291924 0.8870807 ]]\n",
            "[[1.1298695e-04 9.9988699e-01]]\n",
            "[[0.00691541 0.99308455]]\n",
            "[[6.8476918e-04 9.9931526e-01]]\n",
            "[[9.311013e-04 9.990689e-01]]\n",
            "[[0.01619747 0.98380256]]\n",
            "[[0.00215911 0.9978409 ]]\n",
            "[[0.00324899 0.99675107]]\n",
            "[[2.1846982e-04 9.9978155e-01]]\n",
            "[[0.00649696 0.99350303]]\n",
            "[[0.00109993 0.99890006]]\n",
            "[[9.110161e-04 9.990890e-01]]\n",
            "[[0.00791984 0.99208015]]\n",
            "[[0.21079645 0.7892036 ]]\n",
            "[[1.30581975e-05 9.99986887e-01]]\n",
            "[[0.04672804 0.9532719 ]]\n",
            "[[0.17376544 0.82623464]]\n",
            "[[2.5270156e-05 9.9997473e-01]]\n",
            "[[0.00537179 0.99462825]]\n",
            "[[0.53358686 0.4664131 ]]\n",
            "[[0.01828703 0.981713  ]]\n",
            "[[0.00673363 0.99326634]]\n",
            "[[0.18662417 0.8133758 ]]\n",
            "[[0.00138276 0.9986173 ]]\n",
            "[[2.7363971e-04 9.9972636e-01]]\n",
            "[[0.96389085 0.03610912]]\n",
            "[[0.00130793 0.9986921 ]]\n",
            "[[2.8809815e-04 9.9971193e-01]]\n",
            "[[0.00319436 0.99680567]]\n",
            "[[8.2920218e-05 9.9991703e-01]]\n",
            "[[1.283186e-04 9.998716e-01]]\n",
            "[[0.0112698  0.98873013]]\n",
            "[[0.09641358 0.9035864 ]]\n",
            "[[0.01258744 0.9874126 ]]\n",
            "[[0.00731291 0.9926871 ]]\n",
            "[[9.428650e-05 9.999057e-01]]\n",
            "[[2.7618478e-05 9.9997234e-01]]\n",
            "[[3.0630370e-05 9.9996936e-01]]\n",
            "[[2.7819801e-04 9.9972183e-01]]\n",
            "[[4.887936e-05 9.999511e-01]]\n",
            "[[2.6260444e-05 9.9997377e-01]]\n",
            "[[0.53707904 0.46292096]]\n",
            "[[0.00427841 0.9957216 ]]\n",
            "[[7.188110e-05 9.999281e-01]]\n",
            "[[0.26882628 0.73117375]]\n",
            "[[6.7181805e-05 9.9993277e-01]]\n",
            "[[0.00749961 0.99250036]]\n",
            "[[1.8579725e-04 9.9981421e-01]]\n",
            "[[8.239001e-04 9.991761e-01]]\n",
            "[[0.66921514 0.3307849 ]]\n",
            "[[0.02276153 0.9772385 ]]\n",
            "[[4.3197098e-05 9.9995685e-01]]\n",
            "[[0.01209719 0.9879028 ]]\n",
            "[[0.00411156 0.9958884 ]]\n",
            "[[0.00208395 0.99791604]]\n",
            "[[0.00273195 0.9972681 ]]\n",
            "[[0.00486837 0.9951316 ]]\n",
            "[[2.6233136e-04 9.9973768e-01]]\n",
            "[[0.05406242 0.9459375 ]]\n",
            "[[0.38560593 0.61439407]]\n",
            "[[6.0161245e-05 9.9993980e-01]]\n",
            "[[0.00177193 0.99822813]]\n",
            "[[0.01808573 0.98191434]]\n",
            "[[0.04328469 0.9567153 ]]\n",
            "[[2.3639467e-04 9.9976367e-01]]\n",
            "[[0.0160937  0.98390627]]\n",
            "[[3.7605053e-06 9.9999619e-01]]\n",
            "[[0.00538823 0.99461174]]\n",
            "[[0.03718741 0.9628126 ]]\n",
            "[[0.01955689 0.98044306]]\n",
            "[[2.7117647e-05 9.9997294e-01]]\n",
            "[[0.81073695 0.18926306]]\n",
            "[[0.28802347 0.7119765 ]]\n",
            "[[0.3587633 0.6412367]]\n",
            "[[6.7490828e-04 9.9932516e-01]]\n",
            "[[4.9511483e-04 9.9950492e-01]]\n",
            "[[8.0448983e-05 9.9991953e-01]]\n",
            "[[0.27814186 0.72185814]]\n",
            "[[0.00132433 0.99867564]]\n",
            "[[0.0339703 0.9660297]]\n",
            "[[4.787559e-07 9.999995e-01]]\n",
            "[[3.2289245e-05 9.9996769e-01]]\n",
            "[[0.00384369 0.9961563 ]]\n",
            "[[0.0082182 0.9917818]]\n",
            "[[0.81461823 0.18538171]]\n",
            "[[1.2655588e-04 9.9987340e-01]]\n",
            "[[8.5337386e-05 9.9991465e-01]]\n",
            "[[0.01288016 0.9871199 ]]\n",
            "[[0.1359893 0.8640107]]\n",
            "[[0.00718174 0.99281824]]\n",
            "[[0.41877118 0.5812288 ]]\n",
            "[[0.05719744 0.9428025 ]]\n",
            "[[0.00282339 0.99717665]]\n",
            "[[0.01318451 0.9868155 ]]\n",
            "[[0.0013777 0.9986223]]\n",
            "[[0.55352116 0.44647875]]\n",
            "[[1.0613472e-05 9.9998939e-01]]\n",
            "[[3.8488605e-04 9.9961507e-01]]\n",
            "[[0.14105184 0.8589482 ]]\n",
            "[[0.02476919 0.9752308 ]]\n",
            "[[4.3820232e-06 9.9999559e-01]]\n",
            "[[0.0447762 0.9552238]]\n",
            "[[1.3185904e-04 9.9986815e-01]]\n",
            "[[0.00249105 0.9975089 ]]\n",
            "[[2.7573062e-04 9.9972421e-01]]\n",
            "[[9.551096e-05 9.999045e-01]]\n",
            "[[1.0842502e-05 9.9998915e-01]]\n",
            "[[0.03752614 0.9624738 ]]\n",
            "[[0.00424076 0.9957592 ]]\n",
            "[[0.28133202 0.718668  ]]\n",
            "[[4.0033719e-06 9.9999595e-01]]\n",
            "[[0.00211375 0.99788624]]\n",
            "[[1.1639917e-04 9.9988353e-01]]\n",
            "[[0.2945835  0.70541644]]\n",
            "[[0.5841931  0.41580686]]\n",
            "[[4.6948028e-07 9.9999952e-01]]\n",
            "[[0.2797798 0.7202202]]\n",
            "[[4.4614266e-04 9.9955386e-01]]\n",
            "[[0.01337623 0.9866238 ]]\n",
            "[[0.37695494 0.62304497]]\n",
            "[[3.8954176e-06 9.9999607e-01]]\n",
            "[[9.0934271e-05 9.9990904e-01]]\n",
            "[[0.00691141 0.99308854]]\n",
            "[[0.78804255 0.21195744]]\n",
            "[[0.1657132 0.8342868]]\n",
            "[[0.03407756 0.96592236]]\n",
            "[[0.3913686  0.60863143]]\n",
            "[[0.05562579 0.9443742 ]]\n",
            "[[0.00588381 0.9941162 ]]\n",
            "[[3.8267844e-04 9.9961734e-01]]\n",
            "[[0.0086466  0.99135333]]\n",
            "[[0.73849934 0.26150072]]\n",
            "[[0.00305874 0.99694127]]\n",
            "[[0.00856473 0.9914352 ]]\n",
            "[[2.307035e-04 9.997693e-01]]\n",
            "[[3.9716958e-04 9.9960285e-01]]\n",
            "[[0.38108492 0.6189151 ]]\n",
            "[[0.938845   0.06115504]]\n",
            "[[0.02412339 0.9758766 ]]\n",
            "[[0.04405625 0.95594376]]\n",
            "[[0.9014536  0.09854646]]\n",
            "[[1.3028998e-04 9.9986970e-01]]\n",
            "[[1.5298334e-04 9.9984694e-01]]\n",
            "[[0.02682428 0.9731757 ]]\n",
            "[[0.01500471 0.9849953 ]]\n",
            "[[0.00553628 0.99446374]]\n",
            "[[2.4430756e-04 9.9975568e-01]]\n",
            "[[0.09557288 0.9044272 ]]\n",
            "[[0.00102865 0.99897134]]\n",
            "[[3.922806e-05 9.999608e-01]]\n",
            "[[1.747460e-04 9.998253e-01]]\n",
            "[[8.649733e-04 9.991351e-01]]\n",
            "[[0.37020132 0.6297987 ]]\n",
            "[[6.8468275e-04 9.9931538e-01]]\n",
            "[[3.6841495e-05 9.9996316e-01]]\n",
            "[[0.00241058 0.99758947]]\n",
            "[[6.7999026e-05 9.9993205e-01]]\n",
            "[[8.0884674e-05 9.9991906e-01]]\n",
            "[[0.00131395 0.998686  ]]\n",
            "[[0.04866966 0.9513303 ]]\n",
            "[[0.03518856 0.96481144]]\n",
            "[[0.00185954 0.99814045]]\n",
            "[[0.00145994 0.99854004]]\n",
            "[[0.13287683 0.8671231 ]]\n",
            "[[0.00715444 0.9928456 ]]\n",
            "[[0.00681733 0.9931827 ]]\n",
            "[[9.824575e-04 9.990175e-01]]\n",
            "[[3.3316689e-06 9.9999666e-01]]\n",
            "[[3.0386793e-05 9.9996960e-01]]\n",
            "[[0.00184933 0.99815065]]\n",
            "[[0.13743553 0.8625645 ]]\n",
            "[[0.9958164  0.00418362]]\n",
            "[[0.86761075 0.13238928]]\n",
            "[[0.23456325 0.76543677]]\n",
            "[[0.00164567 0.9983543 ]]\n",
            "[[0.07415038 0.9258497 ]]\n",
            "[[0.05549107 0.94450897]]\n",
            "[[2.288921e-05 9.999771e-01]]\n",
            "[[6.3476531e-05 9.9993646e-01]]\n",
            "[[8.850322e-06 9.999912e-01]]\n",
            "[[0.00164989 0.99835014]]\n",
            "[[4.2169387e-04 9.9957830e-01]]\n",
            "[[0.00312723 0.99687284]]\n",
            "[[0.02893719 0.97106284]]\n",
            "[[0.00121475 0.9987853 ]]\n",
            "[[1.674884e-05 9.999832e-01]]\n",
            "[[1.4807463e-04 9.9985194e-01]]\n",
            "[[4.3431739e-04 9.9956566e-01]]\n",
            "[[0.02401798 0.97598195]]\n",
            "[[0.13875037 0.86124957]]\n",
            "[[2.2537014e-04 9.9977463e-01]]\n",
            "[[0.03560668 0.96439326]]\n",
            "[[0.01318284 0.9868172 ]]\n",
            "[[0.1579442 0.8420558]]\n",
            "[[0.00302936 0.99697065]]\n",
            "[[4.167147e-04 9.995833e-01]]\n",
            "[[0.09268149 0.90731853]]\n",
            "[[0.00701263 0.99298733]]\n",
            "[[0.00148489 0.99851507]]\n",
            "[[0.2985836  0.70141643]]\n",
            "[[0.00580946 0.9941906 ]]\n",
            "[[0.00607009 0.99392986]]\n",
            "[[0.321447 0.678553]]\n",
            "[[0.00294888 0.9970511 ]]\n",
            "[[0.04922353 0.9507765 ]]\n",
            "[[0.00305065 0.9969494 ]]\n",
            "[[9.2659716e-04 9.9907339e-01]]\n",
            "[[0.00311214 0.99688786]]\n",
            "[[2.2628545e-05 9.9997735e-01]]\n",
            "[[0.01060378 0.9893963 ]]\n",
            "[[0.8326768 0.1673232]]\n",
            "[[0.1345708 0.8654292]]\n",
            "[[0.07309551 0.9269045 ]]\n",
            "[[5.4804801e-05 9.9994516e-01]]\n",
            "[[1.4896682e-04 9.9985099e-01]]\n",
            "[[0.00747644 0.9925236 ]]\n",
            "[[0.35036063 0.6496394 ]]\n",
            "[[0.65669596 0.34330398]]\n",
            "[[0.9072366  0.09276343]]\n",
            "[[0.1987087 0.8012913]]\n",
            "[[0.00774487 0.9922551 ]]\n",
            "[[3.4837567e-04 9.9965167e-01]]\n",
            "[[0.78309107 0.21690895]]\n",
            "[[2.2761688e-04 9.9977237e-01]]\n",
            "[[0.01147707 0.988523  ]]\n",
            "[[1.2671007e-04 9.9987328e-01]]\n",
            "[[1.0537318e-04 9.9989462e-01]]\n",
            "[[0.43977413 0.56022584]]\n",
            "[[0.01400147 0.9859985 ]]\n",
            "[[4.6992613e-05 9.9995303e-01]]\n",
            "[[0.0099828  0.99001724]]\n",
            "[[0.02380629 0.97619367]]\n",
            "[[0.00709405 0.992906  ]]\n",
            "[[0.6971628 0.3028372]]\n",
            "[[0.05240735 0.9475927 ]]\n",
            "[[0.42051598 0.579484  ]]\n",
            "[[1.3219316e-04 9.9986780e-01]]\n",
            "[[0.00294097 0.9970591 ]]\n",
            "[[0.08625413 0.9137458 ]]\n",
            "[[5.040279e-04 9.994960e-01]]\n",
            "[[0.51385164 0.48614836]]\n",
            "[[0.04197547 0.9580245 ]]\n",
            "[[2.7461661e-05 9.9997258e-01]]\n",
            "[[8.5435086e-04 9.9914563e-01]]\n",
            "[[0.00102788 0.9989722 ]]\n",
            "[[2.0897448e-06 9.9999785e-01]]\n",
            "[[8.259543e-06 9.999918e-01]]\n",
            "[[1.2429988e-04 9.9987566e-01]]\n",
            "[[0.67236745 0.32763252]]\n",
            "[[2.2539293e-04 9.9977463e-01]]\n",
            "[[1.4263381e-04 9.9985731e-01]]\n",
            "[[0.00461774 0.99538225]]\n",
            "[[1.7747232e-04 9.9982256e-01]]\n",
            "[[0.00743717 0.99256283]]\n",
            "[[0.00552778 0.9944722 ]]\n",
            "[[0.00164626 0.9983537 ]]\n",
            "[[0.00106016 0.9989398 ]]\n",
            "[[0.03079077 0.96920925]]\n",
            "[[0.00336723 0.99663275]]\n",
            "[[0.00118122 0.9988187 ]]\n",
            "[[1.6183938e-05 9.9998379e-01]]\n",
            "[[0.00127823 0.9987218 ]]\n",
            "[[0.90030134 0.09969871]]\n",
            "[[0.00407517 0.99592483]]\n",
            "[[0.00687223 0.9931278 ]]\n",
            "[[0.84215516 0.15784489]]\n",
            "[[0.02607571 0.97392434]]\n",
            "[[0.01466766 0.9853323 ]]\n",
            "[[0.10491005 0.89509   ]]\n",
            "[[1.1985293e-04 9.9988008e-01]]\n",
            "[[0.00371717 0.9962829 ]]\n",
            "[[2.5973773e-06 9.9999738e-01]]\n",
            "[[1.9508127e-05 9.9998045e-01]]\n",
            "[[0.94395703 0.05604299]]\n",
            "[[0.0110524  0.98894763]]\n",
            "[[0.0071593  0.99284065]]\n",
            "[[0.03750224 0.9624978 ]]\n",
            "[[0.02166337 0.9783367 ]]\n",
            "[[9.784938e-06 9.999902e-01]]\n",
            "[[0.00491167 0.9950884 ]]\n",
            "[[5.8782194e-04 9.9941218e-01]]\n",
            "[[0.04134033 0.9586597 ]]\n",
            "[[0.9240283  0.07597164]]\n",
            "[[0.6910136  0.30898649]]\n",
            "[[0.01598386 0.9840162 ]]\n",
            "[[3.0568769e-04 9.9969435e-01]]\n",
            "[[0.01867939 0.98132056]]\n",
            "[[0.08418505 0.9158149 ]]\n",
            "[[0.7685196  0.23148043]]\n",
            "[[2.6582875e-06 9.9999738e-01]]\n",
            "[[0.00216   0.9978399]]\n",
            "[[3.1487438e-05 9.9996853e-01]]\n",
            "[[0.38129032 0.61870974]]\n",
            "[[0.00379593 0.9962041 ]]\n",
            "[[2.4087743e-05 9.9997592e-01]]\n",
            "[[3.9409199e-05 9.9996054e-01]]\n",
            "[[7.807553e-04 9.992192e-01]]\n",
            "[[0.30836153 0.6916384 ]]\n",
            "[[0.04625715 0.95374286]]\n",
            "[[0.35905772 0.6409423 ]]\n",
            "[[0.04371719 0.95628273]]\n",
            "[[0.01540327 0.9845968 ]]\n",
            "[[9.2188668e-05 9.9990785e-01]]\n",
            "[[0.15669236 0.8433077 ]]\n",
            "[[0.03023772 0.9697623 ]]\n",
            "[[0.24539489 0.7546051 ]]\n",
            "[[0.00286509 0.9971349 ]]\n",
            "[[0.00739679 0.99260324]]\n",
            "[[5.2979405e-05 9.9994707e-01]]\n",
            "[[0.02843427 0.9715657 ]]\n",
            "[[8.247482e-05 9.999175e-01]]\n",
            "[[0.00618684 0.99381316]]\n",
            "[[3.9109837e-05 9.9996090e-01]]\n",
            "[[0.00868479 0.9913152 ]]\n",
            "[[0.00345237 0.99654764]]\n",
            "[[0.48848101 0.511519  ]]\n",
            "[[0.00250554 0.99749446]]\n",
            "[[0.08334368 0.9166563 ]]\n",
            "[[0.13277633 0.8672237 ]]\n",
            "[[0.01344588 0.98655415]]\n",
            "[[4.0928414e-04 9.9959069e-01]]\n",
            "[[0.0248132 0.9751868]]\n",
            "[[0.01679016 0.98320985]]\n",
            "[[0.00225448 0.99774545]]\n",
            "[[9.5715375e-05 9.9990427e-01]]\n",
            "[[0.00103062 0.99896944]]\n",
            "[[3.2181494e-04 9.9967825e-01]]\n",
            "[[1.675522e-04 9.998324e-01]]\n",
            "[[0.8923035  0.10769647]]\n",
            "[[8.5838797e-04 9.9914157e-01]]\n",
            "[[0.7526845  0.24731556]]\n",
            "[[1.8191629e-04 9.9981815e-01]]\n",
            "[[2.8582488e-04 9.9971420e-01]]\n",
            "[[0.0012582 0.9987418]]\n",
            "[[0.01706901 0.98293096]]\n",
            "[[4.970311e-05 9.999503e-01]]\n",
            "[[0.01736904 0.9826309 ]]\n",
            "[[0.24417154 0.75582844]]\n",
            "[[0.00610299 0.993897  ]]\n",
            "[[0.00414281 0.9958572 ]]\n",
            "[[0.01682123 0.98317873]]\n",
            "[[2.7260927e-05 9.9997270e-01]]\n",
            "[[7.6053802e-05 9.9992394e-01]]\n",
            "[[0.09094259 0.9090574 ]]\n",
            "[[0.18715595 0.81284404]]\n",
            "[[0.00141505 0.998585  ]]\n",
            "[[0.00343279 0.99656713]]\n",
            "[[0.01137466 0.9886254 ]]\n",
            "[[3.1977845e-05 9.9996805e-01]]\n",
            "[[0.05635669 0.94364333]]\n",
            "[[0.19731572 0.8026843 ]]\n",
            "[[2.2533446e-04 9.9977463e-01]]\n",
            "[[0.79278296 0.207217  ]]\n",
            "[[0.00325148 0.99674857]]\n",
            "[[5.2224013e-06 9.9999475e-01]]\n",
            "[[4.9884886e-05 9.9995017e-01]]\n",
            "[[0.01418604 0.9858139 ]]\n",
            "[[1.6832374e-04 9.9983168e-01]]\n",
            "[[0.03435409 0.96564597]]\n",
            "[[1.8636165e-04 9.9981362e-01]]\n",
            "[[2.4273002e-05 9.9997568e-01]]\n",
            "[[0.00157651 0.99842346]]\n",
            "[[0.00398979 0.9960102 ]]\n",
            "[[9.304907e-04 9.990695e-01]]\n",
            "[[0.21629572 0.78370434]]\n",
            "[[0.02906211 0.97093785]]\n",
            "[[6.1029805e-06 9.9999392e-01]]\n",
            "[[0.06078549 0.93921447]]\n",
            "[[3.0602146e-06 9.9999690e-01]]\n",
            "[[0.00504665 0.99495333]]\n",
            "[[3.8223279e-05 9.9996173e-01]]\n",
            "[[2.1722410e-06 9.9999785e-01]]\n",
            "[[0.00414643 0.99585354]]\n",
            "[[0.30346483 0.6965351 ]]\n",
            "[[0.00396519 0.9960348 ]]\n",
            "[[5.3590891e-05 9.9994636e-01]]\n",
            "[[0.01160051 0.9883995 ]]\n",
            "[[0.8152665  0.18473345]]\n",
            "[[3.175751e-04 9.996824e-01]]\n",
            "[[1.431644e-04 9.998568e-01]]\n",
            "[[4.2067765e-04 9.9957937e-01]]\n",
            "[[0.35053676 0.6494633 ]]\n",
            "[[0.06118866 0.93881136]]\n",
            "[[3.2352869e-04 9.9967647e-01]]\n",
            "[[1.6881947e-06 9.9999833e-01]]\n",
            "[[0.07454518 0.9254548 ]]\n",
            "[[9.744746e-04 9.990256e-01]]\n",
            "[[0.00158091 0.99841905]]\n",
            "[[0.00474393 0.995256  ]]\n",
            "[[0.09964544 0.90035456]]\n",
            "[[1.4082533e-04 9.9985921e-01]]\n",
            "[[0.09922657 0.90077347]]\n",
            "[[0.00799918 0.9920008 ]]\n",
            "[[4.7093818e-05 9.9995291e-01]]\n",
            "[[7.2284114e-05 9.9992776e-01]]\n",
            "[[0.01063023 0.98936975]]\n",
            "[[0.00920068 0.99079925]]\n",
            "[[0.04455759 0.9554424 ]]\n",
            "[[0.00350689 0.99649316]]\n",
            "[[0.00894951 0.9910505 ]]\n",
            "[[5.9061393e-04 9.9940944e-01]]\n",
            "[[0.01176488 0.9882351 ]]\n",
            "[[1.090973e-04 9.998909e-01]]\n",
            "[[0.06940337 0.9305966 ]]\n",
            "[[0.22611374 0.77388626]]\n",
            "[[0.33460432 0.66539574]]\n",
            "[[0.02365393 0.976346  ]]\n",
            "[[4.3081785e-05 9.9995697e-01]]\n",
            "[[0.00174735 0.9982527 ]]\n",
            "[[0.36014235 0.63985765]]\n",
            "[[0.06085217 0.9391478 ]]\n",
            "[[1.3152571e-05 9.9998689e-01]]\n",
            "[[0.38654724 0.61345273]]\n",
            "[[0.0880461 0.9119539]]\n",
            "[[0.7422266  0.25777343]]\n",
            "[[0.00180268 0.9981974 ]]\n",
            "[[4.959006e-06 9.999950e-01]]\n",
            "[[7.3608324e-05 9.9992633e-01]]\n",
            "[[0.2719118 0.7280882]]\n",
            "[[7.363770e-07 9.999993e-01]]\n",
            "[[0.00130533 0.9986947 ]]\n",
            "[[0.00195683 0.9980432 ]]\n",
            "[[0.00989506 0.990105  ]]\n",
            "[[2.5073884e-04 9.9974924e-01]]\n",
            "[[0.6446742  0.35532582]]\n",
            "[[4.0680362e-04 9.9959320e-01]]\n",
            "[[4.734355e-05 9.999527e-01]]\n",
            "[[0.45977035 0.5402296 ]]\n",
            "[[0.9034879  0.09651205]]\n",
            "[[0.1111028 0.8888972]]\n",
            "[[0.00438184 0.9956182 ]]\n",
            "[[0.55090874 0.44909126]]\n",
            "[[0.35372803 0.646272  ]]\n",
            "[[0.00296087 0.9970392 ]]\n",
            "[[2.4276045e-04 9.9975723e-01]]\n",
            "[[0.04496249 0.9550375 ]]\n",
            "[[0.01985789 0.9801421 ]]\n",
            "[[0.18433897 0.815661  ]]\n",
            "[[0.0205848 0.9794152]]\n",
            "[[0.00350122 0.9964987 ]]\n",
            "[[0.00193215 0.9980679 ]]\n",
            "[[0.08508082 0.9149192 ]]\n",
            "[[0.0028753  0.99712473]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1f_sG1Cr2cF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "371a73f7-968f-4a85-80d1-a1477eafab80"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  ID label\n",
              "0  ASG001bqwg_2.jpeg     0\n",
              "1  ASG001g2rm_2.jpeg     0\n",
              "2  ASG001f5n2_2.jpeg     0\n",
              "3  ASG001g7sb_2.jpeg     0\n",
              "4  ASG001dr9n_2.jpeg     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-730d087c-e95d-4af7-8b16-95b4b0a433c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ASG001bqwg_2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ASG001g2rm_2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ASG001f5n2_2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ASG001g7sb_2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ASG001dr9n_2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-730d087c-e95d-4af7-8b16-95b4b0a433c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-730d087c-e95d-4af7-8b16-95b4b0a433c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-730d087c-e95d-4af7-8b16-95b4b0a433c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use vision transformers"
      ],
      "metadata": {
        "id": "njJE3B77jJqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LdmjqZIPCHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLlEHO8xPCKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0GQ9nW0PCOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUL1QGzVPCQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhzW0P7aPCS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThDUqj5tPCVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByYxntLcPCX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqqwvBCvPCZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdmPv66BnZPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}